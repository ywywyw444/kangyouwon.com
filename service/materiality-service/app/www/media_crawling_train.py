"""
ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¥¼ ì‚¬ìš©í•œ ë¯¸ë””ì–´ í¬ë¡¤ë§ ëª¨ë“ˆ
(pubDate ê¸°ë°˜ ë‚ ì§œ í•„í„°ë§ + ë§í¬ ì •ê·œí™” + í•™ìŠµë°ì´í„° êµ¬ì¶• ê¸°ëŠ¥ + ìš”ì²­ ì“°ë¡œí‹€ë§/ë°±ì˜¤í”„)
"""
import os
import sys
import time
import random
import requests
from typing import Dict, Any, List
import logging
from datetime import datetime, timezone
import email.utils  # pubDate íŒŒì‹±ìš©
from pathlib import Path
import pandas as pd

# materiality-service ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
SERVICE_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(SERVICE_ROOT))

# .env íŒŒì¼ ë¡œë“œ
from dotenv import load_dotenv
load_dotenv(SERVICE_ROOT / ".env")

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)
if not logger.handlers:
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s"
    )


class NaverNewsCrawler:
    """ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ í¬ë¡¤ë§ í´ë˜ìŠ¤"""

    def __init__(
        self,
        min_interval: float = None,
        per_keyword_pause: float = None,
        max_retries: int = None,
    ):
        self.client_id = os.getenv("NAVER_CLIENT_ID")
        self.client_secret = os.getenv("NAVER_CLIENT_SECRET")

        if not self.client_id or not self.client_secret:
            raise ValueError("NAVER_CLIENT_IDì™€ NAVER_CLIENT_SECRET í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.base_url = "https://openapi.naver.com/v1/search/news.json"

        # ìš”ì²­ ê°„ ìµœì†Œ ê°„ê²©(ì´ˆ) ë° í‚¤ì›Œë“œ ê°„ ì¶”ê°€ ëŒ€ê¸°(ì´ˆ), ì¬ì‹œë„ íšŸìˆ˜
        self.min_interval = float(os.getenv("NAVER_API_MIN_INTERVAL", "1.2")) if min_interval is None else float(min_interval)
        self.per_keyword_pause = float(os.getenv("NAVER_API_PER_KEYWORD_PAUSE", "2.0")) if per_keyword_pause is None else float(per_keyword_pause)
        self.max_retries = int(os.getenv("NAVER_API_MAX_RETRIES", "3")) if max_retries is None else int(max_retries)

        # ì§€í„°(ë¬´ì‘ìœ„ ì†ŒëŸ‰ ëŒ€ê¸°) ë²”ìœ„
        self._jitter_range = (0.05, 0.25)

        # ë§ˆì§€ë§‰ ìš”ì²­ ì‹œê° (ì“°ë¡œí‹€ë§ìš©)
        self._last_request_ts = 0.0

        # ì„¸ì…˜ ì¬ì‚¬ìš©
        self.session = requests.Session()
        self.session.headers.update({
            'X-Naver-Client-Id': self.client_id,
            'X-Naver-Client-Secret': self.client_secret,
            'User-Agent': 'materiality-service/1.0'
        })

    def _throttle(self):
        """ìš”ì²­ ê°„ ìµœì†Œ ê°„ê²© ë³´ì¥ + ì†ŒëŸ‰ ì§€í„°"""
        now = time.time()
        elapsed = now - self._last_request_ts
        wait_needed = self.min_interval - elapsed
        if wait_needed > 0:
            jitter = random.uniform(*self._jitter_range)
            sleep_for = wait_needed + jitter
            time.sleep(sleep_for)
        self._last_request_ts = time.time()

    def _request_with_retry(self, url: str, params: Dict[str, Any]) -> Dict[str, Any]:
        """ì¬ì‹œë„ + ë°±ì˜¤í”„ë¥¼ í¬í•¨í•œ GET ìš”ì²­ ë˜í¼"""
        last_exc = None
        for attempt in range(1, self.max_retries + 1):
            try:
                self._throttle()
                resp = self.session.get(url, params=params, timeout=10)
                # 429/5xx ì²˜ë¦¬
                if resp.status_code == 429 or 500 <= resp.status_code < 600:
                    raise requests.exceptions.HTTPError(
                        f"HTTP {resp.status_code}: {resp.text[:200]}"
                    )
                resp.raise_for_status()
                return resp.json()
            except requests.exceptions.RequestException as e:
                last_exc = e
                backoff = (self.min_interval * (2 ** (attempt - 1))) + random.uniform(*self._jitter_range)
                logger.warning(f"ìš”ì²­ ì‹¤íŒ¨(ì‹œë„ {attempt}/{self.max_retries}): {e}. {backoff:.2f}s í›„ ì¬ì‹œë„")
                time.sleep(backoff)
        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨
        logger.error(f"âŒ ë„¤ì´ë²„ ë‰´ìŠ¤ API ìš”ì²­ ì‹¤íŒ¨(ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼): {last_exc}")
        raise last_exc

    def search_news(self, keyword: str, display: int = 5, sort: str = "date", start: int = 1) -> Dict[str, Any]:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰"""
        params = {
            'query': keyword,
            'display': display,
            'sort': sort,
            'start': start
        }
        return self._request_with_retry(self.base_url, params=params)

    def search_news_by_date_range(self, keyword: str, start_date: str, end_date: str, max_results: int = 100) -> Dict[str, Any]:
        """
        pubDate ê¸°ë°˜ìœ¼ë¡œ íŠ¹ì • ë‚ ì§œ ë²”ìœ„ ë‚´ ë‰´ìŠ¤ë¥¼ í•„í„°ë§
        (í˜ì´ì§€ë„¤ì´ì…˜ ìš”ì²­ë„ ì“°ë¡œí‹€ë§/ë°±ì˜¤í”„ ì ìš©)
        """
        # ë‚ ì§œ ë³€í™˜ (ì‹œê°„ëŒ€ ì •ë³´ ì¶”ê°€)
        start_dt = datetime.strptime(start_date, "%Y-%m-%d").replace(tzinfo=timezone.utc)
        end_dt = datetime.strptime(end_date, "%Y-%m-%d").replace(tzinfo=timezone.utc)

        if start_dt > end_dt:
            raise ValueError("ì‹œì‘ ë‚ ì§œê°€ ì¢…ë£Œ ë‚ ì§œë³´ë‹¤ ëŠ¦ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        all_items: List[Dict[str, Any]] = []
        start = 1
        display = 100  # ë„¤ì´ë²„ API ìµœëŒ€ê°’

        while start <= max_results and len(all_items) < max_results:
            result = self.search_news(keyword, display=display, start=start, sort="date")
            items = result.get("items", [])
            if not items:
                break

            # pubDate íŒŒì‹±í•´ì„œ í•„í„°ë§
            for item in items:
                try:
                    pub_dt = email.utils.parsedate_to_datetime(item["pubDate"])  # RFC 2822 â†’ datetime
                except Exception:
                    # pubDate íŒŒì‹± ì‹¤íŒ¨ ì‹œ ìŠ¤í‚µ
                    continue
                if start_dt <= pub_dt <= end_dt:
                    all_items.append(item)

            start += display
            if start > 1000:  # ë„¤ì´ë²„ API ìµœëŒ€ ì œí•œ
                break

        # ğŸ”¹ ë§í¬ ì •ê·œí™” ì¶”ê°€
        for item in all_items:
            origin = item.get("originallink", "")
            link = item.get("link", "")

            # ë„¤ì´ë²„ ë‰´ìŠ¤ ë„ë©”ì¸ë§Œ ì¸ì •
            if link and "n.news.naver.com" in link:
                item["ë„¤ì´ë²„ë§í¬"] = link
            else:
                item["ë„¤ì´ë²„ë§í¬"] = ""   # ë„¤ì´ë²„ ë§í¬ ì—†ìœ¼ë©´ ê³µë€ ì²˜ë¦¬
            item["ì›ë³¸ë§í¬"] = origin

        return {
            "total": len(all_items),
            "items": all_items[:max_results],
            "start_date": start_date,
            "end_date": end_date
        }

    def save_to_excel(self, data: Dict[str, Any], filename: str = "naver_news_results.xlsx") -> str:
        """
        ê²€ìƒ‰ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ Excel íŒŒì¼ë¡œ ì €ì¥
        """
        try:
            if not data.get("items"):
                logger.warning("ì €ì¥í•  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return ""

            # DataFrame ìƒì„±
            df = pd.DataFrame(data["items"])

            # ë‚ ì§œ ì •ë³´ ì¶”ê°€
            df["ê²€ìƒ‰_ì‹œì‘ì¼"] = data.get("start_date", "")
            df["ê²€ìƒ‰_ì¢…ë£Œì¼"] = data.get("end_date", "")
            df["ì´_ê²€ìƒ‰ê²°ê³¼ìˆ˜"] = data.get("total", 0)

            # ì»¬ëŸ¼ ìˆœì„œ ì¡°ì • ë° í•œê¸€ ì»¬ëŸ¼ëª…
            columns_mapping = {
                "title": "ì œëª©",
                "description": "ìš”ì•½",
                "pubDate": "ë°œí–‰ì¼",
                "ì›ë³¸ë§í¬": "ì›ë³¸ë§í¬",
                "ë„¤ì´ë²„ë§í¬": "ë„¤ì´ë²„ë§í¬",
            }

            # ì»¬ëŸ¼ëª… ë³€ê²½
            df = df.rename(columns=columns_mapping)

            # ì»¬ëŸ¼ ìˆœì„œ ì¡°ì •
            display_columns = ["ì œëª©", "ìš”ì•½", "ë°œí–‰ì¼", "ì›ë³¸ë§í¬", "ë„¤ì´ë²„ë§í¬", "ê²€ìƒ‰_ì‹œì‘ì¼", "ê²€ìƒ‰_ì¢…ë£Œì¼", "ì´_ê²€ìƒ‰ê²°ê³¼ìˆ˜"]
            df = df.reindex(columns=display_columns)

            # Excel íŒŒì¼ ì €ì¥ (í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ì— ì €ì¥)
            excel_path = Path.cwd() / filename
            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
                df.to_excel(writer, sheet_name='ë‰´ìŠ¤ê²€ìƒ‰ê²°ê³¼', index=False)

                # ì›Œí¬ì‹œíŠ¸ ê°€ì ¸ì˜¤ê¸°
                worksheet = writer.sheets['ë‰´ìŠ¤ê²€ìƒ‰ê²°ê³¼']

                # ì»¬ëŸ¼ ë„ˆë¹„ ìë™ ì¡°ì •
                for column in worksheet.columns:
                    max_length = 0
                    column_letter = column[0].column_letter
                    for cell in column:
                        try:
                            if len(str(cell.value)) > max_length:
                                max_length = len(str(cell.value))
                        except Exception:
                            pass
                    adjusted_width = min(max_length + 2, 50)  # ìµœëŒ€ 50ìë¡œ ì œí•œ
                    worksheet.column_dimensions[column_letter].width = adjusted_width

            logger.info(f"âœ… Excel íŒŒì¼ ì €ì¥ ì™„ë£Œ: {excel_path}")
            return str(excel_path)

        except Exception as e:
            logger.error(f"âŒ Excel ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            raise

    def build_training_dataset(
        self,
        excel_path: str,
        output_file: str = "2023,2022ë…„ ì¤‘ëŒ€ì„±í‰ê°€ ëª©ë¡ ê¸°ë°˜ ë‰´ìŠ¤ë°ì´í„°.xlsx",
        start_date: str = "2023-01-01",
        end_date: str = "2023-12-31",
        max_results_per_keyword: int = 1000,
        per_keyword_pause: float = None,
    ):
        """
        ê¸°ì—…ëª… + ì¤‘ëŒ€ì„±í‰ê°€ ëª©ë¡ì„ ì¡°í•©í•œ í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³  ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ì—‘ì…€ë¡œ ì €ì¥
        - í‚¤ì›Œë“œë³„ ê²€ìƒ‰ ì¢…ë£Œ í›„ per_keyword_pause ë§Œí¼ ëŒ€ê¸°í•˜ì—¬ ê³¼ë„í•œ í˜¸ì¶œ ë°©ì§€
        - ë‚´ë¶€ì ìœ¼ë¡œ ê° ìš”ì²­ì€ min_interval + ì§€í„°ë¡œ ì“°ë¡œí‹€ë§ë¨
        """
        pause = self.per_keyword_pause if per_keyword_pause is None else float(per_keyword_pause)

        df = pd.read_excel(excel_path)
        all_results = []  # ì „ì²´ ë‰´ìŠ¤ ë°ì´í„° ì €ì¥ìš©

        for idx, row in df.iterrows():
            company = str(row["ê¸°ì—…ëª…"]).strip()
            issue = str(row["ì¤‘ëŒ€ì„±í‰ê°€ ëª©ë¡"]).strip()
            keyword = f"{company} {issue}"

            try:
                print(f"ğŸ” ê²€ìƒ‰ ì‹œì‘: {keyword}")
                result = self.search_news_by_date_range(
                    keyword=keyword,
                    start_date=start_date,
                    end_date=end_date,
                    max_results=max_results_per_keyword
                )

                for item in result["items"]:
                    item["ê¸°ì—…ëª…"] = company
                    item["ì¤‘ëŒ€ì„±í‰ê°€_í•­ëª©"] = issue
                    all_results.append(item)

                # í‚¤ì›Œë“œë³„ ì¶”ê°€ ëŒ€ê¸°
                jitter = random.uniform(*self._jitter_range)
                sleep_for = max(0.0, pause) + jitter
                time.sleep(sleep_for)
                logger.info(f"â³ í‚¤ì›Œë“œ ê°„ ëŒ€ê¸°: {sleep_for:.2f}s (ê¸°ì¤€ {pause}s + ì§€í„°)")

            except Exception as e:
                print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {keyword} - {e}")

        if not all_results:
            print("âš ï¸ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return ""

        # DataFrame ë³€í™˜
        news_df = pd.DataFrame(all_results)

        # ì—‘ì…€ ì €ì¥
        save_path = Path.cwd() / output_file
        with pd.ExcelWriter(save_path, engine="openpyxl") as writer:
            news_df.to_excel(writer, sheet_name="ë‰´ìŠ¤ë°ì´í„°", index=False)

        print(f"âœ… ì „ì²´ ë‰´ìŠ¤ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {save_path}")
        return str(save_path)
