"""
Middleissue Service - ì¤‘ëŒ€ì„± í‰ê°€ ê´€ë ¨ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì²˜ë¦¬
í¬ë¡¤ë§ ë°ì´í„° ì²˜ë¦¬, ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì ìš©, ì ìˆ˜ ê³„ì‚° ë“±ì„ ë‹´ë‹¹
"""
# 1. í¬ë¡¤ë§í•œ ì „ì²´ ë°ì´í„° -> ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë¡œ ê¸ë¶€ì •í‰ê°€
# 2. relevance, recent, negative, rank(ê²€ìƒ‰ì—°ë„-1), reference(NULL) ë¼ë²¨ ë¶€ì—¬
# 3. ê° ì§€í‘œë³„ score ë¶€ì—¬
# 4. final score ê³„ì‚°
# 5. frontendë¡œ ë³´ë‚´ê³  ë©”ëª¨ë¦¬ ì €ì¥

import logging
import os
import re
import json
import joblib
import numpy as np
from datetime import datetime
from typing import Dict, Any, List, Set, Tuple

from dateutil import parser
from app.domain.middleissue.schema import (
    MiddleIssueRequest, MiddleIssueResponse, Article,
    CategoryDetailsResponse, BaseIssuePool
)
from app.domain.middleissue.repository import MiddleIssueRepository

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

NEGATIVE_LEXICON = {
    "ê°ì†Œ","í•˜ë½","ë¶€ì§„","ì•…í™”","ì˜¤ì—¼","ìœ„ë°˜","ë‹´í•©","ë¶€íŒ¨","ë‡Œë¬¼","íš¡ë ¹","ë°°ì„","ì‚¬ê¸°",
    "ê³¼ì§•ê¸ˆ","ë²Œê¸ˆ","ì‚¬ê³ ","ì‚¬ë§","íŒŒì—…","ë¶„ìŸ","ê°ˆë“±","ë…¼ë€","ì†Œì†¡","ë¦¬ì½œ","ê²°í•¨","ë¶ˆëŸ‰",
    "ëˆ„ì¶œ","ìœ ì¶œ","í™”ì¬","ì ì","íŒŒì‚°","êµ¬ì¡°ì¡°ì •","ì •ë¦¬í•´ê³ ","ì¤‘ë‹¨","ì°¨ì§ˆ","ì‹¤íŒ¨","ë¶ˆë²•",
    "ì² ìˆ˜","í‡´ì¶œ","ë¶€ì •","ë¶ˆê³µì •","ê°‘ì§ˆ","ì§ì¥ê´´ë¡­í˜","í­ì–¸","íš¡í¬","í™˜ë¶ˆ","íšŒìˆ˜","ì†ì‹¤",
    "ê²½ê³ ","ì œì¬","í•´ì§€","ì·¨ì†Œ","ë‚™ì œ","ë¶€ê³¼","ì§•ê³„","ì¤‘ì§•ê³„","ë¶€ì •ì²­íƒ","ê²½ì˜ê¶Œë¶„ìŸ","ìœ„ê¸°","ì²­ì‚°"
}

POSITIVE_LEXICON = {
    "ì„±ì¥","í™•ëŒ€","ì¦ê°€","ê°œì„ ","í˜¸ì¡°","í‘ì","ìµœê³ ","ì„ ì •","ìˆ˜ìƒ","í¬ìƒ","ì‚°ì—…í¬ì¥",
    "ê°•í™”","ìƒìƒ","í˜‘ë ¥","ë„ì…","ì¶œì‹œ","ì„ ë„","ì¸ì¦","í™•ë³´","ìš°ìˆ˜","ë„ì•½","í™•ì¥","íšŒë³µ",
    "ê³ ë„í™”","ìµœì í™”","ì•ˆì •í™”","ì‹ ì„¤","ì±„ìš©","ì¦ì„¤","ì¦ì‚°","í™•ì¶©","ê³µê¸‰","ìˆ˜ì£¼","ëª¨ë²”",
    "ë‹¬ì„±","ì‹ ê¸°ìˆ ","ê°œì‹œ","ì¦ë¹™","ì„±ê³¼","ë§¤ì¶œì¦ê°€","ê³ ì„±ì¥","ì„ ë„ê¸°ì—…","ìˆ˜ì¶œí™•ëŒ€",
    "í•´ì™¸ì§„ì¶œ","íŒŒíŠ¸ë„ˆì‹­","ë¦¬ë”","í‰íŒ","ì¬ìƒì—ë„ˆì§€","ê°ì¶•","ì´í–‰","í˜ì‹ ","ê°œë°œ","ì—­ëŒ€",
    "ìˆœí•­","ê»‘ì¶©","ê¸°ì¦","ê¸°ë¶€","ì „ë‹¬","ì§€ì›","ìº í˜ì¸","í›„ì›"
}

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
MODEL_PATH = os.path.join(
    os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
    'models',
    'model_multinomialnb.joblib'
)

# ì •ê·œì‹ íŒ¨í„´ ì»´íŒŒì¼
_NEG_RE = re.compile("|".join(map(re.escape, sorted(NEGATIVE_LEXICON, key=len, reverse=True))))
_POS_RE = re.compile("|".join(map(re.escape, sorted(POSITIVE_LEXICON, key=len, reverse=True))))

def parse_pubdate(date_str: str) -> datetime:
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ íŒŒì‹±"""
    try:
        # 1) ISO í˜•ì‹
        try:
            return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        except ValueError:
            pass
        # 2) RSS í˜•ì‹
        try:
            from email.utils import parsedate_to_datetime
            return parsedate_to_datetime(date_str)
        except Exception:
            pass
        # 3) ì¼ë°˜ íŒŒì„œ
        return parser.parse(date_str)
    except Exception as e:
        logger.warning(f"âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ({date_str}): {str(e)}")
        return datetime.now()  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜

def extract_keywords(text: str, patt: re.Pattern) -> List[str]:
    if not isinstance(text, str):
        return []
    return sorted(set(patt.findall(text)))

def load_sentiment_model():
    """ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ"""
    try:
        logger.info(f"ğŸ¤– ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹œë„: {MODEL_PATH}")
        model = joblib.load(MODEL_PATH)
        logger.info("âœ… ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        return model
    except Exception as e:
        logger.error(f"âŒ ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None

def analyze_sentiment(model, articles: List[Article]) -> List[Dict[str, Any]]:
    """ê¸°ì‚¬ ê°ì„± ë¶„ì„ ìˆ˜í–‰"""
    try:
        analyzed_articles: List[Dict[str, Any]] = []
        for article in articles:
            try:
                title_text = article.title
                desc_text = article.description
                full_text = f"{title_text} {desc_text}"

                # í‚¤ì›Œë“œ ê¸°ë°˜
                neg_keywords = extract_keywords(full_text, _NEG_RE)
                pos_keywords = extract_keywords(full_text, _POS_RE)
                has_both = len(neg_keywords) > 0 and len(pos_keywords) > 0

                # ëª¨ë¸ ê¸°ë°˜
                if model is not None:
                    try:
                        y_pred = model.predict([full_text])[0]
                        probas = model.predict_proba([full_text])[0]

                        classes = getattr(model.named_steps.get("clf", model), "classes_", None)
                        if classes is None:
                            classes = getattr(model, "classes_", None)

                        if classes is not None and "negative" in classes:
                            neg_idx = int(np.where(classes == "negative")[0][0])
                            neg_proba = float(probas[neg_idx])
                        else:
                            neg_proba = 0.0

                        if y_pred == "negative" and has_both:
                            final_sentiment = "other"
                            final_basis = "ë¶€ì •+ê¸ì • ë™ì‹œ ì¶œí˜„ â†’ other"
                        else:
                            final_sentiment = y_pred
                            final_basis = "ëª¨ë¸ ì˜ˆì¸¡ ìœ ì§€"
                    except Exception as e:
                        logger.error(f"âŒ ëª¨ë¸ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                        final_sentiment = "negative" if len(neg_keywords) > len(pos_keywords) else "other"
                        final_basis = "í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨ (ëª¨ë¸ ì‹¤íŒ¨)"
                        neg_proba = 1.0 if final_sentiment == "negative" else 0.0
                else:
                    final_sentiment = "negative" if len(neg_keywords) > len(pos_keywords) else "other"
                    final_basis = "í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨ (ëª¨ë¸ ì—†ìŒ)"
                    neg_proba = 1.0 if final_sentiment == "negative" else 0.0

                analyzed_articles.append({
                    "title": title_text,
                    "description": desc_text,
                    "sentiment": final_sentiment,
                    "sentiment_confidence": float(neg_proba if final_sentiment == "negative" else (1 - neg_proba)),
                    "neg_keywords": ", ".join(neg_keywords),
                    "pos_keywords": ", ".join(pos_keywords),
                    "sentiment_basis": final_basis,
                    "original_category": article.original_category,
                    "issue": article.issue,
                    "pubDate": article.pubDate,
                    "originallink": article.originallink,
                    "company": article.company,
                })

            except Exception as e:
                logger.error(f"âŒ ê¸°ì‚¬ ê°ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}; ì œëª©: {getattr(article, 'title', '')}")
                continue

        return analyzed_articles
    except Exception as e:
        logger.error(f"âŒ ê°ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []

async def add_relevance_labels(
    articles: List[Dict[str, Any]],
    company_id: str,
    search_date: datetime,
    prev_year_categories: Set[str],   # (ê²€ìƒ‰ ê¸°ì¤€ì—°ë„ - 1)ì˜ category id ì§‘í•©
    reference_categories: Set[str],   # publish_year = NULL/''/'0' ì˜ category id ì§‘í•©
) -> List[Dict[str, Any]]:
    """
    ë¼ë²¨ ì •ì˜:
    - relevance : ì œëª©ì— ê¸°ì—…ëª… í¬í•¨ì´ë©´ '++' (True)
    - recent    : 3ê°œì›” ì´ë‚´=1.0, 3~6ê°œì›”=0.5, ê·¸ ì™¸=0.0
    - rank      : original_category âˆˆ prev_year_categories â†’ True
    - reference : original_category âˆˆ reference_categories â†’ True
    """
    try:
        repository = MiddleIssueRepository()
        
        for a in articles:
            a["relevance_label"] = False
            a["recent_value"] = 0.0
            a["rank_label"] = False
            a["reference_label"] = False
            a["label_reasons"] = []

            # relevance
            title = a.get("title") or ""
            if isinstance(title, str) and company_id and company_id in title:
                a["relevance_label"] = True
                a["label_reasons"].append("ì œëª©ì— ê¸°ì—…ëª… í¬í•¨")

            # recent
            pub_str = a.get("pubDate")
            if pub_str:
                try:
                    pub_dt = parse_pubdate(pub_str)
                    months_diff = (search_date - pub_dt).days / 30
                    if months_diff <= 3:
                        a["recent_value"] = 1.0
                        a["label_reasons"].append("ìµœê·¼ 3ê°œì›” ì´ë‚´")
                    elif months_diff <= 6:
                        a["recent_value"] = 0.5
                        a["label_reasons"].append("ìµœê·¼ 3~6ê°œì›”")
                except Exception as e:
                    logger.warning(f"âš ï¸ recent ê³„ì‚° ì¤‘ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {e}")

            # rank/reference (ì¹´í…Œê³ ë¦¬ ì´ë¦„ì„ IDë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ)
            oc = a.get("original_category")
            if oc is not None:
                try:
                    # ì¹´í…Œê³ ë¦¬ ì´ë¦„ì„ IDë¡œ ë³€í™˜
                    category_id = await repository.get_category_id_by_name(str(oc))
                    if category_id is not None:
                        oc_key = str(category_id)
                        
                        if oc_key in prev_year_categories:
                            a["rank_label"] = True
                            a["label_reasons"].append("ì´ì „ë…„ë„ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­")

                        if oc_key in reference_categories:
                            a["reference_label"] = True
                            a["label_reasons"].append("ê³µí†µ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­")
                    else:
                        logger.warning(f"âš ï¸ ì¹´í…Œê³ ë¦¬ ì´ë¦„ '{oc}'ì„ IDë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŒ")
                except Exception as e:
                    logger.warning(f"âš ï¸ ì¹´í…Œê³ ë¦¬ ID ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")

        return articles
    except Exception as e:
        logger.error(f"âŒ ë¼ë²¨ ë¶€ì—¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return articles

def calculate_category_scores(articles: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
    """
    ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°

    ì ìˆ˜ ì²´ê³„:
    - frequency_score: í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë¹ˆë„ (0~1)
    - relevance_score: ì¹´í…Œê³ ë¦¬ ê¸°ì‚¬ë“¤ì˜ relevance_label í‰ê·  (True=1, False=0)
    - recent_score   : ì¹´í…Œê³ ë¦¬ ê¸°ì‚¬ë“¤ì˜ recent_value í‰ê·  (1/0.5/0)
    - rank_score     : ì¹´í…Œê³ ë¦¬ ë‚´ rank_label ì¡´ì¬ ì—¬ë¶€(0/1)  â€» ì „ë¶€ ë™ì¼í•˜ë‹¤ëŠ” ê°€ì •
    - negative_score : ì¹´í…Œê³ ë¦¬ ë‚´ ë¶€ì • ê¸°ì‚¬ ë¹„ìœ¨ (0~1)
    - reference_score: ì¹´í…Œê³ ë¦¬ ë‚´ reference_label ì¡´ì¬ ì—¬ë¶€(0/1)

    ìµœì¢… ì ìˆ˜:
    final = 0.4*frequency
          + 0.6*relevance
          + 0.2*recent
          + 0.4*rank
          + 0.6*reference
          + 0.8*negative*(1 + 0.5*frequency + 0.5*relevance)
    """
    try:
        total_articles = len(articles)
        if total_articles == 0:
            return {}

        buckets: Dict[str, Dict[str, Any]] = {}
        for a in articles:
            cat = a.get("original_category")
            if cat is None:
                continue
            key = str(cat)
            b = buckets.setdefault(key, {
                "count": 0,
                "relevance_sum": 0.0,
                "recent_sum": 0.0,
                "negative_count": 0,
                "rank_label": None,
                "reference_label": None,
                "articles": []
            })

            b["count"] += 1
            b["articles"].append(a)
            b["relevance_sum"] += 1.0 if a.get("relevance_label") else 0.0
            b["recent_sum"] += float(a.get("recent_value", 0.0))
            if a.get("sentiment") == "negative":
                b["negative_count"] += 1
            if b["rank_label"] is None:
                b["rank_label"] = 1.0 if a.get("rank_label") else 0.0
            if b["reference_label"] is None:
                b["reference_label"] = 1.0 if a.get("reference_label") else 0.0

        results: Dict[str, Dict[str, Any]] = {}
        for key, b in buckets.items():
            c = b["count"]
            frequency = c / total_articles
            relevance = (b["relevance_sum"] / c) if c else 0.0
            recent = (b["recent_sum"] / c) if c else 0.0
            rank = b["rank_label"] or 0.0
            reference = b["reference_label"] or 0.0
            negative = (b["negative_count"] / c) if c else 0.0

            final_score = (
                0.4 * frequency
                + 0.6 * relevance
                + 0.2 * recent
                + 0.4 * rank
                + 0.6 * reference
                + 0.8 * negative * (1 + 0.5 * frequency + 0.5 * relevance)
            )

            results[key] = {
                "count": c,
                "frequency_score": round(frequency, 6),
                "relevance_score": round(relevance, 6),
                "recent_score": round(recent, 6),
                "rank_score": round(rank, 6),
                "reference_score": round(reference, 6),
                "negative_score": round(negative, 6),
                "final_score": round(final_score, 6),
                "articles": b["articles"],
            }

        return results
    except Exception as e:
        logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return {}

def rank_categories_by_score(category_scores: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ì¹´í…Œê³ ë¦¬ë¥¼ final_score ê¸°ì¤€ìœ¼ë¡œ ìˆœìœ„ ë§¤ê¸°ê¸°"""
    try:
        ranked = sorted(
            category_scores.items(),
            key=lambda x: x[1]["final_score"],
            reverse=True
        )
        out: List[Dict[str, Any]] = []
        for idx, (cat, scores) in enumerate(ranked, start=1):
            out.append({
                "rank": idx,
                "category": cat,
                **scores
            })
        return out
    except Exception as e:
        logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ ìˆœìœ„ ë§¤ê¸°ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []

async def match_categories_with_esg_and_issuepool(
    ranked_categories: List[Dict[str, Any]], 
    company_id: str, 
    search_year: int
) -> List[Dict[str, Any]]:
    """
    ì¹´í…Œê³ ë¦¬ë³„ë¡œ ESG ë¶„ë¥˜ì™€ base_issuepoolì„ ë§¤ì¹­
    
    ë§¤ì¹­ ê·œì¹™:
    1. ì¹´í…Œê³ ë¦¬ ì´ë¦„ìœ¼ë¡œ ì§ì ‘ ì¡°íšŒ (í† í°í™”/ë³„ì¹­ ë§¤í•‘ ì—†ìŒ)
    2. repositoryì˜ JOIN ê¸°ëŠ¥ì„ í™œìš©í•˜ì—¬ ëª¨ë“  ì •ë³´ë¥¼ í•œ ë²ˆì— ê°€ì ¸ì˜¤ê¸°
    3. ì¹´í…Œê³ ë¦¬ í•˜ë‚˜ë‹¹ ESG ë¶„ë¥˜ëŠ” í•˜ë‚˜, base_issuepoolì€ ì—¬ëŸ¬ ê°œ
    """
    try:
        repository = MiddleIssueRepository()
        matched_categories = []
        
        logger.info(f"ğŸ” ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ ì‹œì‘ - ê¸°ì—…: {company_id}, ì—°ë„: {search_year}")
        logger.info(f"ğŸ” ë§¤ì¹­í•  ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(ranked_categories)}")
        
        for category_info in ranked_categories:
            name_or_id = str(category_info['category'])
            
            try:
                # IDì¸ì§€ ì´ë¦„ì¸ì§€ êµ¬ë¶„í•˜ì—¬ ì²˜ë¦¬
                if name_or_id.isdigit():
                    # IDë¡œ ì¡°íšŒ
                    category_details = await repository.get_category_details(
                        corporation_name=company_id,
                        category_id=int(name_or_id),
                        year=search_year,   # repoê°€ ë‚´ë¶€ì—ì„œ -1 ì²˜ë¦¬
                    )
                else:
                    # ì´ë¦„ìœ¼ë¡œ ì§ì ‘ ì¡°íšŒ
                    category_details = await repository.get_category_by_name_direct(
                        corporation_name=company_id,
                        category_name=name_or_id,
                        year=search_year,   # repoê°€ ë‚´ë¶€ì—ì„œ -1 ì²˜ë¦¬í•˜ë„ë¡ ìœ„ì—ì„œ ìˆ˜ì •
                    )
                
                if category_details:
                    # ì´ë¯¸ ëª¨ë“  ì •ë³´ê°€ í¬í•¨ëœ CategoryDetailsResponseì—ì„œ ì¶”ì¶œ
                    esg_classification = category_details.esg_classification_name or 'ë¯¸ë¶„ë¥˜'
                    esg_classification_id = category_details.esg_classification_id
                    
                    # BaseIssuePool ê°ì²´ë¥¼ dictë¡œ ë³€í™˜
                    base_issuepools = []
                    if category_details.base_issuepools:
                        for issue in category_details.base_issuepools:
                            # BaseIssuePool ê°ì²´ì˜ ì†ì„±ì— ì§ì ‘ ì ‘ê·¼
                            base_issuepools.append({
                                "id": issue.id,
                                "base_issue_pool": issue.base_issue_pool,
                                "issue_pool": issue.issue_pool,
                                "ranking": issue.ranking,
                                "esg_classification_id": esg_classification_id,
                                "esg_classification_name": esg_classification
                            })
                    
                    # ë§¤ì¹­ëœ ì¹´í…Œê³ ë¦¬ ì •ë³´ ìƒì„±
                    matched_category = {
                        **category_info,  # ê¸°ì¡´ ì ìˆ˜ ì •ë³´ ìœ ì§€
                        "esg_classification": esg_classification,
                        "esg_classification_id": esg_classification_id,
                        "base_issuepools": base_issuepools,
                        "total_issuepools": len(base_issuepools)
                    }
                    
                    matched_categories.append(matched_category)
                    
                    # ì¹´í…Œê³ ë¦¬-ESG ë§¤í•‘ ë° base issuepool ë§¤í•‘ ê²°ê³¼ ë¡œê·¸
                    logger.info(f"âœ… ì¹´í…Œê³ ë¦¬ '{name_or_id}' ë§¤ì¹­ ì™„ë£Œ:")
                    logger.info(f"   - ESG ë¶„ë¥˜: {esg_classification} (ID: {esg_classification_id})")
                    logger.info(f"   - Base IssuePool ìˆ˜: {len(base_issuepools)}ê°œ")
                    if base_issuepools:
                        for i, pool in enumerate(base_issuepools[:3]):  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                            logger.info(f"     {i+1}. {pool['base_issue_pool']} (ìˆœìœ„: {pool['ranking']})")
                        if len(base_issuepools) > 3:
                            logger.info(f"     ... ì™¸ {len(base_issuepools) - 3}ê°œ")
                else:
                    # ë§¤ì¹­ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
                    matched_category = {
                        **category_info,
                        "esg_classification": "ë¯¸ë¶„ë¥˜",
                        "esg_classification_id": None,
                        "base_issuepools": [],
                        "total_issuepools": 0
                    }
                    matched_categories.append(matched_category)
                    
                    logger.warning(f"âš ï¸ ì¹´í…Œê³ ë¦¬ '{name_or_id}' ë§¤ì¹­ ì‹¤íŒ¨: ESG ë¶„ë¥˜ ë° ì´ìŠˆí’€ ì •ë³´ ì—†ìŒ")
                    
            except Exception as e:
                logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ '{name_or_id}' ë§¤ì¹­ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •
                matched_category = {
                    **category_info,
                    "esg_classification": "ë¯¸ë¶„ë¥˜",
                    "esg_classification_id": None,
                    "base_issuepools": [],
                    "total_issuepools": 0
                }
                matched_categories.append(matched_category)
        
        logger.info(f"ğŸ”— ì´ {len(matched_categories)}ê°œ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ ì™„ë£Œ")
        return matched_categories
        
    except Exception as e:
        logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ ì¤‘ ì „ì²´ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.error(f"âŒ ì˜¤ë¥˜ ìƒì„¸: {type(e).__name__}: {str(e)}")
        import traceback
        logger.error(f"âŒ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì›ë³¸ ì¹´í…Œê³ ë¦¬ ì •ë³´ ë°˜í™˜
        return ranked_categories

async def start_assessment(request: MiddleIssueRequest) -> Dict[str, Any]:
    """
    ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘ - í¬ë¡¤ë§ ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ ì‹œì‘
    """
    try:
        # 1) ìš”ì²­ ë¡œê¹…
        logger.info("="*50)
        logger.info("ğŸš€ ìƒˆë¡œìš´ ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘")
        logger.info(f"ê¸°ì—…ëª…: {request.company_id}")
        logger.info(f"ë³´ê³ ê¸°ê°„: {request.report_period}")
        logger.info(f"ìš”ì²­ íƒ€ì…: {request.request_type}")
        logger.info(f"íƒ€ì„ìŠ¤íƒ¬í”„: {request.timestamp}")
        logger.info(f"ì´ í¬ë¡¤ë§ ê¸°ì‚¬ ìˆ˜: {request.total_results}")
        logger.info("-"*50)

        # 2) ëª¨ë¸ ë¡œë“œ
        model = load_sentiment_model()
        if not model:
            raise Exception("ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")

        # 3) ê°ì„± ë¶„ì„
        logger.info("ğŸ“Š í¬ë¡¤ë§ ë°ì´í„° ê°ì„± ë¶„ì„ ì‹œì‘")
        analyzed_articles = analyze_sentiment(model, request.articles)

        # 4) (ê²€ìƒ‰ ê¸°ì¤€ì—°ë„ - 1) & ê³µí†µ(NULL) ì¹´í…Œê³ ë¦¬ ì¡°íšŒ
        repository = MiddleIssueRepository()
        search_year = int(request.report_period["end_date"][:4])  # ê²€ìƒ‰ ê¸°ì¤€ì—°ë„ (YYYY)
        
        # repository ë‚´ë¶€ì—ì„œ -1 ì²˜ë¦¬í•˜ë¯€ë¡œ search_yearë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬
        corp_issues_prev = await repository.get_corporation_issues(
            corporation_name=request.company_id,
            year=search_year  # repository ë‚´ë¶€ì—ì„œ -1 ì²˜ë¦¬
        )
        # 2. prev_year_categoriesì™€ reference_categories ìˆ˜ì§‘
        # prev_year ê¸°ì¤€ ì¹´í…Œê³ ë¦¬ì™€ ê³µí†µ(NULL/ë¹ˆë¬¸ìì—´/'0') ì¹´í…Œê³ ë¦¬ ì„¸íŠ¸
        prev_year_categories = {str(issue.category_id) for issue in corp_issues_prev.year_issues}
        reference_categories = {str(issue.category_id) for issue in corp_issues_prev.common_issues}
        
        logger.info(f"ğŸ” prev_year_categories: {len(prev_year_categories)}ê°œ - {prev_year_categories}")
        logger.info(f"ğŸ” reference_categories: {len(reference_categories)}ê°œ - {reference_categories}")

        # 5) ë¼ë²¨ ë¶€ì—¬
        logger.info("ğŸ·ï¸ ë¼ë²¨(relevance/recent/rank/reference) ë¶€ì—¬ ì‹œì‘")
        search_date = datetime.now()
        labeled_articles = await add_relevance_labels(
            analyzed_articles,
            request.company_id,
            search_date,
            prev_year_categories,
            reference_categories
        )

        # 6) ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°
        logger.info("ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚° ì‹œì‘")
        category_scores = calculate_category_scores(labeled_articles)

        # 7) ì¹´í…Œê³ ë¦¬ ë­í‚¹
        logger.info("ğŸ† ì¹´í…Œê³ ë¦¬ ìˆœìœ„ ë§¤ê¸°ê¸° ì‹œì‘")
        ranked_categories = rank_categories_by_score(category_scores)

        # 8) ì¹´í…Œê³ ë¦¬ë³„ ESG ë¶„ë¥˜ ë° ì´ìŠˆí’€ ë§¤ì¹­
        logger.info("ğŸ”— ì¹´í…Œê³ ë¦¬ë³„ ESG ë¶„ë¥˜ ë° ì´ìŠˆí’€ ë§¤ì¹­ ì‹œì‘")
        matched_categories = await match_categories_with_esg_and_issuepool(
            ranked_categories, 
            request.company_id, 
            search_year
        )

        # 9) í†µê³„/ë¡œê¹…
        negative_count = sum(1 for a in labeled_articles if a["sentiment"] == "negative")
        logger.info(f"ğŸ“Š ë¶„ì„ ì™„ë£Œ í†µê³„:")
        logger.info(f"   - ë¶„ì„ëœ ê¸°ì‚¬ ìˆ˜: {len(labeled_articles)}")
        logger.info(f"   - ë¶€ì •ì  ê¸°ì‚¬ ìˆ˜: {negative_count}")
        logger.info(f"   - ë¶„ì„ëœ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(category_scores)}")
        logger.info(f"   - ë§¤ì¹­ëœ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(matched_categories)}")

        # ğŸ”¥ ìµœì¢… ì¹´í…Œê³ ë¦¬ ìˆœìœ„ (ESG ë¶„ë¥˜ ë° base issuepool ë§¤ì¹­ ê²°ê³¼)
        logger.info("\nğŸ† ìµœì¢… ì¹´í…Œê³ ë¦¬ ìˆœìœ„ (ESG ë¶„ë¥˜ ë° base issuepool ë§¤ì¹­ ì™„ë£Œ):")
        for i, row in enumerate(matched_categories[:10]):  # ìƒìœ„ 10ê°œ
            esg_name = row.get('esg_classification', 'ë¯¸ë¶„ë¥˜')
            issue_count = len(row.get('base_issuepools', []))
            final_score = row.get('final_score', 0.0)
            logger.info(
                f"{i+1:>2}ìœ„ | ì¹´í…Œê³ ë¦¬: {row['category']} | ESG: {esg_name} | "
                f"ì´ìŠˆí’€: {issue_count}ê°œ | ìµœì¢…ì ìˆ˜: {final_score:.3f}"
            )
            
            # base issuepool ìƒì„¸ ì •ë³´ (ìƒìœ„ 3ê°œë§Œ)
            base_issuepools = row.get('base_issuepools', [])
            if base_issuepools:
                for j, pool in enumerate(base_issuepools[:3]):
                    logger.info(f"     {j+1}. {pool.get('base_issue_pool', 'N/A')} (ìˆœìœ„: {pool.get('ranking', 'N/A')})")
                if len(base_issuepools) > 3:
                    logger.info(f"     ... ì™¸ {len(base_issuepools) - 3}ê°œ")

        # ğŸ”¥ ì „ì²´ ì¹´í…Œê³ ë¦¬ ìˆœìœ„ ìš”ì•½
        logger.info(f"\nğŸ“‹ ì „ì²´ {len(matched_categories)}ê°œ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ ì™„ë£Œ")
        logger.info("="*50)

        # 9) ì‘ë‹µ
        response_data = {
            "success": True,
            "message": "ì¤‘ëŒ€ì„± í‰ê°€ ë°ì´í„° ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "data": {
                "company_id": request.company_id,
                "report_period": request.report_period,
                "assessment_status": "analyzed",
                "total_articles": len(labeled_articles),
                "negative_articles": negative_count,
                "negative_ratio": (negative_count / len(labeled_articles))*100 if labeled_articles else 0.0,
                "total_categories": len(category_scores),
                "matched_categories": matched_categories,  # ESG ë¶„ë¥˜ ë° ì´ìŠˆí’€ ë§¤ì¹­ëœ ì¹´í…Œê³ ë¦¬
                "ranked_categories": ranked_categories[:20],  # ìƒìœ„ 20ê°œ (ì›ë³¸)
                # í•„ìš” ì‹œ í”„ë¡ íŠ¸ ë””ë²„ê¹…/ë¦¬ë·°ìš© ì›ìë£Œ
                "category_scores": category_scores,
                "analyzed_samples": labeled_articles[:3],
            }
        }

        logger.info("âœ… ë°ì´í„° ë¶„ì„ ì™„ë£Œ")
        logger.info("="*50)
        return response_data

    except Exception as e:
        error_msg = f"âŒ ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(error_msg)
        logger.error("="*50)
        return {"success": False, "message": error_msg, "data": None}
