"""
Middleissue Service - ì¤‘ëŒ€ì„± í‰ê°€ ê´€ë ¨ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì²˜ë¦¬
í¬ë¡¤ë§ ë°ì´í„° ì²˜ë¦¬, ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì ìš©, ì ìˆ˜ ê³„ì‚° ë“±ì„ ë‹´ë‹¹
"""
# 1. í¬ë¡¤ë§í•œ ì „ì²´ ë°ì´í„° -> ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë¡œ ê¸ë¶€ì •í‰ê°€
# 2. relevance, recent, negative, rank, ê¸°ì¤€ì„œ/í‰ê°€ê¸°ê´€ ì§€í‘œ íŒë‹¨
# 3. ê° ì§€í‘œë³„ score ë¶€ì—¬
# 4. final score ê³„ì‚°
# 5. frontendë¡œ ë³´ë‚´ê³  ë©”ëª¨ë¦¬ ì €ì¥

import logging
import json
import os
import joblib
from datetime import datetime, timedelta
from typing import Dict, Any, List, Set, Tuple
from app.domain.middleissue.schema import MiddleIssueRequest, MiddleIssueResponse, Article
from app.domain.middleissue.repository import MiddleIssueRepository
import re
import numpy as np
from dateutil import parser

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

NEGATIVE_LEXICON = {
    "ê°ì†Œ","í•˜ë½","ë¶€ì§„","ì•…í™”","ì˜¤ì—¼","ìœ„ë°˜","ë‹´í•©","ë¶€íŒ¨","ë‡Œë¬¼","íš¡ë ¹","ë°°ì„","ì‚¬ê¸°",
    "ê³¼ì§•ê¸ˆ","ë²Œê¸ˆ","ì‚¬ê³ ","ì‚¬ë§","íŒŒì—…","ë¶„ìŸ","ê°ˆë“±","ë…¼ë€","ì†Œì†¡","ë¦¬ì½œ","ê²°í•¨","ë¶ˆëŸ‰",
    "ëˆ„ì¶œ","ìœ ì¶œ","í™”ì¬","ì ì","íŒŒì‚°","êµ¬ì¡°ì¡°ì •","ì •ë¦¬í•´ê³ ","ì¤‘ë‹¨","ì°¨ì§ˆ","ì‹¤íŒ¨","ë¶ˆë²•",
    "ì² ìˆ˜","í‡´ì¶œ","ë¶€ì •","ë¶ˆê³µì •","ê°‘ì§ˆ","ì§ì¥ê´´ë¡­í˜","í­ì–¸","íš¡í¬","í™˜ë¶ˆ","íšŒìˆ˜","ì†ì‹¤",
    "ê²½ê³ ","ì œì¬","í•´ì§€","ì·¨ì†Œ","ë‚™ì œ","ë¶€ê³¼","ì§•ê³„","ì¤‘ì§•ê³„","ë¶€ì •ì²­íƒ","ê²½ì˜ê¶Œë¶„ìŸ","ìœ„ê¸°","ì²­ì‚°"
}

POSITIVE_LEXICON = {
    "ì„±ì¥","í™•ëŒ€","ì¦ê°€","ê°œì„ ","í˜¸ì¡°","í‘ì","ìµœê³ ","ì„ ì •","ìˆ˜ìƒ","í¬ìƒ","ì‚°ì—…í¬ì¥",
    "ê°•í™”","ìƒìƒ","í˜‘ë ¥","ë„ì…","ì¶œì‹œ","ì„ ë„","ì¸ì¦","í™•ë³´","ìš°ìˆ˜","ë„ì•½","í™•ì¥","íšŒë³µ",
    "ê³ ë„í™”","ìµœì í™”","ì•ˆì •í™”","ì‹ ì„¤","ì±„ìš©","ì¦ì„¤","ì¦ì‚°","í™•ì¶©","ê³µê¸‰","ìˆ˜ì£¼","ëª¨ë²”",
    "ë‹¬ì„±","ì‹ ê¸°ìˆ ","ê°œì‹œ","ì¦ë¹™","ì„±ê³¼","ë§¤ì¶œì¦ê°€","ê³ ì„±ì¥","ì„ ë„ê¸°ì—…","ìˆ˜ì¶œí™•ëŒ€",
    "í•´ì™¸ì§„ì¶œ","íŒŒíŠ¸ë„ˆì‹­","ë¦¬ë”","í‰íŒ","ì¬ìƒì—ë„ˆì§€","ê°ì¶•","ì´í–‰","í˜ì‹ ","ê°œë°œ","ì—­ëŒ€",
    "ìˆœí•­","ê»‘ì¶©","ê¸°ì¦","ê¸°ë¶€","ì „ë‹¬","ì§€ì›","ìº í˜ì¸","í›„ì›"
}

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
MODEL_PATH = os.path.join(
    os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
    'models',
    'model_multinomialnb.joblib'
)

# ì •ê·œì‹ íŒ¨í„´ ì»´íŒŒì¼
_NEG_RE = re.compile("|".join(map(re.escape, sorted(NEGATIVE_LEXICON, key=len, reverse=True))))
_POS_RE = re.compile("|".join(map(re.escape, sorted(POSITIVE_LEXICON, key=len, reverse=True))))

def parse_pubdate(date_str: str) -> datetime:
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ íŒŒì‹±"""
    try:
        # 1. ISO í˜•ì‹ ì‹œë„
        try:
            return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        except ValueError:
            pass

        # 2. RSS í˜•ì‹ ì‹œë„ (ì˜ˆ: 'Wed, 06 Aug 2023 12:30:00 +0900')
        try:
            from email.utils import parsedate_to_datetime
            return parsedate_to_datetime(date_str)
        except Exception:
            pass

        # 3. ê¸°íƒ€ ì¼ë°˜ì ì¸ í˜•ì‹ë“¤ ì‹œë„
        return parser.parse(date_str)

    except Exception as e:
        logger.warning(f"âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ({date_str}): {str(e)}")
        return datetime.now()  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜

def extract_keywords(text: str, patt: re.Pattern) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    if not isinstance(text, str):
        return []
    return sorted(set(patt.findall(text)))

def load_sentiment_model():
    """ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ"""
    try:
        logger.info(f"ğŸ¤– ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹œë„: {MODEL_PATH}")
        model = joblib.load(MODEL_PATH)
        logger.info("âœ… ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        return model
    except Exception as e:
        logger.error(f"âŒ ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None

def analyze_sentiment(model, articles: List[Article]) -> List[Dict[str, Any]]:
    """ê¸°ì‚¬ ê°ì„± ë¶„ì„ ìˆ˜í–‰"""
    try:
        analyzed_articles = []
        
        for article in articles:
            try:
                # 1. í…ìŠ¤íŠ¸ ì¤€ë¹„
                title_text = article.title
                desc_text = article.description
                full_text = f"{title_text} {desc_text}"
                
                # 2. í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„
                neg_keywords = extract_keywords(full_text, _NEG_RE)
                pos_keywords = extract_keywords(full_text, _POS_RE)
                has_both = len(neg_keywords) > 0 and len(pos_keywords) > 0
                
                # 3. ëª¨ë¸ ê¸°ë°˜ ë¶„ì„
                if model is not None:
                    try:
                        # ì˜ˆì¸¡ ë° í™•ë¥  ê³„ì‚°
                        y_pred = model.predict([full_text])[0]
                        probas = model.predict_proba([full_text])[0]
                        
                        # negative í´ë˜ìŠ¤ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
                        classes = getattr(model.named_steps["clf"], "classes_", None)
                        if classes is None:
                            classes = getattr(model, "classes_", None)
                        
                        if classes is not None and "negative" in classes:
                            neg_idx = int(np.where(classes == "negative")[0][0])
                            neg_proba = probas[neg_idx]
                        else:
                            neg_proba = 0.0
                            
                        # ë¶€ì •+ê¸ì • ë™ì‹œ ì¶œí˜„ ì‹œ otherë¡œ ë³€ê²½
                        if y_pred == "negative" and has_both:
                            final_sentiment = "other"
                            final_basis = "ë¶€ì •+ê¸ì • ë™ì‹œ ì¶œí˜„ â†’ other"
                        else:
                            final_sentiment = y_pred
                            final_basis = "ëª¨ë¸ ì˜ˆì¸¡ ìœ ì§€"
                            
                    except Exception as e:
                        logger.error(f"âŒ ëª¨ë¸ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                        # ëª¨ë¸ ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œë§Œ íŒë‹¨
                        final_sentiment = "negative" if len(neg_keywords) > len(pos_keywords) else "other"
                        final_basis = "í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨ (ëª¨ë¸ ì‹¤íŒ¨)"
                        neg_proba = 1.0 if final_sentiment == "negative" else 0.0
                else:
                    # ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš° í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œë§Œ íŒë‹¨
                    final_sentiment = "negative" if len(neg_keywords) > len(pos_keywords) else "other"
                    final_basis = "í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨ (ëª¨ë¸ ì—†ìŒ)"
                    neg_proba = 1.0 if final_sentiment == "negative" else 0.0
                
                analyzed_articles.append({
                    "title": title_text,
                    "description": desc_text,
                    "sentiment": final_sentiment,
                    "sentiment_confidence": neg_proba if final_sentiment == "negative" else (1 - neg_proba),
                    "neg_keywords": ", ".join(neg_keywords),
                    "pos_keywords": ", ".join(pos_keywords),
                    "sentiment_basis": final_basis,
                    "original_category": article.original_category,
                    "issue": article.issue,
                    "pubDate": article.pubDate,
                    "originallink": article.originallink,
                    "company": article.company,
                    "relevance_score": 0.0  # ê´€ë ¨ì„± ì ìˆ˜ ì´ˆê¸°í™”
                })
                
            except Exception as e:
                logger.error(f"âŒ ê¸°ì‚¬ ê°ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                logger.error(f"ë¬¸ì œëœ ê¸°ì‚¬ ì œëª©: {article.title}")
                continue

        return analyzed_articles
    except Exception as e:
        logger.error(f"âŒ ê°ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []

async def add_relevance_labels(
    articles: List[Dict[str, Any]], 
    company_id: str,
    search_date: datetime,
    year_categories: Set[str],
    common_categories: Set[str]
) -> List[Dict[str, Any]]:
    """
    ê¸°ì‚¬ì— ê´€ë ¨ì„± ë¼ë²¨ ì¶”ê°€ ë° ì ìˆ˜ ê³„ì‚°
    
    ë¼ë²¨ ì²´ê³„:
    - relevance: titleì— ê¸°ì—…ëª… í¬í•¨ ì—¬ë¶€ (++ ë˜ëŠ” ì—†ìŒ)
    - recent: pubdate ìµœì‹ ì„± (++: 3ê°œì›” ì´ë‚´, +: 3-6ê°œì›”, ì—†ìŒ)
    - rank: year-1 ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ ì—¬ë¶€ (++ ë˜ëŠ” ì—†ìŒ)
    - reference: publish_year null ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ ì—¬ë¶€ (++ ë˜ëŠ” ì—†ìŒ)
    """
    try:
        for article in articles:
            # ë¼ë²¨ ì´ˆê¸°í™”
            article["relevance"] = "ì—†ìŒ"
            article["recent"] = "ì—†ìŒ"
            article["rank"] = "ì—†ìŒ"
            article["reference"] = "ì—†ìŒ"
            article["label_reasons"] = []

            # 1. relevance: ì œëª©ì— ê¸°ì—…ëª… í¬í•¨ ì—¬ë¶€
            if company_id in article["title"]:
                article["relevance"] = "++"
                article["label_reasons"].append("ì œëª©ì— ê¸°ì—…ëª… í¬í•¨")

            # 2. recent: ë°œí–‰ì¼ ê¸°ì¤€ ìµœì‹ ì„±
            if article["pubDate"]:
                try:
                    pub_date = parse_pubdate(article["pubDate"])
                    months_diff = (search_date - pub_date).days / 30
                    
                    if months_diff <= 3:
                        article["recent"] = "++"
                        article["label_reasons"].append("ìµœê·¼ 3ê°œì›” ì´ë‚´")
                    elif months_diff <= 6:
                        article["recent"] = "+"
                        article["label_reasons"].append("ìµœê·¼ 3-6ê°œì›”")
                except Exception as e:
                    logger.warning(f"âš ï¸ ë°œí–‰ì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

            # 3. rank: year-1 ì¹´í…Œê³ ë¦¬ ë§¤ì¹­
            if article["original_category"] and article["original_category"] in year_categories:
                article["rank"] = "++"
                article["label_reasons"].append("year-1 ì¹´í…Œê³ ë¦¬ ë§¤ì¹­")

            # 4. reference: publish_year null ì¹´í…Œê³ ë¦¬ ë§¤ì¹­
            if article["original_category"] and article["original_category"] in common_categories:
                article["reference"] = "++"
                article["label_reasons"].append("ê³µí†µ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­")

        return articles
    except Exception as e:
        logger.error(f"âŒ ê´€ë ¨ì„± ë¼ë²¨ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return articles

def calculate_category_scores(articles: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
    """
    ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°
    
    ì ìˆ˜ ì²´ê³„:
    - frequency_score: í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë¹ˆë„ (0~1)
    - relevance_score: relevance ++ë©´ 1ì , ì•„ë‹ˆë©´ 0ì 
    - recent_score: recent ++ë©´ 1ì , +ë©´ 0.5ì , ì•„ë‹ˆë©´ 0ì 
    - rank_score: rank ++ë©´ 1ì , ì•„ë‹ˆë©´ 0ì 
    - negative_score: í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ë¶€ì •ì  ê¸°ì‚¬ ë¹„ìœ¨ (0~1)
    - reference_score: reference ++ë©´ 1ì , ì•„ë‹ˆë©´ 0ì 
    """
    try:
        total_articles = len(articles)
        if total_articles == 0:
            return {}

        # ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ìˆ˜ì§‘
        category_data = {}
        
        for article in articles:
            category = article.get("original_category", "ë¯¸ë¶„ë¥˜")
            if category not in category_data:
                category_data[category] = {
                    "count": 0,
                    "relevance_count": 0,
                    "recent_plus_plus": 0,
                    "recent_plus": 0,
                    "rank_count": 0,
                    "reference_count": 0,
                    "negative_count": 0,
                    "articles": []
                }
            
            category_data[category]["count"] += 1
            category_data[category]["articles"].append(article)
            
            # ê° ë¼ë²¨ë³„ ì¹´ìš´íŠ¸
            if article.get("relevance") == "++":
                category_data[category]["relevance_count"] += 1
            
            if article.get("recent") == "++":
                category_data[category]["recent_plus_plus"] += 1
            elif article.get("recent") == "+":
                category_data[category]["recent_plus"] += 1
            
            if article.get("rank") == "++":
                category_data[category]["rank_count"] += 1
            
            if article.get("reference") == "++":
                category_data[category]["reference_count"] += 1
            
            if article.get("sentiment") == "negative":
                category_data[category]["negative_count"] += 1

        # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°
        category_scores = {}
        
        for category, data in category_data.items():
            count = data["count"]
            
            # 1. frequency_score: í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë¹ˆë„ (0~1)
            frequency_score = count / total_articles
            
            # 2. relevance_score: relevance ++ë©´ 1ì , ì•„ë‹ˆë©´ 0ì 
            relevance_score = 1.0 if data["relevance_count"] > 0 else 0.0
            
            # 3. recent_score: recent ++ë©´ 1ì , +ë©´ 0.5ì , ì•„ë‹ˆë©´ 0ì 
            recent_score = 0.0
            if data["recent_plus_plus"] > 0:
                recent_score = 1.0
            elif data["recent_plus"] > 0:
                recent_score = 0.5
            
            # 4. rank_score: rank ++ë©´ 1ì , ì•„ë‹ˆë©´ 0ì 
            rank_score = 1.0 if data["rank_count"] > 0 else 0.0
            
            # 5. negative_score: í•´ë‹¹ ì¹´í…Œê³ ë¦¬ì˜ ë¶€ì •ì  ê¸°ì‚¬ ë¹„ìœ¨ (0~1)
            negative_score = data["negative_count"] / count if count > 0 else 0.0
            
            # 6. reference_score: reference ++ë©´ 1ì , ì•„ë‹ˆë©´ 0ì 
            reference_score = 1.0 if data["reference_count"] > 0 else 0.0
            
            # 7. final_score ê³„ì‚°
            final_score = (
                0.4 * frequency_score +
                0.6 * relevance_score +
                0.2 * recent_score +
                0.4 * rank_score +
                0.6 * reference_score +
                0.8 * negative_score * (1 + 0.5 * frequency_score + 0.5 * relevance_score)
            )
            
            category_scores[category] = {
                "count": count,
                "frequency_score": frequency_score,
                "relevance_score": relevance_score,
                "recent_score": recent_score,
                "rank_score": rank_score,
                "negative_score": negative_score,
                "reference_score": reference_score,
                "final_score": final_score,
                "articles": data["articles"]
            }
        
        return category_scores
        
    except Exception as e:
        logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return {}

def rank_categories_by_score(category_scores: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ì¹´í…Œê³ ë¦¬ë¥¼ final_score ê¸°ì¤€ìœ¼ë¡œ ìˆœìœ„ ë§¤ê¸°ê¸°
    """
    try:
        # final_score ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        ranked_categories = sorted(
            category_scores.items(),
            key=lambda x: x[1]["final_score"],
            reverse=True
        )
        
        # ìˆœìœ„ ì •ë³´ ì¶”ê°€
        ranked_result = []
        for rank, (category, scores) in enumerate(ranked_categories, 1):
            ranked_result.append({
                "rank": rank,
                "category": category,
                **scores
            })
        
        return ranked_result
        
    except Exception as e:
        logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ ìˆœìœ„ ë§¤ê¸°ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []

async def start_assessment(request: MiddleIssueRequest) -> Dict[str, Any]:
    """
    ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘ - í¬ë¡¤ë§ ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ ì‹œì‘
    
    Args:
        request: ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘ ìš”ì²­ ë°ì´í„° (MiddleIssueRequest)
        
    Returns:
        Dict[str, Any]: ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘ ì‘ë‹µ
    """
    try:
        # 1. ìš”ì²­ ë°ì´í„° ë¡œê¹…
        logger.info("="*50)
        logger.info("ğŸš€ ìƒˆë¡œìš´ ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘")
        logger.info(f"ê¸°ì—…ëª…: {request.company_id}")
        logger.info(f"ë³´ê³ ê¸°ê°„: {request.report_period}")
        logger.info(f"ìš”ì²­ íƒ€ì…: {request.request_type}")
        logger.info(f"íƒ€ì„ìŠ¤íƒ¬í”„: {request.timestamp}")
        logger.info(f"ì´ í¬ë¡¤ë§ ê¸°ì‚¬ ìˆ˜: {request.total_results}")
        
        # í¬ë¡¤ë§ ë°ì´í„° êµ¬ì¡° í™•ì¸
        logger.info("-"*50)
        logger.info("ğŸ“‹ í¬ë¡¤ë§ ë°ì´í„° êµ¬ì¡° í™•ì¸")
        
        if request.articles and len(request.articles) > 0:
            sample_article = request.articles[0]
            logger.info("ìˆ˜ì§‘ëœ ë°ì´í„° í•„ë“œ:")
            
            # í•„ìˆ˜ í•„ë“œ
            logger.info("í•„ìˆ˜ í•„ë“œ:")
            logger.info(f"- title âœ“")
            logger.info(f"- description âœ“")
            logger.info(f"- company âœ“")
            
            # ì„ íƒì  í•„ë“œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            logger.info("\nì„ íƒì  í•„ë“œ:")
            logger.info(f"- originallink {'âœ“' if sample_article.originallink else 'âœ—'}")
            logger.info(f"- pubDate {'âœ“' if sample_article.pubDate else 'âœ—'}")
            logger.info(f"- issue {'âœ“' if sample_article.issue else 'âœ—'}")
            logger.info(f"- original_category {'âœ“' if sample_article.original_category else 'âœ—'}")
            logger.info(f"- query_kind {'âœ“' if sample_article.query_kind else 'âœ—'}")
            logger.info(f"- keyword {'âœ“' if sample_article.keyword else 'âœ—'}")
            
            # ì˜ˆì‹œ ë°ì´í„° êµ¬ì¡°
            logger.info("\në°ì´í„° êµ¬ì¡° ì˜ˆì‹œ:")
            logger.info(f"- title: [ì œëª© í…ìŠ¤íŠ¸...]")
            logger.info(f"- description: [ë³¸ë¬¸ í…ìŠ¤íŠ¸...]")
            logger.info(f"- company: {sample_article.company}")
            if sample_article.original_category:
                logger.info(f"- original_category: {sample_article.original_category}")
            if sample_article.query_kind:
                logger.info(f"- query_kind: {sample_article.query_kind}")
            if sample_article.keyword:
                logger.info(f"- keyword: {sample_article.keyword}")
        else:
            logger.warning("âš ï¸ í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤!")
            
        logger.info("-"*50)

        # 2. ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ
        model = load_sentiment_model()
        if not model:
            raise Exception("ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")

        # 3. í¬ë¡¤ë§ ë°ì´í„° ê°ì„± ë¶„ì„
        logger.info("ğŸ“Š í¬ë¡¤ë§ ë°ì´í„° ê°ì„± ë¶„ì„ ì‹œì‘")
        analyzed_articles = analyze_sentiment(model, request.articles)
        
        # 4. ì¹´í…Œê³ ë¦¬ ë°ì´í„° ì¡°íšŒ
        repository = MiddleIssueRepository()
        search_year = int(request.report_period["end_date"][:4])  # ê²€ìƒ‰ ì—°ë„
        corporation_issues = await repository.get_corporation_issues(
            corporation_name=request.company_id,
            year=search_year
        )

        # ì¹´í…Œê³ ë¦¬ ì„¸íŠ¸ ìƒì„±
        year_categories = {str(issue.category_id) for issue in corporation_issues.year_issues}
        common_categories = {str(issue.category_id) for issue in corporation_issues.common_issues}

        # 5. ê´€ë ¨ì„± ë¼ë²¨ ì¶”ê°€
        logger.info("ğŸ·ï¸ ê´€ë ¨ì„± ë¼ë²¨ ì¶”ê°€ ì‹œì‘")
        search_date = datetime.now()
        labeled_articles = await add_relevance_labels(
            analyzed_articles,
            request.company_id,
            search_date,
            year_categories,
            common_categories
        )

        # 6. ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°
        logger.info("ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚° ì‹œì‘")
        category_scores = calculate_category_scores(labeled_articles)
        
        # 7. ì¹´í…Œê³ ë¦¬ ìˆœìœ„ ë§¤ê¸°ê¸°
        logger.info("ğŸ† ì¹´í…Œê³ ë¦¬ ìˆœìœ„ ë§¤ê¸°ê¸° ì‹œì‘")
        ranked_categories = rank_categories_by_score(category_scores)
        
        # 8. ë¶„ì„ ê²°ê³¼ ë¡œê¹…
        negative_count = sum(1 for article in labeled_articles if article["sentiment"] == "negative")
        
        logger.info(f"ë¶„ì„ëœ ê¸°ì‚¬ ìˆ˜: {len(labeled_articles)}")
        logger.info(f"ë¶€ì •ì  ê¸°ì‚¬ ìˆ˜: {negative_count}")
        logger.info(f"ë¶„ì„ëœ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(category_scores)}")
        
        # 9. ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ìƒì„¸ ë¡œê¹…
        logger.info("\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ìƒì„¸:")
        for rank_info in ranked_categories[:10]:  # ìƒìœ„ 10ê°œë§Œ ë¡œê¹…
            logger.info(f"\nìˆœìœ„ {rank_info['rank']}: {rank_info['category']}")
            logger.info(f"  ê¸°ì‚¬ ìˆ˜: {rank_info['count']}ê°œ")
            logger.info(f"  ìµœì¢… ì ìˆ˜: {rank_info['final_score']:.3f}")
            logger.info(f"  ì„¸ë¶€ ì ìˆ˜:")
            logger.info(f"    - ë¹ˆë„ ì ìˆ˜: {rank_info['frequency_score']:.3f}")
            logger.info(f"    - ê´€ë ¨ì„± ì ìˆ˜: {rank_info['relevance_score']:.3f}")
            logger.info(f"    - ìµœì‹ ì„± ì ìˆ˜: {rank_info['recent_score']:.3f}")
            logger.info(f"    - ìˆœìœ„ ì ìˆ˜: {rank_info['rank_score']:.3f}")
            logger.info(f"    - ì°¸ì¡° ì ìˆ˜: {rank_info['reference_score']:.3f}")
            logger.info(f"    - ë¶€ì •ì„± ì ìˆ˜: {rank_info['negative_score']:.3f}")
            logger.info("-"*30)
        
        # 10. ìµœì¢… ì¹´í…Œê³ ë¦¬ ìˆœìœ„ ìš”ì•½ ë¡œê¹…
                # 10. ìµœì¢… ì¹´í…Œê³ ë¦¬ ìˆœìœ„ ìš”ì•½ ë¡œê¹…
        logger.info("\nğŸ† ìµœì¢… ì¹´í…Œê³ ë¦¬ ìˆœìœ„ ìš”ì•½:")
        logger.info("ìˆœìœ„ | ì¹´í…Œê³ ë¦¬ | ìµœì¢…ì ìˆ˜ | ê¸°ì‚¬ìˆ˜")
        logger.info("-" * 50)
        for rank_info in ranked_categories[:20]:  # ìƒìœ„ 20ê°œ í‘œì‹œ
            logger.info(f"{rank_info['rank']:2d}ìœ„ | {rank_info['category']:15s} | {rank_info['final_score']:6.3f} | {rank_info['count']:3d}ê°œ")

        # âœ… ì „ì²´ ì¹´í…Œê³ ë¦¬ ìˆœìœ„ ë¡œê¹… ì¶”ê°€
        logger.info("\nğŸ“‹ ì „ì²´ ì¹´í…Œê³ ë¦¬ ìˆœìœ„:")
        logger.info("ìˆœìœ„ | ì¹´í…Œê³ ë¦¬ | ìµœì¢…ì ìˆ˜ | ê¸°ì‚¬ìˆ˜")
        logger.info("-" * 50)
        for rank_info in ranked_categories:  # ì „ì²´ ì¶œë ¥
            logger.info(f"{rank_info['rank']:2d}ìœ„ | {rank_info['category']:15s} | {rank_info['final_score']:6.3f} | {rank_info['count']:3d}ê°œ")

        
        # 11. ìƒ˜í”Œ ê¸°ì‚¬ ë¡œê¹… (ìµœëŒ€ 3ê°œ)
        logger.info("\nğŸ“° ìƒ˜í”Œ ê¸°ì‚¬ ë¶„ì„:")
        for idx, article in enumerate(labeled_articles[:3]):
            logger.info(f"\nê¸°ì‚¬ {idx + 1}:")
            logger.info(f"ì œëª©: {article['title']}")
            logger.info(f"ê°ì„±: {article['sentiment']} (ì‹ ë¢°ë„: {article['sentiment_confidence']:.2f})")
            logger.info(f"ì¹´í…Œê³ ë¦¬: {article['original_category']}")
            logger.info(f"ë¼ë²¨: relevance={article.get('relevance', 'ì—†ìŒ')}, recent={article.get('recent', 'ì—†ìŒ')}, rank={article.get('rank', 'ì—†ìŒ')}, reference={article.get('reference', 'ì—†ìŒ')}")
            logger.info(f"ë¼ë²¨ ì´ìœ : {', '.join(article.get('label_reasons', []))}")
            logger.info("-"*30)

        if len(labeled_articles) > 5:
            logger.info(f"... ì™¸ {len(labeled_articles) - 5}ê°œ ê¸°ì‚¬")
        
        # 12. ì‘ë‹µ ë°ì´í„° ìƒì„±
        response_data = {
            "success": True,
            "message": "ì¤‘ëŒ€ì„± í‰ê°€ ë°ì´í„° ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "data": {
                "company_id": request.company_id,
                "report_period": request.report_period,
                "assessment_status": "analyzed",
                "total_articles": len(labeled_articles),
                "negative_articles": negative_count,
                "negative_ratio": (negative_count/len(labeled_articles))*100 if labeled_articles else 0,
                "total_categories": len(category_scores),
                "ranked_categories": ranked_categories[:20],  # ìƒìœ„ 20ê°œ ì¹´í…Œê³ ë¦¬ë§Œ í¬í•¨
                "category_scores": category_scores,
                "analyzed_samples": labeled_articles[:3]  # ìƒ˜í”Œ ë°ì´í„°ë§Œ í¬í•¨
            }
        }
        
        logger.info("âœ… ë°ì´í„° ë¶„ì„ ì™„ë£Œ")
        logger.info("="*50)
        
        return response_data
        
    except Exception as e:
        error_msg = f"âŒ ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(error_msg)
        logger.error("="*50)
        
        return {
            "success": False,
            "message": error_msg,
            "data": None
        }