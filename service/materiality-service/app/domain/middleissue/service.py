"""
Middleissue Service - ì¤‘ëŒ€ì„± í‰ê°€ ê´€ë ¨ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì²˜ë¦¬
í¬ë¡¤ë§ ë°ì´í„° ì²˜ë¦¬, ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì ìš©, ì ìˆ˜ ê³„ì‚° ë“±ì„ ë‹´ë‹¹
"""
# 1. í¬ë¡¤ë§í•œ ì „ì²´ ë°ì´í„° -> ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë¡œ ê¸ë¶€ì •í‰ê°€
# 2. relevance, recent, negative, rank(ê²€ìƒ‰ì—°ë„-1), reference(NULL) ë¼ë²¨ ë¶€ì—¬
# 3. ê° ì§€í‘œë³„ score ë¶€ì—¬
# 4. final score ê³„ì‚°
# 5. frontendë¡œ ë³´ë‚´ê³  ë©”ëª¨ë¦¬ ì €ì¥

import logging
import os
import re
import json
import joblib
import numpy as np
from datetime import datetime
from typing import Dict, Any, List, Set, Tuple

from dateutil import parser
from app.domain.middleissue.schema import (
    MiddleIssueRequest, MiddleIssueResponse, Article,
    CategoryDetailsResponse, BaseIssuePool
)
from app.domain.middleissue.repository import MiddleIssueRepository

# Railway í™˜ê²½ì—ì„œ ë¡œê·¸ ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€ë¥¼ ìœ„í•œ ë¡œê¹… ì„¤ì •
if os.getenv('RAILWAY_ENVIRONMENT') or True:  # ì¦‰ì‹œ ì ìš©ì„ ìœ„í•´ Trueë¡œ ì„¤ì •
    # Railway í™˜ê²½ì—ì„œëŠ” ë¡œê¹… ë ˆë²¨ì„ WARNINGìœ¼ë¡œ ì„¤ì •
    logging.getLogger('app.domain.middleissue.service').setLevel(logging.WARNING)
    logging.getLogger('app.domain.middleissue.repository').setLevel(logging.WARNING)
    print("ğŸš¨ Railway í™˜ê²½ ê°ì§€: ë¡œê¹… ë ˆë²¨ì„ WARNINGìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë¡œê·¸ ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€")

logger = logging.getLogger(__name__)

# ë¡œê¹… ë ˆë²¨ ê°•ì œ ì„¤ì • (ì¦‰ì‹œ ì ìš©)
logger.setLevel(logging.WARNING)

NEGATIVE_LEXICON = {
    "ê°ì†Œ","í•˜ë½","ë¶€ì§„","ì•…í™”","ì˜¤ì—¼","ìœ„ë°˜","ë‹´í•©","ë¶€íŒ¨","ë‡Œë¬¼","íš¡ë ¹","ë°°ì„","ì‚¬ê¸°",
    "ê³¼ì§•ê¸ˆ","ë²Œê¸ˆ","ì‚¬ê³ ","ì‚¬ë§","íŒŒì—…","ë¶„ìŸ","ê°ˆë“±","ë…¼ë€","ì†Œì†¡","ë¦¬ì½œ","ê²°í•¨","ë¶ˆëŸ‰",
    "ëˆ„ì¶œ","ìœ ì¶œ","í™”ì¬","ì ì","íŒŒì‚°","êµ¬ì¡°ì¡°ì •","ì •ë¦¬í•´ê³ ","ì¤‘ë‹¨","ì°¨ì§ˆ","ì‹¤íŒ¨","ë¶ˆë²•",
    "ì² ìˆ˜","í‡´ì¶œ","ë¶€ì •","ë¶ˆê³µì •","ê°‘ì§ˆ","ì§ì¥ê´´ë¡­í˜","í­ì–¸","íš¡í¬","í™˜ë¶ˆ","íšŒìˆ˜","ì†ì‹¤",
    "ê²½ê³ ","ì œì¬","í•´ì§€","ì·¨ì†Œ","ë‚™ì œ","ë¶€ê³¼","ì§•ê³„","ì¤‘ì§•ê³„","ë¶€ì •ì²­íƒ","ê²½ì˜ê¶Œë¶„ìŸ","ìœ„ê¸°","ì²­ì‚°"
}

POSITIVE_LEXICON = {
    "ì„±ì¥","í™•ëŒ€","ì¦ê°€","ê°œì„ ","í˜¸ì¡°","í‘ì","ìµœê³ ","ì„ ì •","ìˆ˜ìƒ","í¬ìƒ","ì‚°ì—…í¬ì¥",
    "ê°•í™”","ìƒìƒ","í˜‘ë ¥","ë„ì…","ì¶œì‹œ","ì„ ë„","ì¸ì¦","í™•ë³´","ìš°ìˆ˜","ë„ì•½","í™•ì¥","íšŒë³µ",
    "ê³ ë„í™”","ìµœì í™”","ì•ˆì •í™”","ì‹ ì„¤","ì±„ìš©","ì¦ì„¤","ì¦ì‚°","í™•ì¶©","ê³µê¸‰","ìˆ˜ì£¼","ëª¨ë²”",
    "ë‹¬ì„±","ì‹ ê¸°ìˆ ","ê°œì‹œ","ì¦ë¹™","ì„±ê³¼","ë§¤ì¶œì¦ê°€","ê³ ì„±ì¥","ì„ ë„ê¸°ì—…","ìˆ˜ì¶œí™•ëŒ€",
    "í•´ì™¸ì§„ì¶œ","íŒŒíŠ¸ë„ˆì‹­","ë¦¬ë”","í‰íŒ","ì¬ìƒì—ë„ˆì§€","ê°ì¶•","ì´í–‰","í˜ì‹ ","ê°œë°œ","ì—­ëŒ€",
    "ìˆœí•­","ê»‘ì¶©","ê¸°ì¦","ê¸°ë¶€","ì „ë‹¬","ì§€ì›","ìº í˜ì¸","í›„ì›"
}

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
MODEL_PATH = os.path.join(
    os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
    'models',
    'model_multinomialnb.joblib'
)

# ì •ê·œì‹ íŒ¨í„´ ì»´íŒŒì¼
_NEG_RE = re.compile("|".join(map(re.escape, sorted(NEGATIVE_LEXICON, key=len, reverse=True))))
_POS_RE = re.compile("|".join(map(re.escape, sorted(POSITIVE_LEXICON, key=len, reverse=True))))

def parse_pubdate(date_str: str) -> datetime:
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ íŒŒì‹±"""
    try:
        # 1) ISO í˜•ì‹
        try:
            return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        except ValueError:
            pass
        # 2) RSS í˜•ì‹
        try:
            from email.utils import parsedate_to_datetime
            return parsedate_to_datetime(date_str)
        except Exception:
            pass
        # 3) ì¼ë°˜ íŒŒì„œ
        return parser.parse(date_str)
    except Exception as e:
        logger.warning(f"âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ({date_str}): {str(e)}")
        return datetime.now()  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜

def extract_keywords(text: str, patt: re.Pattern) -> List[str]:
    if not isinstance(text, str):
        return []
    return sorted(set(patt.findall(text)))

def load_sentiment_model():
    """ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ"""
    try:
        logger.info(f"ğŸ¤– ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹œë„: {MODEL_PATH}")
        model = joblib.load(MODEL_PATH)
        logger.info("âœ… ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        return model
    except Exception as e:
        logger.error(f"âŒ ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None

def analyze_sentiment(model, articles: List[Article]) -> List[Dict[str, Any]]:
    """ê¸°ì‚¬ ê°ì„± ë¶„ì„ ìˆ˜í–‰"""
    try:
        analyzed_articles: List[Dict[str, Any]] = []
        for article in articles:
            try:
                title_text = article.title
                desc_text = article.description
                full_text = f"{title_text} {desc_text}"

                # í‚¤ì›Œë“œ ê¸°ë°˜
                neg_keywords = extract_keywords(full_text, _NEG_RE)
                pos_keywords = extract_keywords(full_text, _POS_RE)
                has_both = len(neg_keywords) > 0 and len(pos_keywords) > 0

                # ëª¨ë¸ ê¸°ë°˜
                if model is not None:
                    try:
                        y_pred = model.predict([full_text])[0]
                        probas = model.predict_proba([full_text])[0]

                        classes = getattr(model.named_steps.get("clf", model), "classes_", None)
                        if classes is None:
                            classes = getattr(model, "classes_", None)

                        if classes is not None and "negative" in classes:
                            neg_idx = int(np.where(classes == "negative")[0][0])
                            neg_proba = float(probas[neg_idx])
                        else:
                            neg_proba = 0.0

                        if y_pred == "negative" and has_both:
                            final_sentiment = "other"
                            final_basis = "ë¶€ì •+ê¸ì • ë™ì‹œ ì¶œí˜„ â†’ other"
                        else:
                            final_sentiment = y_pred
                            final_basis = "ëª¨ë¸ ì˜ˆì¸¡ ìœ ì§€"
                    except Exception as e:
                        logger.error(f"âŒ ëª¨ë¸ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                        final_sentiment = "negative" if len(neg_keywords) > len(pos_keywords) else "other"
                        final_basis = "í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨ (ëª¨ë¸ ì‹¤íŒ¨)"
                        neg_proba = 1.0 if final_sentiment == "negative" else 0.0
                else:
                    final_sentiment = "negative" if len(neg_keywords) > len(pos_keywords) else "other"
                    final_basis = "í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨ (ëª¨ë¸ ì—†ìŒ)"
                    neg_proba = 1.0 if final_sentiment == "negative" else 0.0

                analyzed_articles.append({
                    "title": title_text,
                    "description": desc_text,
                    "sentiment": final_sentiment,
                    "sentiment_confidence": float(neg_proba if final_sentiment == "negative" else (1 - neg_proba)),
                    "neg_keywords": ", ".join(neg_keywords),
                    "pos_keywords": ", ".join(pos_keywords),
                    "sentiment_basis": final_basis,
                    "original_category": article.original_category,
                    "issue": article.issue,
                    "pubDate": article.pubDate,
                    "originallink": article.originallink,
                    "company": article.company,
                })

            except Exception as e:
                logger.error(f"âŒ ê¸°ì‚¬ ê°ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}; ì œëª©: {getattr(article, 'title', '')}")
                continue

        return analyzed_articles
    except Exception as e:
        logger.error(f"âŒ ê°ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []

async def add_relevance_labels(
    articles: List[Dict[str, Any]],
    company_id: str,
    search_date: datetime,
    prev_year_categories: Set[str],   # (ê²€ìƒ‰ ê¸°ì¤€ì—°ë„ - 1)ì˜ category id ì§‘í•©
    reference_categories: Set[str],   # publish_year = NULL/''/'0' ì˜ category id ì§‘í•©
) -> List[Dict[str, Any]]:
    """
    ë¼ë²¨ ì •ì˜:
    - relevance : ì œëª©ì— ê¸°ì—…ëª… í¬í•¨ì´ë©´ '++' (True)
    - recent    : 3ê°œì›” ì´ë‚´=1.0, 3~6ê°œì›”=0.5, ê·¸ ì™¸=0.0
    - rank      : original_category âˆˆ prev_year_categories â†’ True
    - reference : original_category âˆˆ reference_categories â†’ True
    """
    try:
        repository = MiddleIssueRepository()
        
        for a in articles:
            a["relevance_label"] = False
            a["recent_value"] = 0.0
            a["rank_label"] = False
            a["reference_label"] = False
            a["label_reasons"] = []

            # relevance
            title = a.get("title") or ""
            if isinstance(title, str) and company_id and company_id in title:
                a["relevance_label"] = True
                a["label_reasons"].append("ì œëª©ì— ê¸°ì—…ëª… í¬í•¨")

            # recent
            pub_str = a.get("pubDate")
            if pub_str:
                try:
                    pub_dt = parse_pubdate(pub_str)
                    months_diff = (search_date - pub_dt).days / 30
                    if months_diff <= 3:
                        a["recent_value"] = 1.0
                        a["label_reasons"].append("ìµœê·¼ 3ê°œì›” ì´ë‚´")
                    elif months_diff <= 6:
                        a["recent_value"] = 0.5
                        a["label_reasons"].append("ìµœê·¼ 3~6ê°œì›”")
                except Exception as e:
                    logger.warning(f"âš ï¸ recent ê³„ì‚° ì¤‘ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {e}")

            # rank/reference (ì¹´í…Œê³ ë¦¬ ì´ë¦„ì„ IDë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ)
            oc = a.get("original_category")
            if oc is not None:
                try:
                    # ì¹´í…Œê³ ë¦¬ ì´ë¦„ì„ IDë¡œ ë³€í™˜
                    category_id = await repository.get_category_id_by_name(str(oc))
                    if category_id is not None:
                        oc_key = str(category_id)
                        
                        if oc_key in prev_year_categories:
                            a["rank_label"] = True
                            a["label_reasons"].append("ì´ì „ë…„ë„ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­")

                        if oc_key in reference_categories:
                            a["reference_label"] = True
                            a["label_reasons"].append("ê³µí†µ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­")
                    else:
                        logger.warning(f"âš ï¸ ì¹´í…Œê³ ë¦¬ ì´ë¦„ '{oc}'ì„ IDë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŒ")
                except Exception as e:
                    logger.warning(f"âš ï¸ ì¹´í…Œê³ ë¦¬ ID ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")

        return articles
    except Exception as e:
        logger.error(f"âŒ ë¼ë²¨ ë¶€ì—¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return articles

def calculate_category_scores(articles: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
    """
    ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°

    ì ìˆ˜ ì²´ê³„:
    - frequency_score: í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë¹ˆë„ (0~1)
    - relevance_score: ì¹´í…Œê³ ë¦¬ ê¸°ì‚¬ë“¤ì˜ relevance_label í‰ê·  (True=1, False=0)
    - recent_score   : ì¹´í…Œê³ ë¦¬ ê¸°ì‚¬ë“¤ì˜ recent_value í‰ê·  (1/0.5/0)
    - rank_score     : ì¹´í…Œê³ ë¦¬ ë‚´ rank_label ì¡´ì¬ ì—¬ë¶€(0/1)  â€» ì „ë¶€ ë™ì¼í•˜ë‹¤ëŠ” ê°€ì •
    - negative_score : ì¹´í…Œê³ ë¦¬ ë‚´ ë¶€ì • ê¸°ì‚¬ ë¹„ìœ¨ (0~1)
    - reference_score: ì¹´í…Œê³ ë¦¬ ë‚´ reference_label ì¡´ì¬ ì—¬ë¶€(0/1)

    ìµœì¢… ì ìˆ˜:
    final = 0.4*frequency
          + 0.6*relevance
          + 0.2*recent
          + 0.4*rank
          + 0.6*reference
          + 0.8*negative*(1 + 0.5*frequency + 0.5*relevance)
    """
    try:
        total_articles = len(articles)
        if total_articles == 0:
            return {}

        buckets: Dict[str, Dict[str, Any]] = {}
        empty_category_count = 0
        missing_category_count = 0
        
        for a in articles:
            cat = a.get("original_category")
            if cat is None:
                missing_category_count += 1
                logger.warning(f"ğŸš¨ original_categoryê°€ Noneì¸ ê¸°ì‚¬ ë°œê²¬ - ê¸°ì‚¬ ì œëª©: '{a.get('title', 'N/A')[:50]}...'")
                continue
            
            # ë¹ˆ ì¹´í…Œê³ ë¦¬ ì²´í¬ (ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ)
            key = str(cat).strip()
            if not key:  # ë¹ˆ ë¬¸ìì—´ì´ê±°ë‚˜ ê³µë°±ë§Œ ìˆëŠ” ê²½ìš°
                empty_category_count += 1
                logger.warning(f"ğŸš¨ ë¹ˆ ì¹´í…Œê³ ë¦¬ ë°œê²¬ - ê¸°ì‚¬ ì œëª©: '{a.get('title', 'N/A')[:50]}...', original_category: '{cat}'")
                # ë¹ˆ ì¹´í…Œê³ ë¦¬ë„ í¬í•¨í•˜ì—¬ ë¶„ì„ (í¬ë¡¤ë§ ë°ì´í„°ëŠ” ì´ë¯¸ ë§¤í•‘ë˜ì–´ì•¼ í•¨)
                key = f"ë¹ˆì¹´í…Œê³ ë¦¬_{empty_category_count}"  # ì„ì‹œ ì‹ë³„ì
            
            b = buckets.setdefault(key, {
                "count": 0,
                "relevance_sum": 0.0,
                "recent_sum": 0.0,
                "negative_count": 0,
                "rank_sum": 0.0,        # rank_label í•©ê³„ë¡œ ë³€ê²½
                "reference_sum": 0.0,   # reference_label í•©ê³„ë¡œ ë³€ê²½
                "articles": []
            })

            b["count"] += 1
            b["articles"].append(a)
            b["relevance_sum"] += 1.0 if a.get("relevance_label") else 0.0
            b["recent_sum"] += float(a.get("recent_value", 0.0))
            
            # ì•ˆì „í•œ ë¶€ì •ì ìˆ˜ ê³„ì‚°
            sentiment = a.get("sentiment")
            if sentiment is not None:
                # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ë¹„êµ
                if str(sentiment).lower() == "negative":
                    b["negative_count"] += 1
                    logger.debug(f"ğŸ” ë¶€ì • ê¸°ì‚¬ ê°ì§€: {sentiment}")
                elif str(sentiment).lower() not in ["positive", "other"]:
                    logger.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ sentiment ê°’: '{sentiment}'")
            else:
                logger.warning(f"âš ï¸ sentiment ê°’ì´ Noneì¸ ê¸°ì‚¬ ë°œê²¬")
            
            # rankì™€ referenceë¥¼ í•©ê³„ë¡œ ëˆ„ì 
            rank_label = a.get("rank_label")
            reference_label = a.get("reference_label")
            
            # ì•ˆì „í•œ rank_label ì²˜ë¦¬
            if rank_label is not None:
                if isinstance(rank_label, bool):
                    b["rank_sum"] += 1.0 if rank_label else 0.0
                elif isinstance(rank_label, (int, float)):
                    b["rank_sum"] += float(rank_label)
                else:
                    logger.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ rank_label íƒ€ì…: {type(rank_label)}, ê°’: {rank_label}")
                    b["rank_sum"] += 0.0
            else:
                b["rank_sum"] += 0.0
            
            # ì•ˆì „í•œ reference_label ì²˜ë¦¬
            if reference_label is not None:
                if isinstance(reference_label, bool):
                    b["reference_sum"] += 1.0 if reference_label else 0.0
                elif isinstance(reference_label, (int, float)):
                    b["reference_sum"] += float(reference_label)
                else:
                    logger.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ reference_label íƒ€ì…: {type(reference_label)}, ê°’: {reference_label}")
                    b["reference_sum"] += 0.0
            else:
                b["reference_sum"] += 0.0

        # ì¹´í…Œê³ ë¦¬ ë°ì´í„° í’ˆì§ˆ í†µê³„ ë¡œê¹…
        if missing_category_count > 0:
            logger.warning(f"ğŸš¨ original_categoryê°€ Noneì¸ ê¸°ì‚¬: {missing_category_count}ê°œ")
        if empty_category_count > 0:
            logger.warning(f"ğŸš¨ ë¹ˆ ì¹´í…Œê³ ë¦¬ ê¸°ì‚¬: {empty_category_count}ê°œ (ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ)")
            logger.warning(f"ğŸš¨ ì´ëŠ” í¬ë¡¤ë§ ë‹¨ê³„ì—ì„œ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ì´ ì œëŒ€ë¡œ ë˜ì§€ ì•Šì•˜ìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.")
        
        results: Dict[str, Dict[str, Any]] = {}
        for key, b in buckets.items():
            c = b["count"]
            
            # ì•ˆì „í•œ ë¹ˆë„ì ìˆ˜ ê³„ì‚°
            try:
                frequency = c / total_articles if total_articles > 0 else 0.0
                # ë…¼ë¦¬ì  ê²€ì¦: ë¹ˆë„ê°€ 1ì„ ì´ˆê³¼í•  ìˆ˜ ì—†ìŒ
                if frequency > 1.0:
                    logger.warning(f"âš ï¸ ì¹´í…Œê³ ë¦¬ '{key}' ë¹ˆë„ì ìˆ˜ ë¹„ì •ìƒ: {frequency:.4f} > 1.0, 1.0ìœ¼ë¡œ ì¡°ì •")
                    frequency = 1.0
            except Exception as e:
                logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ '{key}' ë¹ˆë„ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}, ê¸°ë³¸ê°’ 0.0 ì‚¬ìš©")
                frequency = 0.0
            
            # ì•ˆì „í•œ ë‹¤ë¥¸ ì ìˆ˜ë“¤ ê³„ì‚°
            relevance = (b["relevance_sum"] / c) if c > 0 else 0.0
            recent = (b["recent_sum"] / c) if c > 0 else 0.0
            rank = (b["rank_sum"] / c) if c > 0 else 0.0
            reference = (b["reference_sum"] / c) if c > 0 else 0.0
            negative = (b["negative_count"] / c) if c > 0 else 0.0

            # ì•ˆì „í•œ ìµœì¢… ì ìˆ˜ ê³„ì‚°
            try:
                final_score = (
                    0.4 * frequency
                    + 0.6 * relevance
                    + 0.2 * recent
                    + 0.4 * rank
                    + 0.6 * reference
                    + 0.8 * negative * (1 + 0.5 * frequency + 0.5 * relevance)
                )
                
                # ì ìˆ˜ ë²”ìœ„ ê²€ì¦ (0~10 ë²”ìœ„ë¡œ ê°€ì •)
                if final_score < 0:
                    logger.warning(f"âš ï¸ ì¹´í…Œê³ ë¦¬ '{key}' ìµœì¢…ì ìˆ˜ ìŒìˆ˜: {final_score:.6f}, 0ìœ¼ë¡œ ì¡°ì •")
                    final_score = 0.0
                elif final_score > 10:
                    logger.warning(f"âš ï¸ ì¹´í…Œê³ ë¦¬ '{key}' ìµœì¢…ì ìˆ˜ ê³¼ëŒ€: {final_score:.6f}, 10ìœ¼ë¡œ ì¡°ì •")
                    final_score = 10.0
                    
            except Exception as e:
                logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ '{key}' ìµœì¢…ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}, ê¸°ë³¸ê°’ 0.0 ì‚¬ìš©")
                final_score = 0.0

            results[key] = {
                "count": c,
                "frequency_score": round(frequency, 6),
                "relevance_score": round(relevance, 6),
                "recent_score": round(recent, 6),
                "rank_score": round(rank, 6),
                "reference_score": round(reference, 6),
                "negative_score": round(negative, 6),
                "final_score": round(final_score, 6),
                "articles": b["articles"],
            }

        return results
    except Exception as e:
        logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return {}

def rank_categories_by_score(category_scores: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ì¹´í…Œê³ ë¦¬ë¥¼ final_score ê¸°ì¤€ìœ¼ë¡œ ìˆœìœ„ ë§¤ê¸°ê¸°"""
    try:
        # ë¹ˆ ì¹´í…Œê³ ë¦¬ í¬í•¨í•˜ì—¬ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬
        empty_categories = []
        valid_categories = []
        
        for cat, scores in category_scores.items():
            if not str(cat).strip():  # ë¹ˆ ë¬¸ìì—´ì´ê±°ë‚˜ ê³µë°±ë§Œ ìˆëŠ” ê²½ìš°
                empty_categories.append(cat)
            else:
                valid_categories.append(cat)
        
        if empty_categories:
            logger.warning(f"ğŸš¨ ìˆœìœ„ ë§¤ê¸°ê¸°ì— í¬í•¨ëœ ë¹ˆ ì¹´í…Œê³ ë¦¬: {len(empty_categories)}ê°œ - {empty_categories}")
            logger.warning(f"ğŸš¨ ì´ëŠ” í¬ë¡¤ë§ ë°ì´í„° í’ˆì§ˆ ë¬¸ì œë¡œ, ì›ë³¸ ë°ì´í„°ë¥¼ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        # ëª¨ë“  ì¹´í…Œê³ ë¦¬ í¬í•¨í•˜ì—¬ ìˆœìœ„ ë§¤ê¸°ê¸°
        ranked = sorted(
            category_scores.items(),
            key=lambda x: x[1]["final_score"],
            reverse=True
        )
        out: List[Dict[str, Any]] = []
        for idx, (cat, scores) in enumerate(ranked, start=1):
            out.append({
                "rank": idx,
                "category": cat,
                **scores
            })
        return out
    except Exception as e:
        logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ ìˆœìœ„ ë§¤ê¸°ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []

async def match_categories_with_esg_and_issuepool(
    ranked_categories: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    ì¹´í…Œê³ ë¦¬ë³„ë¡œ ESG ë¶„ë¥˜ì™€ base_issuepoolì„ ë°°ì¹˜ ì¿¼ë¦¬ë¡œ ë§¤ì¹­
    
    ë§¤ì¹­ ê·œì¹™:
    1. materiality_category DBì—ì„œ ì¹´í…Œê³ ë¦¬ë³„ ESG ë¶„ë¥˜ ì¡°íšŒ (company_id, ì—°ë„ ì¡°ê±´ ì—†ìŒ)
    2. issuepool DBì—ì„œ ì¹´í…Œê³ ë¦¬ë³„ base_issue_pool ì¡°íšŒ (company_id, ì—°ë„ ì¡°ê±´ ì—†ìŒ)
    3. ì¹´í…Œê³ ë¦¬ í•˜ë‚˜ë‹¹ ESG ë¶„ë¥˜ëŠ” í•˜ë‚˜, base_issuepoolì€ ì—¬ëŸ¬ ê°œ
    """
    try:
        repository = MiddleIssueRepository()
        
        logger.warning(f"ğŸ” ë°°ì¹˜ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ ì‹œì‘")
        logger.warning(f"ğŸ” ë§¤ì¹­í•  ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(ranked_categories)}")
        
        # 1. ëª¨ë“  ì¹´í…Œê³ ë¦¬ í‚¤ ìˆ˜ì§‘
        category_keys = [str(cat['category']) for cat in ranked_categories]
        
        # 2. ğŸ”¥ ìƒˆë¡œìš´ ë°°ì¹˜ ì¡°íšŒ ë©”ì„œë“œ ì‚¬ìš© (N+1 ë¬¸ì œ í•´ê²°)
        logger.warning(f"ğŸ” ë°°ì¹˜ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹œì‘: {len(category_keys)}ê°œ ì¹´í…Œê³ ë¦¬")
        
        try:
            # ìƒˆë¡œìš´ ë°°ì¹˜ ì¡°íšŒ ë©”ì„œë“œ ì‚¬ìš© (company_id, ì—°ë„ ì¡°ê±´ ì—†ìŒ)
            details_map = await repository.get_categories_by_names_batch(
                category_names=category_keys
            )
            
            logger.warning(f"âœ… ë°°ì¹˜ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì™„ë£Œ: {len(details_map)}ê°œ ì¹´í…Œê³ ë¦¬")
            
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ë¹ˆ ë°°ì—´ë¡œ ì„¤ì •
            details_map = {name: None for name in category_keys}
        
        # 3. ê²°ê³¼ ì¡°í•© (ë¹ ë¥¸ ì²˜ë¦¬)
        logger.warning(f"ğŸ” ê²°ê³¼ ì¡°í•© ì‹œì‘")
        matched_categories = []
        
        for category_info in ranked_categories:
            category_key = str(category_info['category'])
            
            # ë°°ì¹˜ ì¡°íšŒ ê²°ê³¼ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            details = details_map.get(category_key)
            
            if details:
                # ë°°ì¹˜ ì¡°íšŒì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„° ì‚¬ìš©
                esg_classification = details.esg_classification_name or 'ë¯¸ë¶„ë¥˜'
                esg_classification_id = details.esg_classification_id
                base_issuepools = []
                
                # BaseIssuePoolì„ dictë¡œ ë³€í™˜
                for issue in details.base_issuepools:
                    base_issuepools.append({
                        "id": issue.id,
                        "base_issue_pool": issue.base_issue_pool,
                        "issue_pool": issue.issue_pool,
                        "ranking": issue.ranking,
                        "esg_classification_id": esg_classification_id,
                        "esg_classification_name": esg_classification
                    })
                
                total_issuepools = len(base_issuepools)
            else:
                # ë°°ì¹˜ ì¡°íšŒì—ì„œ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’
                esg_classification = 'ë¯¸ë¶„ë¥˜'
                esg_classification_id = None
                base_issuepools = []
                total_issuepools = 0
            
            matched_category = {
                **category_info,  # ê¸°ì¡´ ì ìˆ˜ ì •ë³´ ìœ ì§€
                "esg_classification": esg_classification,
                "esg_classification_id": esg_classification_id,
                "base_issuepools": base_issuepools,
                "total_issuepools": total_issuepools
            }
            
            matched_categories.append(matched_category)
        
        # 4. ìš”ì•½ ë¡œê¹… (ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ê°„ì†Œí™”)
        total_issuepools = sum(len(cat.get('base_issuepools', [])) for cat in matched_categories)
        
        # ESG ë¶„í¬ ê³„ì‚°
        esg_distribution = {}
        for cat in matched_categories:
            esg = cat.get('esg_classification', 'ë¯¸ë¶„ë¥˜')
            esg_distribution[esg] = esg_distribution.get(esg, 0) + 1
        
        logger.warning(f"ğŸ”— ë°°ì¹˜ ë§¤ì¹­ ì™„ë£Œ:")
        logger.warning(f"   - ì´ ì¹´í…Œê³ ë¦¬: {len(matched_categories)}ê°œ")
        logger.warning(f"   - ì´ IssuePool: {total_issuepools}ê°œ")
        logger.warning(f"   - ESG ë¶„í¬: {esg_distribution}")
        
        # ğŸ” base_issue_pool ë§¤ì¹­ ê²°ê³¼ ìš”ì•½ (í•µì‹¬ë§Œ)
        matched_count = sum(1 for cat in matched_categories if cat.get('total_issuepools', 0) > 0)
        unmatched_count = len(matched_categories) - matched_count
        
        logger.warning(f"ğŸ” Base IssuePool ë§¤ì¹­ ê²°ê³¼:")
        logger.warning(f"   - ë§¤ì¹­ ì„±ê³µ: {matched_count}ê°œ ì¹´í…Œê³ ë¦¬")
        logger.warning(f"   - ë§¤ì¹­ ì‹¤íŒ¨: {unmatched_count}ê°œ ì¹´í…Œê³ ë¦¬")
        
        if unmatched_count > 0:
            unmatched_categories = [cat['category'] for cat in matched_categories if cat.get('total_issuepools', 0) == 0]
            logger.warning(f"   - ë§¤ì¹­ ì‹¤íŒ¨ ì¹´í…Œê³ ë¦¬: {unmatched_categories}")
        
        # ğŸ” ESG ë¶„ë¥˜ ë§¤í•‘ ìƒíƒœ ê°„ë‹¨ í™•ì¸
        esg_mapped_count = sum(1 for cat in matched_categories if cat.get('esg_classification') and cat.get('esg_classification') != 'ë¯¸ë¶„ë¥˜')
        esg_unmapped_count = len(matched_categories) - esg_mapped_count
        
        logger.warning(f"ğŸ” ESG ë¶„ë¥˜ ë§¤í•‘ ìƒíƒœ:")
        logger.warning(f"   - ESG ë§¤í•‘ ì„±ê³µ: {esg_mapped_count}ê°œ ì¹´í…Œê³ ë¦¬")
        logger.warning(f"   - ESG ë§¤í•‘ ì‹¤íŒ¨: {esg_unmapped_count}ê°œ ì¹´í…Œê³ ë¦¬")
        
        if esg_unmapped_count > 0:
            unmapped_esg_categories = [cat['category'] for cat in matched_categories if not cat.get('esg_classification') or cat.get('esg_classification') == 'ë¯¸ë¶„ë¥˜']
            logger.warning(f"   - ESG ë§¤í•‘ ì‹¤íŒ¨ ì¹´í…Œê³ ë¦¬: {unmapped_esg_categories}")
            
            # ë¹ˆ ì¹´í…Œê³ ë¦¬ íŠ¹ë³„ ì²˜ë¦¬
            empty_categories = [cat['category'] for cat in matched_categories if not str(cat['category']).strip()]
            if empty_categories:
                logger.warning(f"ğŸš¨ ë¹ˆ ì¹´í…Œê³ ë¦¬ ë°œê²¬: {empty_categories}")
                logger.warning(f"ğŸš¨ ì´ëŠ” í¬ë¡¤ë§ ë‹¨ê³„ì—ì„œ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ì´ ëˆ„ë½ëœ ë°ì´í„° í’ˆì§ˆ ë¬¸ì œì…ë‹ˆë‹¤.")
                logger.warning(f"ğŸš¨ ì›ë³¸ í¬ë¡¤ë§ ë°ì´í„°ë¥¼ í™•ì¸í•˜ì—¬ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ì„ ìˆ˜ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        
        return matched_categories
        
    except Exception as e:
        logger.error(f"âŒ ë°°ì¹˜ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ ì¤‘ ì „ì²´ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.error(f"âŒ ì˜¤ë¥˜ ìƒì„¸: {type(e).__name__}: {str(e)}")
        import traceback
        logger.error(f"âŒ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ì¡´ ê°œë³„ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ fallback
        logger.info(f"ğŸ”„ ê¸°ì¡´ ê°œë³„ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ fallback")
        return await _fallback_individual_matching(ranked_categories)


async def _fallback_individual_matching(
    ranked_categories: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    Fallback: ê¸°ì¡´ ê°œë³„ ì²˜ë¦¬ ë°©ì‹ (ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
    """
    try:
        repository = MiddleIssueRepository()
        matched_categories = []
        
        logger.info(f"ğŸ”„ Fallback ê°œë³„ ì²˜ë¦¬ ì‹œì‘: {len(ranked_categories)}ê°œ ì¹´í…Œê³ ë¦¬")
        
        for category_info in ranked_categories:
            category_name = str(category_info['category'])
            
            try:
                # 1. materiality_category DBì—ì„œ ESG ë¶„ë¥˜ ì¡°íšŒ (company_id, ì—°ë„ ì¡°ê±´ ì—†ìŒ)
                esg_classification = await repository.get_category_esg_direct(category_name)
                if not esg_classification:
                    esg_classification = 'ë¯¸ë¶„ë¥˜'
                
                # 2. base_issuepool ì •ë³´ ì¡°íšŒ (company_id, ì—°ë„ ì¡°ê±´ ì—†ìŒ - ì¹´í…Œê³ ë¦¬ë§Œ ë§¤ì¹­)
                base_issuepools = []
                try:
                    if category_name.isdigit():
                        # IDë¡œ ì¡°íšŒí•˜ë˜ company_id, ì—°ë„ ì¡°ê±´ ì œê±°
                        category_details = await repository.get_category_details(
                            corporation_name="",  # ë¹ˆ ë¬¸ìì—´ë¡œ ì „ë‹¬ (ë‚´ë¶€ì—ì„œ ë¬´ì‹œë¨)
                            category_id=int(category_name),
                            year=0  # 0ìœ¼ë¡œ ì „ë‹¬ (ë‚´ë¶€ì—ì„œ ë¬´ì‹œë¨)
                        )
                    else:
                        # ì´ë¦„ìœ¼ë¡œ ì§ì ‘ ì¡°íšŒí•˜ë˜ company_id, ì—°ë„ ì¡°ê±´ ì œê±°
                        category_details = await repository.get_category_by_name_direct(
                            corporation_name="",  # ë¹ˆ ë¬¸ìì—´ë¡œ ì „ë‹¬ (ë‚´ë¶€ì—ì„œ ë¬´ì‹œë¨)
                            category_name=category_name,
                            year=0  # 0ìœ¼ë¡œ ì „ë‹¬ (ë‚´ë¶€ì—ì„œ ë¬´ì‹œë¨)
                        )
                    
                    if category_details and category_details.base_issuepools:
                        # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ set ì‚¬ìš©
                        seen_pools = set()
                        for issue in category_details.base_issuepools:
                            # ê³µë°±ì„ í¬í•¨í•œ ë¬¸ì ê·¸ëŒ€ë¡œ ë¹„êµí•˜ì—¬ ì¤‘ë³µ ì²´í¬
                            pool_key = (issue.base_issue_pool, issue.issue_pool)
                            if pool_key not in seen_pools:
                                seen_pools.add(pool_key)
                                base_issuepools.append({
                                    "id": issue.id,
                                    "base_issue_pool": issue.base_issue_pool,
                                    "issue_pool": issue.issue_pool,
                                    "ranking": issue.ranking,
                                    "esg_classification_id": category_details.esg_classification_id,
                                    "esg_classification_name": esg_classification
                                })
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ Fallback: ì¹´í…Œê³ ë¦¬ '{category_name}' base_issuepool ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
                
                # 3. ë§¤ì¹­ëœ ì¹´í…Œê³ ë¦¬ ì •ë³´ ìƒì„±
                matched_category = {
                    **category_info,
                    "esg_classification": esg_classification,
                    "esg_classification_id": None,
                    "base_issuepools": base_issuepools,
                    "total_issuepools": len(base_issuepools)
                }
                
                matched_categories.append(matched_category)
                
            except Exception as e:
                logger.error(f"âŒ Fallback: ì¹´í…Œê³ ë¦¬ '{category_name}' ë§¤ì¹­ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                matched_category = {
                    **category_info,
                    "esg_classification": "ë¯¸ë¶„ë¥˜",
                    "esg_classification_id": None,
                    "base_issuepools": [],
                    "total_issuepools": 0
                }
                matched_categories.append(matched_category)
        
        logger.info(f"ğŸ”„ Fallback ê°œë³„ ì²˜ë¦¬ ì™„ë£Œ: {len(matched_categories)}ê°œ ì¹´í…Œê³ ë¦¬")
        return matched_categories
        
    except Exception as e:
        logger.error(f"âŒ Fallback ê°œë³„ ì²˜ë¦¬ë„ ì‹¤íŒ¨: {str(e)}")
        # ìµœí›„ ìˆ˜ë‹¨: ì›ë³¸ ì¹´í…Œê³ ë¦¬ ì •ë³´ë§Œ ë°˜í™˜
        return ranked_categories

async def start_assessment(request: MiddleIssueRequest) -> Dict[str, Any]:
    """
    ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘ - í¬ë¡¤ë§ ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ ì‹œì‘
    """
    try:
        # 1) ìš”ì²­ ë¡œê¹…
        start_time = datetime.now()
        logger.warning("ğŸš€ start_assessment í•¨ìˆ˜ ì‹œì‘")
        logger.warning("="*50)
        logger.warning("ğŸš€ ìƒˆë¡œìš´ ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘")
        logger.warning(f"ê¸°ì—…ëª…: {request.company_id}")
        logger.warning(f"ë³´ê³ ê¸°ê°„: {request.report_period}")
        logger.warning(f"ìš”ì²­ íƒ€ì…: {request.request_type}")
        logger.warning(f"ì´ í¬ë¡¤ë§ ê¸°ì‚¬ ìˆ˜: {request.total_results}")
        logger.warning("-"*50)

        # 2) ëª¨ë¸ ë¡œë“œ
        model_start = datetime.now()
        logger.info("ğŸ”¥ í¬ë¡¤ë§ ë°ì´í„° ê°ì„± ë¶„ì„ ì‹œì‘")
        model = load_sentiment_model()
        if not model:
            raise Exception("ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
        model_load_time = (datetime.now() - model_start).total_seconds()
        logger.info(f"â±ï¸ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_load_time:.2f}ì´ˆ")

        # 3) ê°ì„± ë¶„ì„
        sentiment_start = datetime.now()
        logger.info("ğŸ”¥ í¬ë¡¤ë§ ë°ì´í„° ê°ì„± ë¶„ì„ ì‹œì‘")
        analyzed_articles = analyze_sentiment(model, request.articles)
        sentiment_time = (datetime.now() - sentiment_start).total_seconds()
        logger.info(f"â±ï¸ ê°ì„± ë¶„ì„ ì™„ë£Œ: {sentiment_time:.2f}ì´ˆ ({len(analyzed_articles)}ê°œ ê¸°ì‚¬)")

        # 4) (ê²€ìƒ‰ ê¸°ì¤€ì—°ë„ - 1) & ê³µí†µ(NULL) ì¹´í…Œê³ ë¦¬ ì¡°íšŒ
        db_query_start = datetime.now()
        repository = MiddleIssueRepository()
        
        # ì•ˆì „í•œ ì—°ë„ íŒŒì‹±
        try:
            search_year = int(request.report_period["end_date"][:4])  # ê²€ìƒ‰ ê¸°ì¤€ì—°ë„ (YYYY)
        except (KeyError, ValueError, IndexError) as e:
            logger.error(f"âŒ ì—°ë„ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")
            logger.error(f"ğŸ” report_period: {request.report_period}")
            # ê¸°ë³¸ê°’ìœ¼ë¡œ í˜„ì¬ ì—°ë„ ì‚¬ìš©
            search_year = datetime.now().year
            logger.warning(f"âš ï¸ ê¸°ë³¸ê°’ ì‚¬ìš©: {search_year}ë…„")
        
        # repository ë‚´ë¶€ì—ì„œ -1 ì²˜ë¦¬í•˜ë¯€ë¡œ search_yearë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬
        corp_issues_prev = await repository.get_corporation_issues(
            corporation_name=request.company_id,
            year=search_year  # repository ë‚´ë¶€ì—ì„œ -1 ì²˜ë¦¬
        )
        # 2. prev_year_categoriesì™€ reference_categories ìˆ˜ì§‘
        # prev_year ê¸°ì¤€ ì¹´í…Œê³ ë¦¬ì™€ ê³µí†µ(NULL/ë¹ˆë¬¸ìì—´/'0') ì¹´í…Œê³ ë¦¬ ì„¸íŠ¸
        prev_year_categories = {str(issue.category_id) for issue in corp_issues_prev.year_issues}
        reference_categories = {str(issue.category_id) for issue in corp_issues_prev.common_issues}
        
        logger.info(f"ğŸ” prev_year_categories: {len(prev_year_categories)}ê°œ, reference_categories: {len(reference_categories)}ê°œ")
        db_query_time = (datetime.now() - db_query_start).total_seconds()
        logger.info(f"â±ï¸ DB ì¡°íšŒ ì™„ë£Œ: {db_query_time:.2f}ì´ˆ")

        # 5) ë¼ë²¨ ë¶€ì—¬
        labeling_start = datetime.now()
        logger.info("ğŸ·ï¸ ë¼ë²¨(relevance/recent/rank/reference) ë¶€ì—¬ ì‹œì‘")
        search_date = datetime.now()
        labeled_articles = await add_relevance_labels(
            analyzed_articles,
            request.company_id,
            search_date,
            prev_year_categories,
            reference_categories
        )
        labeling_time = (datetime.now() - labeling_start).total_seconds()
        logger.info(f"â±ï¸ ë¼ë²¨ ë¶€ì—¬ ì™„ë£Œ: {labeling_time:.2f}ì´ˆ")

        # 6) ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°
        scoring_start = datetime.now()
        logger.info("ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚° ì‹œì‘")
        category_scores = calculate_category_scores(labeled_articles)
        scoring_time = (datetime.now() - scoring_start).total_seconds()
        logger.info(f"â±ï¸ ì ìˆ˜ ê³„ì‚° ì™„ë£Œ: {scoring_time:.2f}ì´ˆ")

        # ğŸ” ë””ë²„ê¹…: ë¼ë²¨ë§ ê²°ê³¼ ë¶„ì„
        debug_labeling_results(labeled_articles, category_scores)

        # 7) ì¹´í…Œê³ ë¦¬ ë­í‚¹
        ranking_start = datetime.now()
        logger.info("ğŸ† ì¹´í…Œê³ ë¦¬ ìˆœìœ„ ë§¤ê¸°ê¸° ì‹œì‘")
        ranked_categories = rank_categories_by_score(category_scores)
        ranking_time = (datetime.now() - ranking_start).total_seconds()
        logger.info(f"â±ï¸ ì¹´í…Œê³ ë¦¬ ë­í‚¹ ì™„ë£Œ: {ranking_time:.2f}ì´ˆ")

        # 8) ì¹´í…Œê³ ë¦¬ë³„ ESG ë¶„ë¥˜ ë° ì´ìŠˆí’€ ë§¤ì¹­ (ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ)
        matching_start = datetime.now()
        logger.info("ğŸ”— ì¹´í…Œê³ ë¦¬ë³„ ESG ë¶„ë¥˜ ë° ì´ìŠˆí’€ ë§¤ì¹­ ì‹œì‘ (ë°°ì¹˜ ì²˜ë¦¬)")
        matched_categories = await match_categories_with_esg_and_issuepool(
            ranked_categories
        )
        matching_time = (datetime.now() - matching_start).total_seconds()
        logger.info(f"â±ï¸ ESG/ì´ìŠˆí’€ ë§¤ì¹­ ì™„ë£Œ: {matching_time:.2f}ì´ˆ")

        # 9) í†µê³„/ë¡œê¹…
        negative_count = sum(1 for a in labeled_articles if a["sentiment"] == "negative")
        logger.info(f"ğŸ“Š ë¶„ì„ ì™„ë£Œ í†µê³„:")
        logger.info(f"   - ë¶„ì„ëœ ê¸°ì‚¬ ìˆ˜: {len(labeled_articles)}")
        logger.info(f"   - ë¶€ì •ì  ê¸°ì‚¬ ìˆ˜: {negative_count}")
        logger.info(f"   - ë¶„ì„ëœ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(category_scores)}")
        logger.info(f"   - ë§¤ì¹­ëœ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(matched_categories)}")

                # ğŸ”¥ ìµœì¢… ì¹´í…Œê³ ë¦¬ ìˆœìœ„ (ìƒìœ„ 5ê°œë§Œ infoë¡œ)
        logger.info("\nğŸ† ìµœì¢… ì¹´í…Œê³ ë¦¬ ìˆœìœ„ (ìƒìœ„ 5ê°œ):")
        for i, row in enumerate(matched_categories[:5]):
            esg_name = row.get('esg_classification', 'ë¯¸ë¶„ë¥˜')
            issue_count = len(row.get('base_issuepools', []))
            final_score = row.get('final_score', 0.0)
            category_name = str(row['category']).strip()
            
            # ë¹ˆ ì¹´í…Œê³ ë¦¬ ì²´í¬
            if not category_name:
                logger.warning(f"ğŸš¨ {i+1:>2}ìœ„ | ë¹ˆ ì¹´í…Œê³ ë¦¬ ë°œê²¬! ì´ëŠ” ë°ì´í„° í’ˆì§ˆ ë¬¸ì œì…ë‹ˆë‹¤.")
                continue
                
            logger.info(
                f"{i+1:>2}ìœ„ | ì¹´í…Œê³ ë¦¬: {category_name} | ESG: {esg_name} | "
                f"ì´ìŠˆí’€: {issue_count}ê°œ | ìµœì¢…ì ìˆ˜: {final_score:.3f}"
            )
        
        # ë‚˜ë¨¸ì§€ëŠ” debug ë ˆë²¨ë¡œ
        if len(matched_categories) > 5:
            logger.debug(f"ğŸ“‹ ë‚˜ë¨¸ì§€ {len(matched_categories) - 5}ê°œ ì¹´í…Œê³ ë¦¬ (debug ë ˆë²¨)")
        
        # ğŸ”¥ ì „ì²´ ì¹´í…Œê³ ë¦¬ ìˆœìœ„ ìš”ì•½
        valid_categories = [cat for cat in matched_categories if str(cat['category']).strip()]
        empty_categories = [cat for cat in matched_categories if not str(cat['category']).strip()]
        
        logger.info(f"\nğŸ“‹ ì „ì²´ {len(matched_categories)}ê°œ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ ì™„ë£Œ")
        if empty_categories:
            logger.warning(f"ğŸš¨ ë¹ˆ ì¹´í…Œê³ ë¦¬ {len(empty_categories)}ê°œ ë°œê²¬: {[cat['category'] for cat in empty_categories]}")
            logger.warning(f"ğŸš¨ ìœ íš¨í•œ ì¹´í…Œê³ ë¦¬: {len(valid_categories)}ê°œ")
            logger.warning(f"ğŸš¨ ë¹ˆ ì¹´í…Œê³ ë¦¬ëŠ” í¬ë¡¤ë§ ë‹¨ê³„ì—ì„œ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ì´ ëˆ„ë½ëœ ê²ƒìœ¼ë¡œ, ë°ì´í„° í’ˆì§ˆ ë¬¸ì œì…ë‹ˆë‹¤.")
        else:
            logger.info(f"âœ… ëª¨ë“  ì¹´í…Œê³ ë¦¬ê°€ ìœ íš¨í•¨: {len(valid_categories)}ê°œ")
        
        # â±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ìš”ì•½
        total_time = (datetime.now() - start_time).total_seconds()
        logger.info("="*50)
        logger.info(f"â±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ìš”ì•½:")
        logger.info(f"   - ëª¨ë¸ ë¡œë“œ: {model_load_time:.2f}ì´ˆ")
        logger.info(f"   - ê°ì„± ë¶„ì„: {sentiment_time:.2f}ì´ˆ")
        logger.info(f"   - DB ì¡°íšŒ: {db_query_time:.2f}ì´ˆ")
        logger.info(f"   - ë¼ë²¨ ë¶€ì—¬: {labeling_time:.2f}ì´ˆ")
        logger.info(f"   - ì ìˆ˜ ê³„ì‚°: {scoring_time:.2f}ì´ˆ")
        logger.info(f"   - ì¹´í…Œê³ ë¦¬ ë­í‚¹: {ranking_time:.2f}ì´ˆ")
        logger.info(f"   - ESG/ì´ìŠˆí’€ ë§¤ì¹­: {matching_time:.2f}ì´ˆ")
        logger.info(f"   - ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
        logger.info("="*50)

        # 9) ì‘ë‹µ
        response_data = {
            "success": True,
            "message": "ì¤‘ëŒ€ì„± í‰ê°€ ë°ì´í„° ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "data": {
                "company_id": request.company_id,
                "report_period": request.report_period,
                "assessment_status": "analyzed",
                "total_articles": len(labeled_articles),
                "negative_articles": negative_count,
                "negative_ratio": (negative_count / len(labeled_articles))*100 if labeled_articles else 0.0,
                "total_categories": len(category_scores),
                "matched_categories": matched_categories,  # ESG ë¶„ë¥˜ ë° ì´ìŠˆí’€ ë§¤ì¹­ëœ ì¹´í…Œê³ ë¦¬
                "ranked_categories": ranked_categories[:20],  # ìƒìœ„ 20ê°œ (ì›ë³¸)
                # í•„ìš” ì‹œ í”„ë¡ íŠ¸ ë””ë²„ê¹…/ë¦¬ë·°ìš© ì›ìë£Œ
                "category_scores": category_scores,
                "analyzed_samples": labeled_articles[:3],
            }
        }

        logger.info("âœ… ë°ì´í„° ë¶„ì„ ì™„ë£Œ")
        logger.info("="*50)
        return response_data

    except Exception as e:
        error_msg = f"âŒ ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(error_msg)
        logger.error("="*50)
        return {"success": False, "message": error_msg, "data": None}


# ============================================================================
# ğŸš§ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ íƒ€ì„ì•„ì›ƒ ë˜í¼ í•¨ìˆ˜
# ============================================================================

async def start_assessment_with_timeout(request: MiddleIssueRequest, timeout_seconds: int = 1000) -> Dict[str, Any]:
    """
    ì¤‘ëŒ€ì„± í‰ê°€ë¥¼ íƒ€ì„ì•„ì›ƒê³¼ í•¨ê»˜ ì‹¤í–‰ (500 ì—ëŸ¬ ë°©ì§€)
    
    Args:
        request: ì¤‘ëŒ€ì„± í‰ê°€ ìš”ì²­
        timeout_seconds: íƒ€ì„ì•„ì›ƒ ì‹œê°„ (ì´ˆ), ê¸°ë³¸ê°’ 5ë¶„
    
    Returns:
        Dict[str, Any]: ì¤‘ëŒ€ì„± í‰ê°€ ê²°ê³¼ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ ì—ëŸ¬
    """
    try:
        import asyncio
        
        logger.warning(f"â° ì¤‘ëŒ€ì„± í‰ê°€ íƒ€ì„ì•„ì›ƒ ì„¤ì •: {timeout_seconds}ì´ˆ")
        logger.warning(f"ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ ì ìš©ë¨")
        logger.warning(f"ğŸ” ìš”ì²­ ì •ë³´: ê¸°ì—…={request.company_id}, ê¸°ì‚¬ìˆ˜={len(request.articles)}")
        
        # Serviceë¡œ ìš”ì²­ ì „ë‹¬ (íƒ€ì„ì•„ì›ƒ 5ë¶„ ì ìš©)
        logger.warning("ğŸš€ start_assessment í•¨ìˆ˜ í˜¸ì¶œ ì‹œì‘")
        
        try:
            # íƒ€ì„ì•„ì›ƒê³¼ í•¨ê»˜ ì¤‘ëŒ€ì„± í‰ê°€ ì‹¤í–‰
            result = await asyncio.wait_for(
                start_assessment(request), 
                timeout=timeout_seconds
            )
            
            logger.warning("âœ… start_assessment í•¨ìˆ˜ í˜¸ì¶œ ì™„ë£Œ")
            logger.warning("âœ… ì¤‘ëŒ€ì„± í‰ê°€ íƒ€ì„ì•„ì›ƒ ë‚´ ì™„ë£Œ")
            return result
        
        except asyncio.TimeoutError:
            error_msg = f"âŒ ì¤‘ëŒ€ì„± í‰ê°€ íƒ€ì„ì•„ì›ƒ ({timeout_seconds}ì´ˆ ì´ˆê³¼)"
            logger.error(error_msg)
            logger.error(f"ğŸ” íƒ€ì„ì•„ì›ƒ ë°œìƒ ìš”ì²­ ì •ë³´:")
            logger.error(f"   - ê¸°ì—…: {request.company_id}")
            logger.error(f"   - ê¸°ì‚¬ ìˆ˜: {len(request.articles)}")
            logger.error(f"   - ìš”ì²­ í¬ê¸°: {len(str(request))} bytes")
            logger.error("ğŸ’¡ ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹ ì ìš© í›„ì—ë„ íƒ€ì„ì•„ì›ƒ ë°œìƒ - ì¶”ê°€ ì„±ëŠ¥ ìµœì í™” í•„ìš”")
            logger.error("="*50)
            return {
                "success": False, 
                "message": error_msg, 
                "data": None,
                "timeout": True,
                "request_info": {
                    "company_id": request.company_id,
                    "article_count": len(request.articles),
                    "timeout_seconds": timeout_seconds
                }
            }
        except Exception as e:
            error_msg = f"âŒ ì¤‘ëŒ€ì„± í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}"
            logger.error(error_msg)
            logger.error(f"ğŸ” ì˜¤ë¥˜ ë°œìƒ ìš”ì²­ ì •ë³´:")
            logger.error(f"   - ê¸°ì—…: {request.company_id}")
            logger.error(f"   - ê¸°ì‚¬ ìˆ˜: {len(request.articles)}")
            logger.error(f"   - ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            logger.error("="*50)
            return {
                "success": False, 
                "message": error_msg, 
                "data": None,
                "error_type": type(e).__name__,
                "request_info": {
                    "company_id": request.company_id,
                    "article_count": len(request.articles)
                }
            }
    
    except Exception as e:
        error_msg = f"âŒ start_assessment_with_timeout í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}"
        logger.error(error_msg)
        logger.error(f"ğŸ” ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
        import traceback
        logger.error(f"âŒ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        return {
            "success": False, 
            "message": error_msg, 
            "data": None,
            "error_type": type(e).__name__
        }


# ============================================================================
# ğŸš§ ë””ë²„ê¹…ìš© Excel ë‚´ë³´ë‚´ê¸° í•¨ìˆ˜ë“¤ (ë‚˜ì¤‘ì— ì‚­ì œ ê°€ëŠ¥)
# ============================================================================

# ì—‘ì…€ ê¸°ëŠ¥ ì œê±° - 500 ì—ëŸ¬ ë°©ì§€
# def export_labeled_articles_to_excel(...)
# def export_category_scores_to_excel(...)
# def export_combined_analysis_to_excel(...)

# ============================================================================
# ğŸš§ ë””ë²„ê¹…ìš© Excel ë‚´ë³´ë‚´ê¸° í•¨ìˆ˜ë“¤ ë
# ============================================================================

def debug_labeling_results(labeled_articles: List[Dict[str, Any]], category_scores: Dict[str, Dict[str, Any]]):
    """
    ë¼ë²¨ë§ ê²°ê³¼ë¥¼ ë””ë²„ê¹…í•˜ê¸° ìœ„í•œ ê°„ë‹¨í•œ ë¶„ì„ í•¨ìˆ˜
    """
    try:
        logger.info("ğŸ” ë¼ë²¨ë§ ê²°ê³¼ ë””ë²„ê¹… ì‹œì‘")
        
        # 1. ë¼ë²¨ë§ í†µê³„
        total_articles = len(labeled_articles)
        logger.info(f"ğŸ“Š ì´ ê¸°ì‚¬ ìˆ˜: {total_articles}")
        
        if total_articles == 0:
            logger.warning("âš ï¸ ë¼ë²¨ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 2. ìµœì‹ ì„± ì ìˆ˜ ë¶„ì„
        recent_values = [a.get('recent_value', 0.0) for a in labeled_articles]
        recent_1_0 = sum(1 for v in recent_values if v == 1.0)
        recent_0_5 = sum(1 for v in recent_values if v == 0.5)
        recent_0_0 = sum(1 for v in recent_values if v == 0.0)
        
        logger.info(f"ğŸ” ìµœì‹ ì„± ì ìˆ˜ ë¶„ì„:")
        logger.info(f"   - 1.0 (3ê°œì›” ì´ë‚´): {recent_1_0}ê°œ ({recent_1_0/total_articles*100:.1f}%)")
        logger.info(f"   - 0.5 (3~6ê°œì›”): {recent_0_5}ê°œ ({recent_0_5/total_articles*100:.1f}%)")
        logger.info(f"   - 0.0 (6ê°œì›” ì´ìƒ): {recent_0_0}ê°œ ({recent_0_0/total_articles*100:.1f}%)")
        
        # 3. rank/reference ë¼ë²¨ ë¶„ì„
        rank_true = sum(1 for a in labeled_articles if a.get('rank_label'))
        reference_true = sum(1 for a in labeled_articles if a.get('reference_label'))
        
        logger.info(f"ğŸ” rank/reference ë¼ë²¨ ë¶„ì„:")
        logger.info(f"   - rank_label True: {rank_true}ê°œ ({rank_true/total_articles*100:.1f}%)")
        logger.info(f"   - reference_label True: {reference_true}ê°œ ({reference_true/total_articles*100:.1f}%)")
        
        # 4. ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ë¶„ì„
        if category_scores:
            logger.info(f"ğŸ” ì¹´í…Œê³ ë¦¬ ì ìˆ˜ ë¶„ì„:")
            for category, scores in list(category_scores.items())[:5]:  # ìƒìœ„ 5ê°œë§Œ
                logger.info(f"   - '{category}':")
                logger.info(f"     recent_score: {scores.get('recent_score', 0.0):.3f}")
                logger.info(f"     rank_score: {scores.get('rank_score', 0.0):.3f}")
                logger.info(f"     reference_score: {scores.get('reference_score', 0.0):.3f}")
        
        # 5. ìƒ˜í”Œ ê¸°ì‚¬ ë¶„ì„
        logger.info(f"ğŸ” ìƒ˜í”Œ ê¸°ì‚¬ ë¶„ì„ (ìƒìœ„ 3ê°œ):")
        for i, article in enumerate(labeled_articles[:3]):
            logger.info(f"   {i+1}. '{article.get('title', '')[:50]}...'")
            logger.info(f"      - recent_value: {article.get('recent_value', 0.0)}")
            logger.info(f"      - rank_label: {article.get('rank_label', False)}")
            logger.info(f"      - reference_label: {article.get('reference_label', False)}")
            logger.info(f"      - original_category: {article.get('original_category', 'N/A')}")
            logger.info(f"      - pubDate: {article.get('pubDate', 'N/A')}")
        
        # 6. ESG ë¶„ë¥˜ ì¡°íšŒ ê²°ê³¼ ë¶„ì„
        logger.info(f"ğŸ” ESG ë¶„ë¥˜ ì¡°íšŒ ê²°ê³¼ ë¶„ì„:")
        esg_results = {}
        for article in labeled_articles:
            category = article.get('original_category', '')
            if category:
                esg_results[category] = esg_results.get(category, 0) + 1
        
        logger.info(f"   - ì´ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(esg_results)}")
        logger.info(f"   - ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ìˆ˜: {esg_results}")
        
        logger.info("âœ… ë¼ë²¨ë§ ê²°ê³¼ ë””ë²„ê¹… ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ ë¼ë²¨ë§ ê²°ê³¼ ë””ë²„ê¹… ì¤‘ ì˜¤ë¥˜: {str(e)}")


# ============================================================================
# ğŸš§ ë””ë²„ê¹… í•¨ìˆ˜ ë
# ============================================================================

async def get_all_issuepool_data() -> Dict[str, Any]:
    """
    issuepool DBì—ì„œ ëª¨ë“  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    
    Returns:
        Dict[str, Any]: issuepool DBì˜ ëª¨ë“  ë°ì´í„° (ì¤‘ë³µ ì œê±°, í–‰ ë‹¨ìœ„ ë§¤ì¹­)
    """
    try:
        repository = MiddleIssueRepository()
        logger.warning("ğŸ” issuepool DB ì „ì²´ ë°ì´í„° ì¡°íšŒ ì‹œì‘")
        
        # 1. ëª¨ë“  ì¹´í…Œê³ ë¦¬ì™€ ESG ë¶„ë¥˜ ì •ë³´ ì¡°íšŒ
        all_categories = await repository.get_all_categories_with_esg()
        
        # 2. ëª¨ë“  base issue pool ì •ë³´ ì¡°íšŒ
        all_base_issuepools = await repository.get_all_base_issuepools()
        
        # 3. ë°ì´í„° êµ¬ì¡°í™” ë° ì¤‘ë³µ ì œê±°
        structured_data = {
            "matched_data": [],  # í–‰ ë‹¨ìœ„ë¡œ ë§¤ì¹­ëœ ë°ì´í„°
            "categories": [],
            "base_issuepools": [],
            "esg_classifications": [],
            "summary": {
                "total_categories": len(all_categories),
                "total_base_issuepools": len(all_base_issuepools),
                "total_matched_rows": 0,
                "esg_distribution": {}
            }
        }
        
        # ì¹´í…Œê³ ë¦¬ ë°ì´í„° êµ¬ì¡°í™”
        for cat in all_categories:
            category_info = {
                "id": cat.id,
                "category_name": cat.category_name,
                "esg_classification_id": cat.esg_classification_id,
                "esg_classification_name": cat.esg_classification_name if hasattr(cat, 'esg_classification_name') else None,
                "created_at": str(cat.created_at) if hasattr(cat, 'created_at') else None,
                "updated_at": str(cat.updated_at) if hasattr(cat, 'updated_at') else None
            }
            structured_data["categories"].append(category_info)
        
        # Base issue pool ë°ì´í„° êµ¬ì¡°í™”
        for pool in all_base_issuepools:
            pool_info = {
                "id": pool.id,
                "base_issue_pool": pool.base_issue_pool,
                "issue_pool": pool.issue_pool,
                "category_id": pool.category_id,
                "ranking": pool.ranking,
                "corporation_id": pool.corporation_id,
                "publish_year": pool.publish_year,
                "created_at": str(pool.created_at) if hasattr(pool, 'created_at') else None,
                "updated_at": str(pool.updated_at) if hasattr(pool, 'updated_at') else None
            }
            structured_data["base_issuepools"].append(pool_info)
        
        # 4. ğŸ”¥ í•µì‹¬: í–‰ ë‹¨ìœ„ë¡œ ë§¤ì¹­í•˜ê³  ì¤‘ë³µ ì œê±°
        logger.warning("ğŸ” í–‰ ë‹¨ìœ„ ë§¤ì¹­ ë° ì¤‘ë³µ ì œê±° ì‹œì‘")
        
        # base_issue_pool ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ set
        seen_base_issue_pools = set()
        matched_rows = []
        
        for pool in all_base_issuepools:
            # base_issue_pool ê°’ (ê³µë°± í¬í•¨)
            base_issue_pool_value = pool.base_issue_pool.strip() if pool.base_issue_pool else ""
            
            # ì¤‘ë³µ ì²´í¬ (ê³µë°±ì„ í¬í•¨í•œ ì •í™•í•œ ê°’ìœ¼ë¡œ)
            if base_issue_pool_value in seen_base_issue_pools:
                logger.debug(f"ğŸ” ì¤‘ë³µ ì œê±°: {base_issue_pool_value}")
                continue
            
            # ì¤‘ë³µì´ ì•„ë‹Œ ê²½ìš° setì— ì¶”ê°€
            seen_base_issue_pools.add(base_issue_pool_value)
            
            # í•´ë‹¹ poolì˜ category_idë¡œ ì¹´í…Œê³ ë¦¬ ì •ë³´ ì°¾ê¸°
            category_info = None
            for cat in all_categories:
                if cat.id == pool.category_id:
                    category_info = cat
                    break
            
            # ESG ë¶„ë¥˜ ì •ë³´
            esg_classification_name = "ë¯¸ë¶„ë¥˜"
            if category_info and hasattr(category_info, 'esg_classification_name'):
                esg_classification_name = category_info.esg_classification_name or "ë¯¸ë¶„ë¥˜"
            
            # í–‰ ë‹¨ìœ„ë¡œ ë§¤ì¹­ëœ ë°ì´í„° ìƒì„±
            matched_row = {
                "id": pool.id,
                "base_issue_pool": base_issue_pool_value,
                "issue_pool": pool.issue_pool,
                "category_id": pool.category_id,
                "category_name": category_info.category_name if category_info else "ë¯¸ë¶„ë¥˜",
                "esg_classification_id": category_info.esg_classification_id if category_info else None,
                "esg_classification_name": esg_classification_name,
                "ranking": pool.ranking,
                "corporation_id": pool.corporation_id,
                "publish_year": pool.publish_year,
                "created_at": str(pool.created_at) if hasattr(pool, 'created_at') else None,
                "updated_at": str(pool.updated_at) if hasattr(pool, 'updated_at') else None
            }
            
            matched_rows.append(matched_row)
        
        # ë§¤ì¹­ëœ ë°ì´í„°ë¥¼ structured_dataì— ì¶”ê°€
        structured_data["matched_data"] = matched_rows
        structured_data["summary"]["total_matched_rows"] = len(matched_rows)
        
        # 5. ESG ë¶„í¬ ê³„ì‚° (ì¤‘ë³µ ì œê±° í›„)
        esg_counts = {}
        for row in matched_rows:
            esg_name = row.get('esg_classification_name', 'ë¯¸ë¶„ë¥˜')
            esg_counts[esg_name] = esg_counts.get(esg_name, 0) + 1
        
        structured_data["summary"]["esg_distribution"] = esg_counts
        
        # 6. ë¡œê¹…
        logger.warning(f"âœ… issuepool DB ì „ì²´ ë°ì´í„° ì¡°íšŒ ì™„ë£Œ:")
        logger.warning(f"   - ì´ ì¹´í…Œê³ ë¦¬: {len(all_categories)}ê°œ")
        logger.warning(f"   - ì´ Base Issue Pool: {len(all_base_issuepools)}ê°œ")
        logger.warning(f"   - ì¤‘ë³µ ì œê±° í›„ ë§¤ì¹­ëœ í–‰: {len(matched_rows)}ê°œ")
        logger.warning(f"   - ì¤‘ë³µ ì œê±°ëœ í–‰: {len(all_base_issuepools) - len(matched_rows)}ê°œ")
        logger.warning(f"   - ESG ë¶„í¬: {esg_counts}")
        
        return {
            "success": True,
            "message": "issuepool DB ì „ì²´ ë°ì´í„° ì¡°íšŒ ì™„ë£Œ (ì¤‘ë³µ ì œê±°, í–‰ ë‹¨ìœ„ ë§¤ì¹­)",
            "data": structured_data
        }
        
    except Exception as e:
        error_msg = f"âŒ issuepool DB ì „ì²´ ë°ì´í„° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(error_msg)
        logger.error(f"ğŸ” ì˜¤ë¥˜ ìƒì„¸: {type(e).__name__}: {str(e)}")
        import traceback
        logger.error(f"âŒ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        
        return {
            "success": False,
            "message": error_msg,
            "data": None,
            "error": str(e)
        }
