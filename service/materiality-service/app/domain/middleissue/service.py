"""
Middleissue Service - ì¤‘ëŒ€ì„± í‰ê°€ ê´€ë ¨ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì²˜ë¦¬
í¬ë¡¤ë§ ë°ì´í„° ì²˜ë¦¬, ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì ìš©, ì ìˆ˜ ê³„ì‚° ë“±ì„ ë‹´ë‹¹
"""
# 1. í¬ë¡¤ë§í•œ ì „ì²´ ë°ì´í„° -> ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë¡œ ê¸ë¶€ì •í‰ê°€
# 2. relevance, recent, negative, rank(ê²€ìƒ‰ì—°ë„-1), reference(NULL) ë¼ë²¨ ë¶€ì—¬
# 3. ê° ì§€í‘œë³„ score ë¶€ì—¬
# 4. final score ê³„ì‚°
# 5. frontendë¡œ ë³´ë‚´ê³  ë©”ëª¨ë¦¬ ì €ì¥

import logging
import os
import re
import json
import joblib
import numpy as np
from datetime import datetime
from typing import Dict, Any, List, Set, Tuple

from dateutil import parser
from app.domain.middleissue.schema import (
    MiddleIssueRequest, MiddleIssueResponse, Article,
    CategoryDetailsResponse, BaseIssuePool
)
from app.domain.middleissue.repository import MiddleIssueRepository

# Railway í™˜ê²½ì—ì„œ ë¡œê·¸ ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€ë¥¼ ìœ„í•œ ë¡œê¹… ì„¤ì •
if os.getenv('RAILWAY_ENVIRONMENT') or True:  # ì¦‰ì‹œ ì ìš©ì„ ìœ„í•´ Trueë¡œ ì„¤ì •
    # Railway í™˜ê²½ì—ì„œëŠ” ë¡œê¹… ë ˆë²¨ì„ WARNINGìœ¼ë¡œ ì„¤ì •
    logging.getLogger('app.domain.middleissue.service').setLevel(logging.WARNING)
    logging.getLogger('app.domain.middleissue.repository').setLevel(logging.WARNING)
    print("ğŸš¨ Railway í™˜ê²½ ê°ì§€: ë¡œê¹… ë ˆë²¨ì„ WARNINGìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ë¡œê·¸ ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€")

logger = logging.getLogger(__name__)

# ë¡œê¹… ë ˆë²¨ ê°•ì œ ì„¤ì • (ì¦‰ì‹œ ì ìš©)
logger.setLevel(logging.WARNING)

NEGATIVE_LEXICON = {
    "ê°ì†Œ","í•˜ë½","ë¶€ì§„","ì•…í™”","ì˜¤ì—¼","ìœ„ë°˜","ë‹´í•©","ë¶€íŒ¨","ë‡Œë¬¼","íš¡ë ¹","ë°°ì„","ì‚¬ê¸°",
    "ê³¼ì§•ê¸ˆ","ë²Œê¸ˆ","ì‚¬ê³ ","ì‚¬ë§","íŒŒì—…","ë¶„ìŸ","ê°ˆë“±","ë…¼ë€","ì†Œì†¡","ë¦¬ì½œ","ê²°í•¨","ë¶ˆëŸ‰",
    "ëˆ„ì¶œ","ìœ ì¶œ","í™”ì¬","ì ì","íŒŒì‚°","êµ¬ì¡°ì¡°ì •","ì •ë¦¬í•´ê³ ","ì¤‘ë‹¨","ì°¨ì§ˆ","ì‹¤íŒ¨","ë¶ˆë²•",
    "ì² ìˆ˜","í‡´ì¶œ","ë¶€ì •","ë¶ˆê³µì •","ê°‘ì§ˆ","ì§ì¥ê´´ë¡­í˜","í­ì–¸","íš¡í¬","í™˜ë¶ˆ","íšŒìˆ˜","ì†ì‹¤",
    "ê²½ê³ ","ì œì¬","í•´ì§€","ì·¨ì†Œ","ë‚™ì œ","ë¶€ê³¼","ì§•ê³„","ì¤‘ì§•ê³„","ë¶€ì •ì²­íƒ","ê²½ì˜ê¶Œë¶„ìŸ","ìœ„ê¸°","ì²­ì‚°"
}

POSITIVE_LEXICON = {
    "ì„±ì¥","í™•ëŒ€","ì¦ê°€","ê°œì„ ","í˜¸ì¡°","í‘ì","ìµœê³ ","ì„ ì •","ìˆ˜ìƒ","í¬ìƒ","ì‚°ì—…í¬ì¥",
    "ê°•í™”","ìƒìƒ","í˜‘ë ¥","ë„ì…","ì¶œì‹œ","ì„ ë„","ì¸ì¦","í™•ë³´","ìš°ìˆ˜","ë„ì•½","í™•ì¥","íšŒë³µ",
    "ê³ ë„í™”","ìµœì í™”","ì•ˆì •í™”","ì‹ ì„¤","ì±„ìš©","ì¦ì„¤","ì¦ì‚°","í™•ì¶©","ê³µê¸‰","ìˆ˜ì£¼","ëª¨ë²”",
    "ë‹¬ì„±","ì‹ ê¸°ìˆ ","ê°œì‹œ","ì¦ë¹™","ì„±ê³¼","ë§¤ì¶œì¦ê°€","ê³ ì„±ì¥","ì„ ë„ê¸°ì—…","ìˆ˜ì¶œí™•ëŒ€",
    "í•´ì™¸ì§„ì¶œ","íŒŒíŠ¸ë„ˆì‹­","ë¦¬ë”","í‰íŒ","ì¬ìƒì—ë„ˆì§€","ê°ì¶•","ì´í–‰","í˜ì‹ ","ê°œë°œ","ì—­ëŒ€",
    "ìˆœí•­","ê»‘ì¶©","ê¸°ì¦","ê¸°ë¶€","ì „ë‹¬","ì§€ì›","ìº í˜ì¸","í›„ì›"
}

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
MODEL_PATH = os.path.join(
    os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
    'models',
    'model_multinomialnb.joblib'
)

# ì •ê·œì‹ íŒ¨í„´ ì»´íŒŒì¼
_NEG_RE = re.compile("|".join(map(re.escape, sorted(NEGATIVE_LEXICON, key=len, reverse=True))))
_POS_RE = re.compile("|".join(map(re.escape, sorted(POSITIVE_LEXICON, key=len, reverse=True))))

def parse_pubdate(date_str: str) -> datetime:
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ íŒŒì‹±"""
    try:
        # 1) ISO í˜•ì‹
        try:
            return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        except ValueError:
            pass
        # 2) RSS í˜•ì‹
        try:
            from email.utils import parsedate_to_datetime
            return parsedate_to_datetime(date_str)
        except Exception:
            pass
        # 3) ì¼ë°˜ íŒŒì„œ
        return parser.parse(date_str)
    except Exception as e:
        logger.warning(f"âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ({date_str}): {str(e)}")
        return datetime.now()  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜

def extract_keywords(text: str, patt: re.Pattern) -> List[str]:
    if not isinstance(text, str):
        return []
    return sorted(set(patt.findall(text)))

def load_sentiment_model():
    """ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ"""
    try:
        logger.info(f"ğŸ¤– ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹œë„: {MODEL_PATH}")
        model = joblib.load(MODEL_PATH)
        logger.info("âœ… ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        return model
    except Exception as e:
        logger.error(f"âŒ ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None

def analyze_sentiment(model, articles: List[Article]) -> List[Dict[str, Any]]:
    """ê¸°ì‚¬ ê°ì„± ë¶„ì„ ìˆ˜í–‰"""
    try:
        analyzed_articles: List[Dict[str, Any]] = []
        for article in articles:
            try:
                title_text = article.title
                desc_text = article.description
                full_text = f"{title_text} {desc_text}"

                # í‚¤ì›Œë“œ ê¸°ë°˜
                neg_keywords = extract_keywords(full_text, _NEG_RE)
                pos_keywords = extract_keywords(full_text, _POS_RE)
                has_both = len(neg_keywords) > 0 and len(pos_keywords) > 0

                # ëª¨ë¸ ê¸°ë°˜
                if model is not None:
                    try:
                        y_pred = model.predict([full_text])[0]
                        probas = model.predict_proba([full_text])[0]

                        classes = getattr(model.named_steps.get("clf", model), "classes_", None)
                        if classes is None:
                            classes = getattr(model, "classes_", None)

                        if classes is not None and "negative" in classes:
                            neg_idx = int(np.where(classes == "negative")[0][0])
                            neg_proba = float(probas[neg_idx])
                        else:
                            neg_proba = 0.0

                        if y_pred == "negative" and has_both:
                            final_sentiment = "other"
                            final_basis = "ë¶€ì •+ê¸ì • ë™ì‹œ ì¶œí˜„ â†’ other"
                        else:
                            final_sentiment = y_pred
                            final_basis = "ëª¨ë¸ ì˜ˆì¸¡ ìœ ì§€"
                    except Exception as e:
                        logger.error(f"âŒ ëª¨ë¸ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                        final_sentiment = "negative" if len(neg_keywords) > len(pos_keywords) else "other"
                        final_basis = "í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨ (ëª¨ë¸ ì‹¤íŒ¨)"
                        neg_proba = 1.0 if final_sentiment == "negative" else 0.0
                else:
                    final_sentiment = "negative" if len(neg_keywords) > len(pos_keywords) else "other"
                    final_basis = "í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨ (ëª¨ë¸ ì—†ìŒ)"
                    neg_proba = 1.0 if final_sentiment == "negative" else 0.0

                analyzed_articles.append({
                    "title": title_text,
                    "description": desc_text,
                    "sentiment": final_sentiment,
                    "sentiment_confidence": float(neg_proba if final_sentiment == "negative" else (1 - neg_proba)),
                    "neg_keywords": ", ".join(neg_keywords),
                    "pos_keywords": ", ".join(pos_keywords),
                    "sentiment_basis": final_basis,
                    "original_category": article.original_category,
                    "issue": article.issue,
                    "pubDate": article.pubDate,
                    "originallink": article.originallink,
                    "company": article.company,
                })

            except Exception as e:
                logger.error(f"âŒ ê¸°ì‚¬ ê°ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}; ì œëª©: {getattr(article, 'title', '')}")
                continue

        return analyzed_articles
    except Exception as e:
        logger.error(f"âŒ ê°ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []

async def add_relevance_labels(
    articles: List[Dict[str, Any]],
    company_id: str,
    search_date: datetime,
    prev_year_categories: Set[str],   # (ê²€ìƒ‰ ê¸°ì¤€ì—°ë„ - 1)ì˜ category id ì§‘í•©
    reference_categories: Set[str],   # publish_year = NULL/''/'0' ì˜ category id ì§‘í•©
) -> List[Dict[str, Any]]:
    """
    ë¼ë²¨ ì •ì˜:
    - relevance : ì œëª©ì— ê¸°ì—…ëª… í¬í•¨ì´ë©´ '++' (True)
    - recent    : 3ê°œì›” ì´ë‚´=1.0, 3~6ê°œì›”=0.5, ê·¸ ì™¸=0.0
    - rank      : original_category âˆˆ prev_year_categories â†’ True
    - reference : original_category âˆˆ reference_categories â†’ True
    """
    try:
        repository = MiddleIssueRepository()
        
        for a in articles:
            a["relevance_label"] = False
            a["recent_value"] = 0.0
            a["rank_label"] = False
            a["reference_label"] = False
            a["label_reasons"] = []

            # relevance
            title = a.get("title") or ""
            if isinstance(title, str) and company_id and company_id in title:
                a["relevance_label"] = True
                a["label_reasons"].append("ì œëª©ì— ê¸°ì—…ëª… í¬í•¨")

            # recent
            pub_str = a.get("pubDate")
            if pub_str:
                try:
                    pub_dt = parse_pubdate(pub_str)
                    months_diff = (search_date - pub_dt).days / 30
                    if months_diff <= 3:
                        a["recent_value"] = 1.0
                        a["label_reasons"].append("ìµœê·¼ 3ê°œì›” ì´ë‚´")
                    elif months_diff <= 6:
                        a["recent_value"] = 0.5
                        a["label_reasons"].append("ìµœê·¼ 3~6ê°œì›”")
                except Exception as e:
                    logger.warning(f"âš ï¸ recent ê³„ì‚° ì¤‘ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {e}")

            # rank/reference (ì¹´í…Œê³ ë¦¬ ì´ë¦„ì„ IDë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ)
            oc = a.get("original_category")
            if oc is not None:
                try:
                    # ì¹´í…Œê³ ë¦¬ ì´ë¦„ì„ IDë¡œ ë³€í™˜
                    category_id = await repository.get_category_id_by_name(str(oc))
                    if category_id is not None:
                        oc_key = str(category_id)
                        
                        if oc_key in prev_year_categories:
                            a["rank_label"] = True
                            a["label_reasons"].append("ì´ì „ë…„ë„ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­")

                        if oc_key in reference_categories:
                            a["reference_label"] = True
                            a["label_reasons"].append("ê³µí†µ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­")
                    else:
                        logger.warning(f"âš ï¸ ì¹´í…Œê³ ë¦¬ ì´ë¦„ '{oc}'ì„ IDë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŒ")
                except Exception as e:
                    logger.warning(f"âš ï¸ ì¹´í…Œê³ ë¦¬ ID ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {e}")

        return articles
    except Exception as e:
        logger.error(f"âŒ ë¼ë²¨ ë¶€ì—¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return articles

def calculate_category_scores(articles: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
    """
    ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°

    ì ìˆ˜ ì²´ê³„:
    - frequency_score: í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë¹ˆë„ (0~1)
    - relevance_score: ì¹´í…Œê³ ë¦¬ ê¸°ì‚¬ë“¤ì˜ relevance_label í‰ê·  (True=1, False=0)
    - recent_score   : ì¹´í…Œê³ ë¦¬ ê¸°ì‚¬ë“¤ì˜ recent_value í‰ê·  (1/0.5/0)
    - rank_score     : ì¹´í…Œê³ ë¦¬ ë‚´ rank_label ì¡´ì¬ ì—¬ë¶€(0/1)  â€» ì „ë¶€ ë™ì¼í•˜ë‹¤ëŠ” ê°€ì •
    - negative_score : ì¹´í…Œê³ ë¦¬ ë‚´ ë¶€ì • ê¸°ì‚¬ ë¹„ìœ¨ (0~1)
    - reference_score: ì¹´í…Œê³ ë¦¬ ë‚´ reference_label ì¡´ì¬ ì—¬ë¶€(0/1)

    ìµœì¢… ì ìˆ˜:
    final = 0.4*frequency
          + 0.6*relevance
          + 0.2*recent
          + 0.4*rank
          + 0.6*reference
          + 0.8*negative*(1 + 0.5*frequency + 0.5*relevance)
    """
    try:
        total_articles = len(articles)
        if total_articles == 0:
            return {}

        buckets: Dict[str, Dict[str, Any]] = {}
        for a in articles:
            cat = a.get("original_category")
            if cat is None:
                continue
            key = str(cat)
            b = buckets.setdefault(key, {
                "count": 0,
                "relevance_sum": 0.0,
                "recent_sum": 0.0,
                "negative_count": 0,
                "rank_sum": 0.0,        # rank_label í•©ê³„ë¡œ ë³€ê²½
                "reference_sum": 0.0,   # reference_label í•©ê³„ë¡œ ë³€ê²½
                "articles": []
            })

            b["count"] += 1
            b["articles"].append(a)
            b["relevance_sum"] += 1.0 if a.get("relevance_label") else 0.0
            b["recent_sum"] += float(a.get("recent_value", 0.0))
            
            # ì•ˆì „í•œ ë¶€ì •ì ìˆ˜ ê³„ì‚°
            sentiment = a.get("sentiment")
            if sentiment is not None:
                # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ë¹„êµ
                if str(sentiment).lower() == "negative":
                    b["negative_count"] += 1
                    logger.debug(f"ğŸ” ë¶€ì • ê¸°ì‚¬ ê°ì§€: {sentiment}")
                elif str(sentiment).lower() not in ["positive", "other"]:
                    logger.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ sentiment ê°’: '{sentiment}'")
            else:
                logger.warning(f"âš ï¸ sentiment ê°’ì´ Noneì¸ ê¸°ì‚¬ ë°œê²¬")
            
            # rankì™€ referenceë¥¼ í•©ê³„ë¡œ ëˆ„ì 
            rank_label = a.get("rank_label")
            reference_label = a.get("reference_label")
            
            # ì•ˆì „í•œ rank_label ì²˜ë¦¬
            if rank_label is not None:
                if isinstance(rank_label, bool):
                    b["rank_sum"] += 1.0 if rank_label else 0.0
                elif isinstance(rank_label, (int, float)):
                    b["rank_sum"] += float(rank_label)
                else:
                    logger.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ rank_label íƒ€ì…: {type(rank_label)}, ê°’: {rank_label}")
                    b["rank_sum"] += 0.0
            else:
                b["rank_sum"] += 0.0
            
            # ì•ˆì „í•œ reference_label ì²˜ë¦¬
            if reference_label is not None:
                if isinstance(reference_label, bool):
                    b["reference_sum"] += 1.0 if reference_label else 0.0
                elif isinstance(reference_label, (int, float)):
                    b["reference_sum"] += float(reference_label)
                else:
                    logger.warning(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ reference_label íƒ€ì…: {type(reference_label)}, ê°’: {reference_label}")
                    b["reference_sum"] += 0.0
            else:
                b["reference_sum"] += 0.0

        results: Dict[str, Dict[str, Any]] = {}
        for key, b in buckets.items():
            c = b["count"]
            
            # ì•ˆì „í•œ ë¹ˆë„ì ìˆ˜ ê³„ì‚°
            try:
                frequency = c / total_articles if total_articles > 0 else 0.0
                # ë…¼ë¦¬ì  ê²€ì¦: ë¹ˆë„ê°€ 1ì„ ì´ˆê³¼í•  ìˆ˜ ì—†ìŒ
                if frequency > 1.0:
                    logger.warning(f"âš ï¸ ì¹´í…Œê³ ë¦¬ '{key}' ë¹ˆë„ì ìˆ˜ ë¹„ì •ìƒ: {frequency:.4f} > 1.0, 1.0ìœ¼ë¡œ ì¡°ì •")
                    frequency = 1.0
            except Exception as e:
                logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ '{key}' ë¹ˆë„ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}, ê¸°ë³¸ê°’ 0.0 ì‚¬ìš©")
                frequency = 0.0
            
            # ì•ˆì „í•œ ë‹¤ë¥¸ ì ìˆ˜ë“¤ ê³„ì‚°
            relevance = (b["relevance_sum"] / c) if c > 0 else 0.0
            recent = (b["recent_sum"] / c) if c > 0 else 0.0
            rank = (b["rank_sum"] / c) if c > 0 else 0.0
            reference = (b["reference_sum"] / c) if c > 0 else 0.0
            negative = (b["negative_count"] / c) if c > 0 else 0.0

            # ì•ˆì „í•œ ìµœì¢… ì ìˆ˜ ê³„ì‚°
            try:
                final_score = (
                    0.4 * frequency
                    + 0.6 * relevance
                    + 0.2 * recent
                    + 0.4 * rank
                    + 0.6 * reference
                    + 0.8 * negative * (1 + 0.5 * frequency + 0.5 * relevance)
                )
                
                # ì ìˆ˜ ë²”ìœ„ ê²€ì¦ (0~10 ë²”ìœ„ë¡œ ê°€ì •)
                if final_score < 0:
                    logger.warning(f"âš ï¸ ì¹´í…Œê³ ë¦¬ '{key}' ìµœì¢…ì ìˆ˜ ìŒìˆ˜: {final_score:.6f}, 0ìœ¼ë¡œ ì¡°ì •")
                    final_score = 0.0
                elif final_score > 10:
                    logger.warning(f"âš ï¸ ì¹´í…Œê³ ë¦¬ '{key}' ìµœì¢…ì ìˆ˜ ê³¼ëŒ€: {final_score:.6f}, 10ìœ¼ë¡œ ì¡°ì •")
                    final_score = 10.0
                    
            except Exception as e:
                logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ '{key}' ìµœì¢…ì ìˆ˜ ê³„ì‚° ì˜¤ë¥˜: {e}, ê¸°ë³¸ê°’ 0.0 ì‚¬ìš©")
                final_score = 0.0

            results[key] = {
                "count": c,
                "frequency_score": round(frequency, 6),
                "relevance_score": round(relevance, 6),
                "recent_score": round(recent, 6),
                "rank_score": round(rank, 6),
                "reference_score": round(reference, 6),
                "negative_score": round(negative, 6),
                "final_score": round(final_score, 6),
                "articles": b["articles"],
            }
            
            # ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ë¡œê·¸ (ë¡œê·¸ ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€ë¥¼ ìœ„í•´ ì œê±°)
            # if logger.isEnabledFor(logging.INFO):
            #     logger.info(f"ğŸ” ì¹´í…Œê³ ë¦¬ '{key}' ì ìˆ˜ ê³„ì‚° ìƒì„¸:")
            #     logger.info(f"   - ë¹ˆë„: {c}/{total_articles} = {frequency:.4f}")
            #     logger.info(f"   - ê´€ë ¨ì„±: {b['relevance_sum']}/{c} = {relevance:.4f}")
            #     logger.info(f"   - ìµœì‹ ì„±: {b['recent_sum']}/{c} = {recent:.4f}")
            #     logger.info(f"   - ìˆœìœ„: {b['rank_sum']}/{c} = {rank:.4f}")
            #     logger.info(f"   - ì°¸ì¡°: {b['reference_sum']}/{c} = {reference:.4f}")
            #     logger.info(f"   - ë¶€ì •ì„±: {b['negative_count']}/{c} = {negative:.4f}")
            #     logger.info(f"   - ìµœì¢…ì ìˆ˜: {final_score:.6f}")

        return results
    except Exception as e:
        logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ ì ìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return {}

def rank_categories_by_score(category_scores: Dict[str, Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ì¹´í…Œê³ ë¦¬ë¥¼ final_score ê¸°ì¤€ìœ¼ë¡œ ìˆœìœ„ ë§¤ê¸°ê¸°"""
    try:
        ranked = sorted(
            category_scores.items(),
            key=lambda x: x[1]["final_score"],
            reverse=True
        )
        out: List[Dict[str, Any]] = []
        for idx, (cat, scores) in enumerate(ranked, start=1):
            out.append({
                "rank": idx,
                "category": cat,
                **scores
            })
        return out
    except Exception as e:
        logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ ìˆœìœ„ ë§¤ê¸°ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []

async def match_categories_with_esg_and_issuepool(
    ranked_categories: List[Dict[str, Any]], 
    company_id: str, 
    search_year: int
) -> List[Dict[str, Any]]:
    """
    ì¹´í…Œê³ ë¦¬ë³„ë¡œ ESG ë¶„ë¥˜ì™€ base_issuepoolì„ ë°°ì¹˜ ì¿¼ë¦¬ë¡œ ë§¤ì¹­
    
    ë§¤ì¹­ ê·œì¹™:
    1. materiality_category DBì—ì„œ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ESG ë¶„ë¥˜ë¥¼ ë°°ì¹˜ë¡œ ì¡°íšŒ (ì—°ë„ ì¡°ê±´ ì—†ìŒ)
    2. base_issuepoolì€ ìƒˆë¡œìš´ ë°°ì¹˜ ì¡°íšŒ ë©”ì„œë“œ ì‚¬ìš© (ì—°ë„ ì¡°ê±´ ì—†ìŒ - ì¹´í…Œê³ ë¦¬ë§Œ ë§¤ì¹­)
    3. ì¹´í…Œê³ ë¦¬ í•˜ë‚˜ë‹¹ ESG ë¶„ë¥˜ëŠ” í•˜ë‚˜, base_issuepoolì€ ì—¬ëŸ¬ ê°œ
    4. ì¤‘ë³µ ì œê±°ëŠ” ê³µë°±ì„ í¬í•¨í•œ ë¬¸ì ê·¸ëŒ€ë¡œ ë¹„êµ
    """
    try:
        repository = MiddleIssueRepository()
        
        logger.info(f"ğŸ” ë°°ì¹˜ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ ì‹œì‘ - ê¸°ì—…: {company_id}, ì—°ë„: {search_year}")
        logger.info(f"ğŸ” ë§¤ì¹­í•  ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(ranked_categories)}")
        
        # 1. ëª¨ë“  ì¹´í…Œê³ ë¦¬ í‚¤ ìˆ˜ì§‘
        category_keys = [str(cat['category']) for cat in ranked_categories]
        
        # 2. ë°°ì¹˜ë¡œ ESG ë¶„ë¥˜ ì¡°íšŒ (í•œ ë²ˆì— ëª¨ë“  ì¹´í…Œê³ ë¦¬)
        logger.info(f"ğŸ” ë°°ì¹˜ ESG ë¶„ë¥˜ ì¡°íšŒ ì‹œì‘: {len(category_keys)}ê°œ ì¹´í…Œê³ ë¦¬")
        esg_mapping = {}
        for category_key in category_keys:
            try:
                esg_classification = await repository.get_category_esg_direct(category_key)
                esg_mapping[category_key] = esg_classification or 'ë¯¸ë¶„ë¥˜'
            except Exception as e:
                logger.warning(f"âš ï¸ ì¹´í…Œê³ ë¦¬ '{category_key}' ESG ë¶„ë¥˜ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
                esg_mapping[category_key] = 'ë¯¸ë¶„ë¥˜'
        
        logger.info(f"âœ… ë°°ì¹˜ ESG ë¶„ë¥˜ ì¡°íšŒ ì™„ë£Œ: {len(esg_mapping)}ê°œ ì¹´í…Œê³ ë¦¬")
        
        # 3. ğŸ”¥ ìƒˆë¡œìš´ ë°°ì¹˜ ì¡°íšŒ ë©”ì„œë“œ ì‚¬ìš© (N+1 ë¬¸ì œ í•´ê²°)
        logger.warning(f"ğŸ” ë°°ì¹˜ Base IssuePool ì¡°íšŒ ì‹œì‘ (ìƒˆë¡œìš´ ë©”ì„œë“œ)")
        details_map = {}
        
        try:
            # ìƒˆë¡œìš´ ë°°ì¹˜ ì¡°íšŒ ë©”ì„œë“œ ì‚¬ìš© (ì—°ë„ ì¡°ê±´ ì—†ìŒ)
            details_map = await repository.get_categories_details_batch(
                corporation_name=company_id,
                categories=category_keys,
                year=search_year  # yearëŠ” ì „ë‹¬í•˜ë˜ ë‚´ë¶€ì—ì„œ ë¬´ì‹œë¨
            )
            
            logger.warning(f"âœ… ë°°ì¹˜ Base IssuePool ì¡°íšŒ ì™„ë£Œ: {len(details_map)}ê°œ ì¹´í…Œê³ ë¦¬")
            
        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ Base IssuePool ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ë¹ˆ ë°°ì—´ë¡œ ì„¤ì •
            details_map = {name: None for name in category_keys}
        
        # 4. ê²°ê³¼ ì¡°í•© (ë¹ ë¥¸ ì²˜ë¦¬)
        logger.warning(f"ğŸ” ê²°ê³¼ ì¡°í•© ì‹œì‘")
        matched_categories = []
        
        for category_info in ranked_categories:
            category_key = str(category_info['category'])
            
            # ë°°ì¹˜ ì¡°íšŒ ê²°ê³¼ì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            details = details_map.get(category_key)
            
            if details:
                # ë°°ì¹˜ ì¡°íšŒì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„° ì‚¬ìš©
                esg_classification = details.esg_classification_name or esg_mapping.get(category_key, 'ë¯¸ë¶„ë¥˜')
                esg_classification_id = details.esg_classification_id
                base_issuepools = []
                
                # BaseIssuePoolì„ dictë¡œ ë³€í™˜
                for issue in details.base_issuepools:
                    base_issuepools.append({
                        "id": issue.id,
                        "base_issue_pool": issue.base_issue_pool,
                        "issue_pool": issue.issue_pool,
                        "ranking": issue.ranking,
                        "esg_classification_id": esg_classification_id,
                        "esg_classification_name": esg_classification
                    })
                
                total_issuepools = len(base_issuepools)
            else:
                # ë°°ì¹˜ ì¡°íšŒì—ì„œ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’
                esg_classification = esg_mapping.get(category_key, 'ë¯¸ë¶„ë¥˜')
                esg_classification_id = None
                base_issuepools = []
                total_issuepools = 0
            
            matched_category = {
                **category_info,  # ê¸°ì¡´ ì ìˆ˜ ì •ë³´ ìœ ì§€
                "esg_classification": esg_classification,
                "esg_classification_id": esg_classification_id,
                "base_issuepools": base_issuepools,
                "total_issuepools": total_issuepools
            }
            
            matched_categories.append(matched_category)
        
        # 5. ìš”ì•½ ë¡œê¹… (ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ê°„ì†Œí™”)
        total_issuepools = sum(len(cat.get('base_issuepools', [])) for cat in matched_categories)
        esg_distribution = {}
        for esg in esg_mapping.values():
            esg_distribution[esg] = esg_distribution.get(esg, 0) + 1
        
        logger.warning(f"ğŸ”— ë°°ì¹˜ ë§¤ì¹­ ì™„ë£Œ:")
        logger.warning(f"   - ì´ ì¹´í…Œê³ ë¦¬: {len(matched_categories)}ê°œ")
        logger.warning(f"   - ì´ IssuePool: {total_issuepools}ê°œ")
        logger.warning(f"   - ESG ë¶„í¬: {esg_distribution}")
        
        # ğŸ” base_issue_pool ë‚´ìš© ìƒì„¸ í™•ì¸ (ìƒìœ„ 5ê°œ ì¹´í…Œê³ ë¦¬ë§Œ)
        logger.warning(f"ğŸ” base_issue_pool ìƒì„¸ ë‚´ìš© í™•ì¸ (ìƒìœ„ 5ê°œ ì¹´í…Œê³ ë¦¬):")
        for i, category_info in enumerate(matched_categories[:5]):
            category_key = str(category_info['category'])
            base_issuepools = category_info.get('base_issuepools', [])
            esg_name = category_info.get('esg_classification', 'ë¯¸ë¶„ë¥˜')
            
            logger.warning(f"   {i+1}. ì¹´í…Œê³ ë¦¬ '{category_key}' (ESG: {esg_name}):")
            logger.warning(f"      - base_issue_pool ìˆ˜: {len(base_issuepools)}ê°œ")
            
            if base_issuepools:
                # ìƒìœ„ 3ê°œë§Œ ìƒì„¸ ë¡œê¹…
                for j, pool in enumerate(base_issuepools[:3]):
                    logger.warning(f"        {j+1}. {pool.get('base_issue_pool', 'N/A')}")
                    logger.warning(f"           issue_pool: {pool.get('issue_pool', 'N/A')}")
                    logger.warning(f"           ranking: {pool.get('ranking', 'N/A')}")
                
                if len(base_issuepools) > 3:
                    logger.warning(f"        ... ì™¸ {len(base_issuepools) - 3}ê°œ")
            else:
                logger.warning(f"        - base_issue_pool ì—†ìŒ")
        
        # ë‚˜ë¨¸ì§€ ì¹´í…Œê³ ë¦¬ëŠ” ìš”ì•½ë§Œ
        if len(matched_categories) > 5:
            remaining_categories = matched_categories[5:]
            remaining_issuepools = sum(len(cat.get('base_issuepools', [])) for cat in remaining_categories)
            logger.warning(f"   ... ë‚˜ë¨¸ì§€ {len(remaining_categories)}ê°œ ì¹´í…Œê³ ë¦¬: ì´ {remaining_issuepools}ê°œ base_issue_pool")
        
        # ğŸ” base_issue_pool ë§¤ì¹­ ê²°ê³¼ ìš”ì•½ (í•µì‹¬ë§Œ)
        matched_count = sum(1 for cat in matched_categories if cat.get('total_issuepools', 0) > 0)
        unmatched_count = len(matched_categories) - matched_count
        
        logger.warning(f"ğŸ” Base IssuePool ë§¤ì¹­ ê²°ê³¼:")
        logger.warning(f"   - ë§¤ì¹­ ì„±ê³µ: {matched_count}ê°œ ì¹´í…Œê³ ë¦¬")
        logger.warning(f"   - ë§¤ì¹­ ì‹¤íŒ¨: {unmatched_count}ê°œ ì¹´í…Œê³ ë¦¬")
        
        if unmatched_count > 0:
            unmatched_categories = [cat['category'] for cat in matched_categories if cat.get('total_issuepools', 0) == 0]
            logger.warning(f"   - ë§¤ì¹­ ì‹¤íŒ¨ ì¹´í…Œê³ ë¦¬: {unmatched_categories}")
        
        return matched_categories
        
    except Exception as e:
        logger.error(f"âŒ ë°°ì¹˜ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ ì¤‘ ì „ì²´ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        logger.error(f"âŒ ì˜¤ë¥˜ ìƒì„¸: {type(e).__name__}: {str(e)}")
        import traceback
        logger.error(f"âŒ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ì¡´ ê°œë³„ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ fallback
        logger.info(f"ğŸ”„ ê¸°ì¡´ ê°œë³„ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ fallback")
        return await _fallback_individual_matching(ranked_categories, company_id, search_year)


async def _fallback_individual_matching(
    ranked_categories: List[Dict[str, Any]], 
    company_id: str, 
    search_year: int
) -> List[Dict[str, Any]]:
    """
    Fallback: ê¸°ì¡´ ê°œë³„ ì²˜ë¦¬ ë°©ì‹ (ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì‚¬ìš©)
    """
    try:
        repository = MiddleIssueRepository()
        matched_categories = []
        
        logger.info(f"ğŸ”„ Fallback ê°œë³„ ì²˜ë¦¬ ì‹œì‘: {len(ranked_categories)}ê°œ ì¹´í…Œê³ ë¦¬")
        
        for category_info in ranked_categories:
            category_name = str(category_info['category'])
            
            try:
                # 1. materiality_category DBì—ì„œ ESG ë¶„ë¥˜ ì¡°íšŒ (ì—°ë„ ì¡°ê±´ ì—†ìŒ)
                esg_classification = await repository.get_category_esg_direct(category_name)
                if not esg_classification:
                    esg_classification = 'ë¯¸ë¶„ë¥˜'
                
                # 2. base_issuepool ì •ë³´ ì¡°íšŒ (ì—°ë„ ì¡°ê±´ ì—†ìŒ - ì¹´í…Œê³ ë¦¬ë§Œ ë§¤ì¹­)
                base_issuepools = []
                try:
                    if category_name.isdigit():
                        # IDë¡œ ì¡°íšŒí•˜ë˜ ì—°ë„ ì¡°ê±´ ì œê±°
                        category_details = await repository.get_category_details(
                            corporation_name=company_id,
                            category_id=int(category_name),
                            year=search_year,  # yearëŠ” ì „ë‹¬í•˜ë˜ ë‚´ë¶€ì—ì„œ ë¬´ì‹œë¨
                        )
                    else:
                        # ì´ë¦„ìœ¼ë¡œ ì§ì ‘ ì¡°íšŒí•˜ë˜ ì—°ë„ ì¡°ê±´ ì œê±°
                        category_details = await repository.get_category_by_name_direct(
                            corporation_name=company_id,
                            category_name=category_name,
                            year=search_year,  # yearëŠ” ì „ë‹¬í•˜ë˜ ë‚´ë¶€ì—ì„œ ë¬´ì‹œë¨
                        )
                    
                    if category_details and category_details.base_issuepools:
                        # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ set ì‚¬ìš©
                        seen_pools = set()
                        for issue in category_details.base_issuepools:
                            # ê³µë°±ì„ í¬í•¨í•œ ë¬¸ì ê·¸ëŒ€ë¡œ ë¹„êµí•˜ì—¬ ì¤‘ë³µ ì²´í¬
                            pool_key = (issue.base_issue_pool, issue.issue_pool)
                            if pool_key not in seen_pools:
                                seen_pools.add(pool_key)
                                base_issuepools.append({
                                    "id": issue.id,
                                    "base_issue_pool": issue.base_issue_pool,
                                    "issue_pool": issue.issue_pool,
                                    "ranking": issue.ranking,
                                    "esg_classification_id": category_details.esg_classification_id,
                                    "esg_classification_name": esg_classification
                                })
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ Fallback: ì¹´í…Œê³ ë¦¬ '{category_name}' base_issuepool ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
                
                # 3. ë§¤ì¹­ëœ ì¹´í…Œê³ ë¦¬ ì •ë³´ ìƒì„±
                matched_category = {
                    **category_info,
                    "esg_classification": esg_classification,
                    "esg_classification_id": None,
                    "base_issuepools": base_issuepools,
                    "total_issuepools": len(base_issuepools)
                }
                
                matched_categories.append(matched_category)
                
            except Exception as e:
                logger.error(f"âŒ Fallback: ì¹´í…Œê³ ë¦¬ '{category_name}' ë§¤ì¹­ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                matched_category = {
                    **category_info,
                    "esg_classification": "ë¯¸ë¶„ë¥˜",
                    "esg_classification_id": None,
                    "base_issuepools": [],
                    "total_issuepools": 0
                }
                matched_categories.append(matched_category)
        
        logger.info(f"ğŸ”„ Fallback ê°œë³„ ì²˜ë¦¬ ì™„ë£Œ: {len(matched_categories)}ê°œ ì¹´í…Œê³ ë¦¬")
        return matched_categories
        
    except Exception as e:
        logger.error(f"âŒ Fallback ê°œë³„ ì²˜ë¦¬ë„ ì‹¤íŒ¨: {str(e)}")
        # ìµœí›„ ìˆ˜ë‹¨: ì›ë³¸ ì¹´í…Œê³ ë¦¬ ì •ë³´ë§Œ ë°˜í™˜
        return ranked_categories

async def start_assessment(request: MiddleIssueRequest) -> Dict[str, Any]:
    """
    ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘ - í¬ë¡¤ë§ ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ ì‹œì‘
    """
    try:
        # 1) ìš”ì²­ ë¡œê¹…
        start_time = datetime.now()
        logger.info("="*50)
        logger.info("ğŸš€ ìƒˆë¡œìš´ ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘")
        logger.info(f"ê¸°ì—…ëª…: {request.company_id}")
        logger.info(f"ë³´ê³ ê¸°ê°„: {request.report_period}")
        logger.info(f"ìš”ì²­ íƒ€ì…: {request.request_type}")
        logger.info(f"ì´ í¬ë¡¤ë§ ê¸°ì‚¬ ìˆ˜: {request.total_results}")
        logger.info("-"*50)

        # 2) ëª¨ë¸ ë¡œë“œ
        model_start = datetime.now()
        logger.info("ğŸ”¥ í¬ë¡¤ë§ ë°ì´í„° ê°ì„± ë¶„ì„ ì‹œì‘")
        model = load_sentiment_model()
        if not model:
            raise Exception("ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
        model_load_time = (datetime.now() - model_start).total_seconds()
        logger.info(f"â±ï¸ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_load_time:.2f}ì´ˆ")

        # 3) ê°ì„± ë¶„ì„
        sentiment_start = datetime.now()
        logger.info("ğŸ”¥ í¬ë¡¤ë§ ë°ì´í„° ê°ì„± ë¶„ì„ ì‹œì‘")
        analyzed_articles = analyze_sentiment(model, request.articles)
        sentiment_time = (datetime.now() - sentiment_start).total_seconds()
        logger.info(f"â±ï¸ ê°ì„± ë¶„ì„ ì™„ë£Œ: {sentiment_time:.2f}ì´ˆ ({len(analyzed_articles)}ê°œ ê¸°ì‚¬)")

        # 4) (ê²€ìƒ‰ ê¸°ì¤€ì—°ë„ - 1) & ê³µí†µ(NULL) ì¹´í…Œê³ ë¦¬ ì¡°íšŒ
        db_query_start = datetime.now()
        repository = MiddleIssueRepository()
        search_year = int(request.report_period["end_date"][:4])  # ê²€ìƒ‰ ê¸°ì¤€ì—°ë„ (YYYY)
        
        # repository ë‚´ë¶€ì—ì„œ -1 ì²˜ë¦¬í•˜ë¯€ë¡œ search_yearë¥¼ ê·¸ëŒ€ë¡œ ì „ë‹¬
        corp_issues_prev = await repository.get_corporation_issues(
            corporation_name=request.company_id,
            year=search_year  # repository ë‚´ë¶€ì—ì„œ -1 ì²˜ë¦¬
        )
        # 2. prev_year_categoriesì™€ reference_categories ìˆ˜ì§‘
        # prev_year ê¸°ì¤€ ì¹´í…Œê³ ë¦¬ì™€ ê³µí†µ(NULL/ë¹ˆë¬¸ìì—´/'0') ì¹´í…Œê³ ë¦¬ ì„¸íŠ¸
        prev_year_categories = {str(issue.category_id) for issue in corp_issues_prev.year_issues}
        reference_categories = {str(issue.category_id) for issue in corp_issues_prev.common_issues}
        
        logger.info(f"ğŸ” prev_year_categories: {len(prev_year_categories)}ê°œ, reference_categories: {len(reference_categories)}ê°œ")
        db_query_time = (datetime.now() - db_query_start).total_seconds()
        logger.info(f"â±ï¸ DB ì¡°íšŒ ì™„ë£Œ: {db_query_time:.2f}ì´ˆ")

        # 5) ë¼ë²¨ ë¶€ì—¬
        labeling_start = datetime.now()
        logger.info("ğŸ·ï¸ ë¼ë²¨(relevance/recent/rank/reference) ë¶€ì—¬ ì‹œì‘")
        search_date = datetime.now()
        labeled_articles = await add_relevance_labels(
            analyzed_articles,
            request.company_id,
            search_date,
            prev_year_categories,
            reference_categories
        )
        labeling_time = (datetime.now() - labeling_start).total_seconds()
        logger.info(f"â±ï¸ ë¼ë²¨ ë¶€ì—¬ ì™„ë£Œ: {labeling_time:.2f}ì´ˆ")

        # 6) ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°
        scoring_start = datetime.now()
        logger.info("ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚° ì‹œì‘")
        category_scores = calculate_category_scores(labeled_articles)
        scoring_time = (datetime.now() - scoring_start).total_seconds()
        logger.info(f"â±ï¸ ì ìˆ˜ ê³„ì‚° ì™„ë£Œ: {scoring_time:.2f}ì´ˆ")

        # ğŸ” ë””ë²„ê¹…: ë¼ë²¨ë§ ê²°ê³¼ ë¶„ì„
        debug_labeling_results(labeled_articles, category_scores)

        # ğŸ“Š ë””ë²„ê¹…ìš© Excel íŒŒì¼ ìë™ ìƒì„±
        excel_start = datetime.now()
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            company_name = request.company_id.replace(" ", "_").replace("/", "_")
            
            # ì ˆëŒ€ ê²½ë¡œë¡œ ì €ì¥ (ì„œë²„ì˜ /tmp ë””ë ‰í† ë¦¬ ë˜ëŠ” í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬)
            import os
            current_dir = os.getcwd()
            logger.info(f"ğŸ“ í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {current_dir}")
            
            # 1. í†µí•© ë¶„ì„ Excel (ë¼ë²¨ë§ + ì ìˆ˜)
            combined_filename = os.path.join(current_dir, f"combined_analysis_{company_name}_{timestamp}.xlsx")
            export_combined_analysis_to_excel(labeled_articles, category_scores, combined_filename)
            
            # 2. ë¼ë²¨ë§ëœ ê¸°ì‚¬ Excel
            labeled_filename = os.path.join(current_dir, f"labeled_articles_{company_name}_{timestamp}.xlsx")
            export_labeled_articles_to_excel(labeled_articles, labeled_filename)
            
            # 3. ì¹´í…Œê³ ë¦¬ ì ìˆ˜ Excel
            scores_filename = os.path.join(current_dir, f"category_scores_{company_name}_{timestamp}.xlsx")
            export_category_scores_to_excel(category_scores, scores_filename)
            
            logger.info(f"ğŸ“Š ë””ë²„ê¹…ìš© Excel íŒŒì¼ ìƒì„± ì™„ë£Œ:")
            logger.info(f"   - í†µí•© ë¶„ì„: {combined_filename}")
            logger.info(f"   - ë¼ë²¨ë§ ê¸°ì‚¬: {labeled_filename}")
            logger.info(f"   - ì¹´í…Œê³ ë¦¬ ì ìˆ˜: {scores_filename}")
            logger.info(f"ğŸ“ íŒŒì¼ ì €ì¥ ìœ„ì¹˜: {current_dir}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ Excel íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        
        excel_time = (datetime.now() - excel_start).total_seconds()
        logger.info(f"â±ï¸ Excel íŒŒì¼ ìƒì„± ì™„ë£Œ: {excel_time:.2f}ì´ˆ")

        # 7) ì¹´í…Œê³ ë¦¬ ë­í‚¹
        ranking_start = datetime.now()
        logger.info("ğŸ† ì¹´í…Œê³ ë¦¬ ìˆœìœ„ ë§¤ê¸°ê¸° ì‹œì‘")
        ranked_categories = rank_categories_by_score(category_scores)
        ranking_time = (datetime.now() - ranking_start).total_seconds()
        logger.info(f"â±ï¸ ì¹´í…Œê³ ë¦¬ ë­í‚¹ ì™„ë£Œ: {ranking_time:.2f}ì´ˆ")

        # 8) ì¹´í…Œê³ ë¦¬ë³„ ESG ë¶„ë¥˜ ë° ì´ìŠˆí’€ ë§¤ì¹­ (ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ)
        matching_start = datetime.now()
        logger.info("ğŸ”— ì¹´í…Œê³ ë¦¬ë³„ ESG ë¶„ë¥˜ ë° ì´ìŠˆí’€ ë§¤ì¹­ ì‹œì‘ (ë°°ì¹˜ ì²˜ë¦¬)")
        matched_categories = await match_categories_with_esg_and_issuepool(
            ranked_categories, 
            request.company_id, 
            search_year
        )
        matching_time = (datetime.now() - matching_start).total_seconds()
        logger.info(f"â±ï¸ ESG/ì´ìŠˆí’€ ë§¤ì¹­ ì™„ë£Œ: {matching_time:.2f}ì´ˆ")

        # 9) í†µê³„/ë¡œê¹…
        negative_count = sum(1 for a in labeled_articles if a["sentiment"] == "negative")
        logger.info(f"ğŸ“Š ë¶„ì„ ì™„ë£Œ í†µê³„:")
        logger.info(f"   - ë¶„ì„ëœ ê¸°ì‚¬ ìˆ˜: {len(labeled_articles)}")
        logger.info(f"   - ë¶€ì •ì  ê¸°ì‚¬ ìˆ˜: {negative_count}")
        logger.info(f"   - ë¶„ì„ëœ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(category_scores)}")
        logger.info(f"   - ë§¤ì¹­ëœ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(matched_categories)}")

        # ğŸ”¥ ìµœì¢… ì¹´í…Œê³ ë¦¬ ìˆœìœ„ (ìƒìœ„ 5ê°œë§Œ infoë¡œ)
        logger.info("\nğŸ† ìµœì¢… ì¹´í…Œê³ ë¦¬ ìˆœìœ„ (ìƒìœ„ 5ê°œ):")
        for i, row in enumerate(matched_categories[:5]):
            esg_name = row.get('esg_classification', 'ë¯¸ë¶„ë¥˜')
            issue_count = len(row.get('base_issuepools', []))
            final_score = row.get('final_score', 0.0)
            logger.info(
                f"{i+1:>2}ìœ„ | ì¹´í…Œê³ ë¦¬: {row['category']} | ESG: {esg_name} | "
                f"ì´ìŠˆí’€: {issue_count}ê°œ | ìµœì¢…ì ìˆ˜: {final_score:.3f}"
            )
        
        # ë‚˜ë¨¸ì§€ëŠ” debug ë ˆë²¨ë¡œ
        if len(matched_categories) > 5:
            logger.debug(f"ğŸ“‹ ë‚˜ë¨¸ì§€ {len(matched_categories) - 5}ê°œ ì¹´í…Œê³ ë¦¬ (debug ë ˆë²¨)")

        # ğŸ”¥ ì „ì²´ ì¹´í…Œê³ ë¦¬ ìˆœìœ„ ìš”ì•½
        logger.info(f"\nğŸ“‹ ì „ì²´ {len(matched_categories)}ê°œ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ ì™„ë£Œ")
        
        # â±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ìš”ì•½
        total_time = (datetime.now() - start_time).total_seconds()
        logger.info("="*50)
        logger.info(f"â±ï¸ ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ìš”ì•½:")
        logger.info(f"   - ëª¨ë¸ ë¡œë“œ: {model_load_time:.2f}ì´ˆ")
        logger.info(f"   - ê°ì„± ë¶„ì„: {sentiment_time:.2f}ì´ˆ")
        logger.info(f"   - DB ì¡°íšŒ: {db_query_time:.2f}ì´ˆ")
        logger.info(f"   - ë¼ë²¨ ë¶€ì—¬: {labeling_time:.2f}ì´ˆ")
        logger.info(f"   - ì ìˆ˜ ê³„ì‚°: {scoring_time:.2f}ì´ˆ")
        logger.info(f"   - Excel ìƒì„±: {excel_time:.2f}ì´ˆ")
        logger.info(f"   - ì¹´í…Œê³ ë¦¬ ë­í‚¹: {ranking_time:.2f}ì´ˆ")
        logger.info(f"   - ESG/ì´ìŠˆí’€ ë§¤ì¹­: {matching_time:.2f}ì´ˆ")
        logger.info(f"   - ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
        logger.info("="*50)

        # 9) ì‘ë‹µ
        response_data = {
            "success": True,
            "message": "ì¤‘ëŒ€ì„± í‰ê°€ ë°ì´í„° ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "data": {
                "company_id": request.company_id,
                "report_period": request.report_period,
                "assessment_status": "analyzed",
                "total_articles": len(labeled_articles),
                "negative_articles": negative_count,
                "negative_ratio": (negative_count / len(labeled_articles))*100 if labeled_articles else 0.0,
                "total_categories": len(category_scores),
                "matched_categories": matched_categories,  # ESG ë¶„ë¥˜ ë° ì´ìŠˆí’€ ë§¤ì¹­ëœ ì¹´í…Œê³ ë¦¬
                "ranked_categories": ranked_categories[:20],  # ìƒìœ„ 20ê°œ (ì›ë³¸)
                # í•„ìš” ì‹œ í”„ë¡ íŠ¸ ë””ë²„ê¹…/ë¦¬ë·°ìš© ì›ìë£Œ
                "category_scores": category_scores,
                "analyzed_samples": labeled_articles[:3],
            }
        }

        logger.info("âœ… ë°ì´í„° ë¶„ì„ ì™„ë£Œ")
        logger.info("="*50)
        return response_data

    except Exception as e:
        error_msg = f"âŒ ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(error_msg)
        logger.error("="*50)
        return {"success": False, "message": error_msg, "data": None}


# ============================================================================
# ğŸš§ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ íƒ€ì„ì•„ì›ƒ ë˜í¼ í•¨ìˆ˜
# ============================================================================

async def start_assessment_with_timeout(request: MiddleIssueRequest, timeout_seconds: int = 300) -> Dict[str, Any]:
    """
    ì¤‘ëŒ€ì„± í‰ê°€ë¥¼ íƒ€ì„ì•„ì›ƒê³¼ í•¨ê»˜ ì‹¤í–‰ (500 ì—ëŸ¬ ë°©ì§€)
    
    Args:
        request: ì¤‘ëŒ€ì„± í‰ê°€ ìš”ì²­
        timeout_seconds: íƒ€ì„ì•„ì›ƒ ì‹œê°„ (ì´ˆ), ê¸°ë³¸ê°’ 5ë¶„
    
    Returns:
        Dict[str, Any]: ì¤‘ëŒ€ì„± í‰ê°€ ê²°ê³¼ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ ì—ëŸ¬
    """
    try:
        import asyncio
        
        logger.info(f"â° ì¤‘ëŒ€ì„± í‰ê°€ íƒ€ì„ì•„ì›ƒ ì„¤ì •: {timeout_seconds}ì´ˆ")
        logger.info(f"ğŸš€ ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ ì ìš©ë¨")
        
        # íƒ€ì„ì•„ì›ƒê³¼ í•¨ê»˜ ì¤‘ëŒ€ì„± í‰ê°€ ì‹¤í–‰
        result = await asyncio.wait_for(
            start_assessment(request), 
            timeout=timeout_seconds
        )
        
        logger.info("âœ… ì¤‘ëŒ€ì„± í‰ê°€ íƒ€ì„ì•„ì›ƒ ë‚´ ì™„ë£Œ")
        return result
        
    except asyncio.TimeoutError:
        error_msg = f"âŒ ì¤‘ëŒ€ì„± í‰ê°€ íƒ€ì„ì•„ì›ƒ ({timeout_seconds}ì´ˆ ì´ˆê³¼)"
        logger.error(error_msg)
        logger.error("ğŸ’¡ ë°°ì¹˜ ì²˜ë¦¬ ë°©ì‹ ì ìš© í›„ì—ë„ íƒ€ì„ì•„ì›ƒ ë°œìƒ - ì¶”ê°€ ì„±ëŠ¥ ìµœì í™” í•„ìš”")
        logger.error("="*50)
        return {
            "success": False, 
            "message": error_msg, 
            "data": None,
            "timeout": True
        }
    except Exception as e:
        error_msg = f"âŒ ì¤‘ëŒ€ì„± í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}"
        logger.error(error_msg)
        logger.error("="*50)
        return {
            "success": False, 
            "message": error_msg, 
            "data": None
        }


# ============================================================================
# ğŸš§ ë””ë²„ê¹…ìš© Excel ë‚´ë³´ë‚´ê¸° í•¨ìˆ˜ë“¤ (ë‚˜ì¤‘ì— ì‚­ì œ ê°€ëŠ¥)
# ============================================================================

def export_labeled_articles_to_excel(labeled_articles: List[Dict[str, Any]], output_path: str = "labeled_articles_debug.xlsx"):
    """
    ë¼ë²¨ë§ëœ ê¸°ì‚¬ë“¤ì„ Excelë¡œ ë‚´ë³´ë‚´ê¸° (ë””ë²„ê¹…ìš©)
    
    Args:
        labeled_articles: ë¼ë²¨ë§ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    
    Note: ì´ í•¨ìˆ˜ëŠ” ë””ë²„ê¹… ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©ë˜ë©°, ë‚˜ì¤‘ì— ì‚­ì œ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """
    try:
        import pandas as pd
        from datetime import datetime
        
        logger.info(f"ğŸ“Š ë¼ë²¨ë§ëœ ê¸°ì‚¬ Excel ë‚´ë³´ë‚´ê¸° ì‹œì‘: {len(labeled_articles)}ê°œ ê¸°ì‚¬")
        
        # 1. Raw Data ì‹œíŠ¸
        raw_data = []
        for article in labeled_articles:
            raw_data.append({
                'company': article.get('company', ''),
                'issue': article.get('issue', ''),
                'original_category': article.get('original_category', ''),
                'title': article.get('title', ''),
                'description': article.get('description', ''),
                'pubDate': article.get('pubDate', ''),
                'sentiment': article.get('sentiment', ''),
                'relevance_label': article.get('relevance_label', False),
                'recent_label': article.get('recent_label', False),
                'rank_label': article.get('rank_label', False),
                'reference_label': article.get('reference_label', False),
                'relevance_score': article.get('relevance_score', 0.0),
                'recent_score': article.get('recent_score', 0.0),
                'rank_score': article.get('rank_score', 0.0),
                'reference_score': article.get('reference_score', 0.0),
                'negative_score': article.get('negative_score', 0.0)
            })
        
        # 2. Summary ì‹œíŠ¸
        summary_data = {
            'ì´ ê¸°ì‚¬ ìˆ˜': len(labeled_articles),
            'ë¶€ì •ì  ê¸°ì‚¬ ìˆ˜': sum(1 for a in labeled_articles if a.get('sentiment') == 'negative'),
            'ê¸ì •ì  ê¸°ì‚¬ ìˆ˜': sum(1 for a in labeled_articles if a.get('sentiment') == 'positive'),
            'ì¤‘ë¦½ì  ê¸°ì‚¬ ìˆ˜': sum(1 for a in labeled_articles if a.get('sentiment') == 'neutral'),
            'relevance_label True': sum(1 for a in labeled_articles if a.get('relevance_label')),
            'recent_label True': sum(1 for a in labeled_articles if a.get('recent_label')),
            'rank_label True': sum(1 for a in labeled_articles if a.get('rank_label')),
            'reference_label True': sum(1 for a in labeled_articles if a.get('reference_label'))
        }
        
        # 3. Category Stats ì‹œíŠ¸
        category_stats = {}
        for article in labeled_articles:
            category = article.get('original_category', '')
            if category not in category_stats:
                category_stats[category] = {
                    'count': 0,
                    'negative_count': 0,
                    'relevance_true': 0,
                    'recent_true': 0,
                    'rank_true': 0,
                    'reference_true': 0
                }
            
            category_stats[category]['count'] += 1
            if article.get('sentiment') == 'negative':
                category_stats[category]['negative_count'] += 1
            if article.get('relevance_label'):
                category_stats[category]['relevance_true'] += 1
            if article.get('recent_label'):
                category_stats[category]['recent_true'] += 1
            if article.get('rank_label'):
                category_stats[category]['rank_true'] += 1
            if article.get('reference_label'):
                category_stats[category]['reference_true'] += 1
        
        # Excel íŒŒì¼ ìƒì„±
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            # Raw Data
            pd.DataFrame(raw_data).to_excel(writer, sheet_name='Raw Data', index=False)
            
            # Summary
            pd.DataFrame([summary_data]).to_excel(writer, sheet_name='Summary', index=False)
            
            # Category Stats
            category_df = pd.DataFrame.from_dict(category_stats, orient='index')
            category_df.to_excel(writer, sheet_name='Category Stats')
        
        logger.info(f"âœ… ë¼ë²¨ë§ëœ ê¸°ì‚¬ Excel ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_path}")
        
    except Exception as e:
        logger.error(f"âŒ ë¼ë²¨ë§ëœ ê¸°ì‚¬ Excel ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {str(e)}")


def export_category_scores_to_excel(category_scores: Dict[str, Dict[str, Any]], output_path: str = "category_scores_debug.xlsx"):
    """
    ì¹´í…Œê³ ë¦¬ ì ìˆ˜ë“¤ì„ Excelë¡œ ë‚´ë³´ë‚´ê¸° (ë””ë²„ê¹…ìš©)
    
    Args:
        category_scores: ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ë”•ì…”ë„ˆë¦¬
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    
    Note: ì´ í•¨ìˆ˜ëŠ” ë””ë²„ê¹… ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©ë˜ë©°, ë‚˜ì¤‘ì— ì‚­ì œ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """
    try:
        import pandas as pd
        
        logger.info(f"ğŸ“Š ì¹´í…Œê³ ë¦¬ ì ìˆ˜ Excel ë‚´ë³´ë‚´ê¸° ì‹œì‘: {len(category_scores)}ê°œ ì¹´í…Œê³ ë¦¬")
        
        # 1. Sorted Scores ì‹œíŠ¸ (ìµœì¢… ì ìˆ˜ ìˆœ)
        sorted_scores = []
        for category, scores in category_scores.items():
            sorted_scores.append({
                'category': category,
                'final_score': scores.get('final_score', 0.0),
                'frequency_score': scores.get('frequency_score', 0.0),
                'relevance_score': scores.get('relevance_score', 0.0),
                'recent_score': scores.get('recent_score', 0.0),
                'rank_score': scores.get('rank_score', 0.0),
                'reference_score': scores.get('reference_score', 0.0),
                'negative_score': scores.get('negative_score', 0.0),
                'total_articles': scores.get('total_articles', 0),
                'negative_articles': scores.get('negative_articles', 0)
            })
        
        # ìµœì¢… ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_scores.sort(key=lambda x: x['final_score'], reverse=True)
        
        # 2. Raw Scores ì‹œíŠ¸ (ì›ë³¸ ë°ì´í„°)
        raw_scores = []
        for category, scores in category_scores.items():
            raw_scores.append({
                'category': category,
                **scores
            })
        
        # 3. Score Distribution ì‹œíŠ¸
        score_ranges = {
            '0-1': 0, '1-2': 0, '2-3': 0, '3-4': 0, '4-5': 0,
            '5-6': 0, '6-7': 0, '7-8': 0, '8-9': 0, '9-10': 0
        }
        
        for category, scores in category_scores.items():
            final_score = scores.get('final_score', 0.0)
            if final_score < 1:
                score_ranges['0-1'] += 1
            elif final_score < 2:
                score_ranges['1-2'] += 1
            elif final_score < 3:
                score_ranges['2-3'] += 1
            elif final_score < 4:
                score_ranges['3-4'] += 1
            elif final_score < 5:
                score_ranges['4-5'] += 1
            elif final_score < 6:
                score_ranges['5-6'] += 1
            elif final_score < 7:
                score_ranges['6-7'] += 1
            elif final_score < 8:
                score_ranges['7-8'] += 1
            elif final_score < 9:
                score_ranges['8-9'] += 1
            else:
                score_ranges['9-10'] += 1
        
        # 4. Top/Bottom Categories ì‹œíŠ¸
        top_categories = sorted_scores[:10]  # ìƒìœ„ 10ê°œ
        bottom_categories = sorted_scores[-10:]  # í•˜ìœ„ 10ê°œ
        
        # Excel íŒŒì¼ ìƒì„±
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            # Sorted Scores
            pd.DataFrame(sorted_scores).to_excel(writer, sheet_name='Sorted Scores', index=False)
            
            # Raw Scores
            pd.DataFrame(raw_scores).to_excel(writer, sheet_name='Raw Scores', index=False)
            
            # Score Distribution
            distribution_df = pd.DataFrame([score_ranges])
            distribution_df.to_excel(writer, sheet_name='Score Distribution', index=False)
            
            # Top Categories
            pd.DataFrame(top_categories).to_excel(writer, sheet_name='Top Categories', index=False)
            
            # Bottom Categories
            pd.DataFrame(bottom_categories).to_excel(writer, sheet_name='Bottom Categories', index=False)
        
        logger.info(f"âœ… ì¹´í…Œê³ ë¦¬ ì ìˆ˜ Excel ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_path}")
        
    except Exception as e:
        logger.error(f"âŒ ì¹´í…Œê³ ë¦¬ ì ìˆ˜ Excel ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {str(e)}")


# ============================================================================
# ğŸš§ ë””ë²„ê¹…ìš© Excel ë‚´ë³´ë‚´ê¸° í•¨ìˆ˜ë“¤ ë
# ============================================================================

def export_combined_analysis_to_excel(
    labeled_articles: List[Dict[str, Any]], 
    category_scores: Dict[str, Dict[str, Any]], 
    output_path: str = "combined_analysis_debug.xlsx"
):
    """
    ë¼ë²¨ë§ê³¼ ì¹´í…Œê³ ë¦¬ ì ìˆ˜ë¥¼ í•˜ë‚˜ì˜ ì‹œíŠ¸ì— ì—´ì„ ê¸¸ê²Œ ì´ì–´ë¶™ì—¬ì„œ í•˜ë‚˜ì˜ Excelë¡œ ë‚´ë³´ë‚´ê¸° (ë””ë²„ê¹…ìš©)
    
    Args:
        labeled_articles: ë¼ë²¨ë§ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        category_scores: ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ë”•ì…”ë„ˆë¦¬
        output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    
    Note: ì´ í•¨ìˆ˜ëŠ” ë””ë²„ê¹… ëª©ì ìœ¼ë¡œë§Œ ì‚¬ìš©ë˜ë©°, ë‚˜ì¤‘ì— ì‚­ì œ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """
    try:
        import pandas as pd
        
        logger.info(f"ğŸ“Š í†µí•© ë¶„ì„ Excel ë‚´ë³´ë‚´ê¸° ì‹œì‘")
        logger.info(f"   - ë¼ë²¨ë§ëœ ê¸°ì‚¬: {len(labeled_articles)}ê°œ")
        logger.info(f"   - ì¹´í…Œê³ ë¦¬ ì ìˆ˜: {len(category_scores)}ê°œ")
        
        # 1. ë¼ë²¨ë§ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        labeled_df = pd.DataFrame(labeled_articles)
        
        # 2. ì¹´í…Œê³ ë¦¬ ì ìˆ˜ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        scores_data = []
        for category, scores in category_scores.items():
            scores_data.append({
                'category': category,
                **scores
            })
        scores_df = pd.DataFrame(scores_data)
        
        # 3. ë‘ DataFrameì„ ì—´ ë°©í–¥ìœ¼ë¡œ ì—°ê²° (ê°€ë¡œë¡œ ì´ì–´ë¶™ì´ê¸°)
        # ë¼ë²¨ë§ ë°ì´í„°ê°€ ë” ê¸¸ë©´ scores_dfë¥¼ ë°˜ë³µí•˜ì—¬ ë§ì¶¤
        if len(labeled_df) > len(scores_df):
            # scores_dfë¥¼ labeled_df ê¸¸ì´ë§Œí¼ ë°˜ë³µ
            repeated_scores = []
            for i in range(len(labeled_df)):
                category_idx = i % len(scores_df)
                repeated_scores.append(scores_data[category_idx])
            scores_df_extended = pd.DataFrame(repeated_scores)
            
            # ì—´ ë°©í–¥ìœ¼ë¡œ ì—°ê²°
            combined_df = pd.concat([labeled_df, scores_df_extended], axis=1)
            
        else:
            # labeled_dfë¥¼ scores_df ê¸¸ì´ë§Œí¼ ë°˜ë³µ
            repeated_labeled = []
            for i in range(len(scores_df)):
                article_idx = i % len(labeled_df) if len(labeled_df) > 0 else 0
                if len(labeled_df) > 0:
                    repeated_labeled.append(labeled_articles[article_idx])
                else:
                    # labeled_articlesê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì±„ì›€
                    repeated_labeled.append({})
            labeled_df_extended = pd.DataFrame(repeated_labeled)
            
            # ì—´ ë°©í–¥ìœ¼ë¡œ ì—°ê²°
            combined_df = pd.concat([labeled_df_extended, scores_df], axis=1)
        
        # 4. ì—´ ì´ë¦„ ì •ë¦¬ ë° ìˆœì„œ ì¡°ì •
        # ë¼ë²¨ë§ ê´€ë ¨ ì—´ë“¤
        labeled_columns = [
            'company', 'issue', 'original_category', 'title', 'description', 
            'pubDate', 'sentiment', 'sentiment_confidence', 'neg_keywords', 
            'pos_keywords', 'sentiment_basis', 'relevance_label', 'recent_value', 
            'rank_label', 'reference_label', 'label_reasons'
        ]
        
        # ì ìˆ˜ ê´€ë ¨ ì—´ë“¤
        score_columns = [
            'category', 'count', 'frequency_score', 'relevance_score', 
            'recent_score', 'rank_score', 'reference_score', 'negative_score', 
            'final_score'
        ]
        
        # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì—´ë“¤ë§Œ í•„í„°ë§
        existing_labeled_cols = [col for col in labeled_columns if col in combined_df.columns]
        existing_score_cols = [col for col in score_columns if col in combined_df.columns]
        
        # ì—´ ìˆœì„œ ì¬ì •ë ¬
        final_columns = existing_labeled_cols + existing_score_cols
        combined_df = combined_df[final_columns]
        
        # 5. Excel íŒŒì¼ ìƒì„±
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            # í†µí•© ë°ì´í„° ì‹œíŠ¸
            combined_df.to_excel(writer, sheet_name='Combined Analysis', index=False)
            
            # ìš”ì•½ í†µê³„ ì‹œíŠ¸
            summary_data = {
                'ë¶„ì„ í•­ëª©': [
                    'ì´ ê¸°ì‚¬ ìˆ˜',
                    'ì´ ì¹´í…Œê³ ë¦¬ ìˆ˜',
                    'ë¶€ì •ì  ê¸°ì‚¬ ìˆ˜',
                    'ê¸ì •ì  ê¸°ì‚¬ ìˆ˜',
                    'ì¤‘ë¦½ì  ê¸°ì‚¬ ìˆ˜',
                    'relevance_label True',
                    'recent_label True',
                    'rank_label True',
                    'reference_label True',
                    'ìµœê³  ì ìˆ˜ ì¹´í…Œê³ ë¦¬',
                    'ìµœì € ì ìˆ˜ ì¹´í…Œê³ ë¦¬',
                    'í‰ê·  ìµœì¢… ì ìˆ˜'
                ],
                'ê°’': [
                    len(labeled_articles),
                    len(category_scores),
                    sum(1 for a in labeled_articles if a.get('sentiment') == 'negative'),
                    sum(1 for a in labeled_articles if a.get('sentiment') == 'positive'),
                    sum(1 for a in labeled_articles if a.get('sentiment') == 'neutral'),
                    sum(1 for a in labeled_articles if a.get('relevance_label')),
                    sum(1 for a in labeled_articles if a.get('recent_value', 0) > 0),
                    sum(1 for a in labeled_articles if a.get('rank_label')),
                    sum(1 for a in labeled_articles if a.get('reference_label')),
                    max(category_scores.items(), key=lambda x: x[1].get('final_score', 0))[0] if category_scores else 'N/A',
                    min(category_scores.items(), key=lambda x: x[1].get('final_score', 0))[0] if category_scores else 'N/A',
                    round(sum(s.get('final_score', 0) for s in category_scores.values()) / len(category_scores), 3) if category_scores else 0.0
                ]
            }
            
            summary_df = pd.DataFrame(summary_data)
            summary_df.to_excel(writer, sheet_name='Summary', index=False)
            
            # ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ìˆœìœ„ ì‹œíŠ¸
            if category_scores:
                sorted_scores = sorted(
                    [(cat, scores) for cat, scores in category_scores.items()],
                    key=lambda x: x[1].get('final_score', 0),
                    reverse=True
                )
                
                ranking_data = []
                for i, (category, scores) in enumerate(sorted_scores, 1):
                    ranking_data.append({
                        'ìˆœìœ„': i,
                        'ì¹´í…Œê³ ë¦¬': category,
                        'ìµœì¢…ì ìˆ˜': scores.get('final_score', 0.0),
                        'ë¹ˆë„ì ìˆ˜': scores.get('frequency_score', 0.0),
                        'ê´€ë ¨ì„±ì ìˆ˜': scores.get('relevance_score', 0.0),
                        'ìµœì‹ ì„±ì ìˆ˜': scores.get('recent_score', 0.0),
                        'ìˆœìœ„ì ìˆ˜': scores.get('rank_score', 0.0),
                        'ì°¸ì¡°ì ìˆ˜': scores.get('reference_score', 0.0),
                        'ë¶€ì •ì„±ì ìˆ˜': scores.get('negative_score', 0.0),
                        'ê¸°ì‚¬ìˆ˜': scores.get('count', 0)
                    })
                
                ranking_df = pd.DataFrame(ranking_data)
                ranking_df.to_excel(writer, sheet_name='Category Ranking', index=False)
        
        logger.info(f"âœ… í†µí•© ë¶„ì„ Excel ë‚´ë³´ë‚´ê¸° ì™„ë£Œ: {output_path}")
        logger.info(f"ğŸ“Š ì´ {len(combined_df)}í–‰, {len(combined_df.columns)}ì—´ì˜ ë°ì´í„° ìƒì„±")
        
    except Exception as e:
        logger.error(f"âŒ í†µí•© ë¶„ì„ Excel ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {str(e)}")
        import traceback
        logger.error(f"âŒ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")


# ============================================================================
# ğŸš§ í†µí•© ë¶„ì„ Excel ë‚´ë³´ë‚´ê¸° í•¨ìˆ˜ ë
# ============================================================================

def debug_labeling_results(labeled_articles: List[Dict[str, Any]], category_scores: Dict[str, Dict[str, Any]]):
    """
    ë¼ë²¨ë§ ê²°ê³¼ë¥¼ ë””ë²„ê¹…í•˜ê¸° ìœ„í•œ ê°„ë‹¨í•œ ë¶„ì„ í•¨ìˆ˜
    """
    try:
        logger.info("ğŸ” ë¼ë²¨ë§ ê²°ê³¼ ë””ë²„ê¹… ì‹œì‘")
        
        # 1. ë¼ë²¨ë§ í†µê³„
        total_articles = len(labeled_articles)
        logger.info(f"ğŸ“Š ì´ ê¸°ì‚¬ ìˆ˜: {total_articles}")
        
        if total_articles == 0:
            logger.warning("âš ï¸ ë¼ë²¨ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 2. ìµœì‹ ì„± ì ìˆ˜ ë¶„ì„
        recent_values = [a.get('recent_value', 0.0) for a in labeled_articles]
        recent_1_0 = sum(1 for v in recent_values if v == 1.0)
        recent_0_5 = sum(1 for v in recent_values if v == 0.5)
        recent_0_0 = sum(1 for v in recent_values if v == 0.0)
        
        logger.info(f"ğŸ” ìµœì‹ ì„± ì ìˆ˜ ë¶„ì„:")
        logger.info(f"   - 1.0 (3ê°œì›” ì´ë‚´): {recent_1_0}ê°œ ({recent_1_0/total_articles*100:.1f}%)")
        logger.info(f"   - 0.5 (3~6ê°œì›”): {recent_0_5}ê°œ ({recent_0_5/total_articles*100:.1f}%)")
        logger.info(f"   - 0.0 (6ê°œì›” ì´ìƒ): {recent_0_0}ê°œ ({recent_0_0/total_articles*100:.1f}%)")
        
        # 3. rank/reference ë¼ë²¨ ë¶„ì„
        rank_true = sum(1 for a in labeled_articles if a.get('rank_label'))
        reference_true = sum(1 for a in labeled_articles if a.get('reference_label'))
        
        logger.info(f"ğŸ” rank/reference ë¼ë²¨ ë¶„ì„:")
        logger.info(f"   - rank_label True: {rank_true}ê°œ ({rank_true/total_articles*100:.1f}%)")
        logger.info(f"   - reference_label True: {reference_true}ê°œ ({reference_true/total_articles*100:.1f}%)")
        
        # 4. ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ë¶„ì„
        if category_scores:
            logger.info(f"ğŸ” ì¹´í…Œê³ ë¦¬ ì ìˆ˜ ë¶„ì„:")
            for category, scores in list(category_scores.items())[:5]:  # ìƒìœ„ 5ê°œë§Œ
                logger.info(f"   - '{category}':")
                logger.info(f"     recent_score: {scores.get('recent_score', 0.0):.3f}")
                logger.info(f"     rank_score: {scores.get('rank_score', 0.0):.3f}")
                logger.info(f"     reference_score: {scores.get('reference_score', 0.0):.3f}")
        
        # 5. ìƒ˜í”Œ ê¸°ì‚¬ ë¶„ì„
        logger.info(f"ğŸ” ìƒ˜í”Œ ê¸°ì‚¬ ë¶„ì„ (ìƒìœ„ 3ê°œ):")
        for i, article in enumerate(labeled_articles[:3]):
            logger.info(f"   {i+1}. '{article.get('title', '')[:50]}...'")
            logger.info(f"      - recent_value: {article.get('recent_value', 0.0)}")
            logger.info(f"      - rank_label: {article.get('rank_label', False)}")
            logger.info(f"      - reference_label: {article.get('reference_label', False)}")
            logger.info(f"      - original_category: {article.get('original_category', 'N/A')}")
            logger.info(f"      - pubDate: {article.get('pubDate', 'N/A')}")
        
        # 6. ESG ë¶„ë¥˜ ì¡°íšŒ ê²°ê³¼ ë¶„ì„
        logger.info(f"ğŸ” ESG ë¶„ë¥˜ ì¡°íšŒ ê²°ê³¼ ë¶„ì„:")
        esg_results = {}
        for article in labeled_articles:
            category = article.get('original_category', '')
            if category:
                esg_results[category] = esg_results.get(category, 0) + 1
        
        logger.info(f"   - ì´ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(esg_results)}")
        logger.info(f"   - ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ìˆ˜: {esg_results}")
        
        logger.info("âœ… ë¼ë²¨ë§ ê²°ê³¼ ë””ë²„ê¹… ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ ë¼ë²¨ë§ ê²°ê³¼ ë””ë²„ê¹… ì¤‘ ì˜¤ë¥˜: {str(e)}")


# ============================================================================
# ğŸš§ ë””ë²„ê¹… í•¨ìˆ˜ ë
# ============================================================================
