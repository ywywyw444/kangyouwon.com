"""
Middleissue Service - ì¤‘ëŒ€ì„± í‰ê°€ ê´€ë ¨ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì²˜ë¦¬
í¬ë¡¤ë§ ë°ì´í„° ì²˜ë¦¬, ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì ìš©, ì ìˆ˜ ê³„ì‚° ë“±ì„ ë‹´ë‹¹
"""
# 1. í¬ë¡¤ë§í•œ ì „ì²´ ë°ì´í„° -> ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë¡œ ê¸ë¶€ì •í‰ê°€
# 2. relevance, recent, negative, rank, ê¸°ì¤€ì„œ/í‰ê°€ê¸°ê´€ ì§€í‘œ íŒë‹¨
# 3. ê° ì§€í‘œë³„ score ë¶€ì—¬
# 4. final score ê³„ì‚°
# 5. frontendë¡œ ë³´ë‚´ê³  ë©”ëª¨ë¦¬ ì €ì¥

import logging
import json
import os
import joblib
from datetime import datetime, timedelta
from typing import Dict, Any, List, Set, Tuple
from app.domain.middleissue.schema import MiddleIssueRequest, MiddleIssueResponse, Article
from app.domain.middleissue.repository import MiddleIssueRepository
import re
import numpy as np
from dateutil import parser

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

NEGATIVE_LEXICON = {
    "ê°ì†Œ","í•˜ë½","ë¶€ì§„","ì•…í™”","ì˜¤ì—¼","ìœ„ë°˜","ë‹´í•©","ë¶€íŒ¨","ë‡Œë¬¼","íš¡ë ¹","ë°°ì„","ì‚¬ê¸°",
    "ê³¼ì§•ê¸ˆ","ë²Œê¸ˆ","ì‚¬ê³ ","ì‚¬ë§","íŒŒì—…","ë¶„ìŸ","ê°ˆë“±","ë…¼ë€","ì†Œì†¡","ë¦¬ì½œ","ê²°í•¨","ë¶ˆëŸ‰",
    "ëˆ„ì¶œ","ìœ ì¶œ","í™”ì¬","ì ì","íŒŒì‚°","êµ¬ì¡°ì¡°ì •","ì •ë¦¬í•´ê³ ","ì¤‘ë‹¨","ì°¨ì§ˆ","ì‹¤íŒ¨","ë¶ˆë²•",
    "ì² ìˆ˜","í‡´ì¶œ","ë¶€ì •","ë¶ˆê³µì •","ê°‘ì§ˆ","ì§ì¥ê´´ë¡­í˜","í­ì–¸","íš¡í¬","í™˜ë¶ˆ","íšŒìˆ˜","ì†ì‹¤",
    "ê²½ê³ ","ì œì¬","í•´ì§€","ì·¨ì†Œ","ë‚™ì œ","ë¶€ê³¼","ì§•ê³„","ì¤‘ì§•ê³„","ë¶€ì •ì²­íƒ","ê²½ì˜ê¶Œë¶„ìŸ","ìœ„ê¸°","ì²­ì‚°"
}

POSITIVE_LEXICON = {
    "ì„±ì¥","í™•ëŒ€","ì¦ê°€","ê°œì„ ","í˜¸ì¡°","í‘ì","ìµœê³ ","ì„ ì •","ìˆ˜ìƒ","í¬ìƒ","ì‚°ì—…í¬ì¥",
    "ê°•í™”","ìƒìƒ","í˜‘ë ¥","ë„ì…","ì¶œì‹œ","ì„ ë„","ì¸ì¦","í™•ë³´","ìš°ìˆ˜","ë„ì•½","í™•ì¥","íšŒë³µ",
    "ê³ ë„í™”","ìµœì í™”","ì•ˆì •í™”","ì‹ ì„¤","ì±„ìš©","ì¦ì„¤","ì¦ì‚°","í™•ì¶©","ê³µê¸‰","ìˆ˜ì£¼","ëª¨ë²”",
    "ë‹¬ì„±","ì‹ ê¸°ìˆ ","ê°œì‹œ","ì¦ë¹™","ì„±ê³¼","ë§¤ì¶œì¦ê°€","ê³ ì„±ì¥","ì„ ë„ê¸°ì—…","ìˆ˜ì¶œí™•ëŒ€",
    "í•´ì™¸ì§„ì¶œ","íŒŒíŠ¸ë„ˆì‹­","ë¦¬ë”","í‰íŒ","ì¬ìƒì—ë„ˆì§€","ê°ì¶•","ì´í–‰","í˜ì‹ ","ê°œë°œ","ì—­ëŒ€",
    "ìˆœí•­","ê»‘ì¶©","ê¸°ì¦","ê¸°ë¶€","ì „ë‹¬","ì§€ì›","ìº í˜ì¸","í›„ì›"
}

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
MODEL_PATH = os.path.join(
    os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
    'models',
    'model_multinomialnb.joblib'
)

# ì •ê·œì‹ íŒ¨í„´ ì»´íŒŒì¼
_NEG_RE = re.compile("|".join(map(re.escape, sorted(NEGATIVE_LEXICON, key=len, reverse=True))))
_POS_RE = re.compile("|".join(map(re.escape, sorted(POSITIVE_LEXICON, key=len, reverse=True))))

def parse_pubdate(date_str: str) -> datetime:
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ì„ datetimeìœ¼ë¡œ íŒŒì‹±"""
    try:
        # 1. ISO í˜•ì‹ ì‹œë„
        try:
            return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
        except ValueError:
            pass

        # 2. RSS í˜•ì‹ ì‹œë„ (ì˜ˆ: 'Wed, 06 Aug 2023 12:30:00 +0900')
        try:
            from email.utils import parsedate_to_datetime
            return parsedate_to_datetime(date_str)
        except Exception:
            pass

        # 3. ê¸°íƒ€ ì¼ë°˜ì ì¸ í˜•ì‹ë“¤ ì‹œë„
        return parser.parse(date_str)

    except Exception as e:
        logger.warning(f"âš ï¸ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ({date_str}): {str(e)}")
        return datetime.now()  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜

def extract_keywords(text: str, patt: re.Pattern) -> List[str]:
    """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    if not isinstance(text, str):
        return []
    return sorted(set(patt.findall(text)))

def load_sentiment_model():
    """ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ"""
    try:
        logger.info(f"ğŸ¤– ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹œë„: {MODEL_PATH}")
        model = joblib.load(MODEL_PATH)
        logger.info("âœ… ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        return model
    except Exception as e:
        logger.error(f"âŒ ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None

def analyze_sentiment(model, articles: List[Article]) -> List[Dict[str, Any]]:
    """ê¸°ì‚¬ ê°ì„± ë¶„ì„ ìˆ˜í–‰"""
    try:
        analyzed_articles = []
        
        for article in articles:
            try:
                # 1. í…ìŠ¤íŠ¸ ì¤€ë¹„
                title_text = article.title
                desc_text = article.description
                full_text = f"{title_text} {desc_text}"
                
                # 2. í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„
                neg_keywords = extract_keywords(full_text, _NEG_RE)
                pos_keywords = extract_keywords(full_text, _POS_RE)
                has_both = len(neg_keywords) > 0 and len(pos_keywords) > 0
                
                # 3. ëª¨ë¸ ê¸°ë°˜ ë¶„ì„
                if model is not None:
                    try:
                        # ì˜ˆì¸¡ ë° í™•ë¥  ê³„ì‚°
                        y_pred = model.predict([full_text])[0]
                        probas = model.predict_proba([full_text])[0]
                        
                        # negative í´ë˜ìŠ¤ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
                        classes = getattr(model.named_steps["clf"], "classes_", None)
                        if classes is None:
                            classes = getattr(model, "classes_", None)
                        
                        if classes is not None and "negative" in classes:
                            neg_idx = int(np.where(classes == "negative")[0][0])
                            neg_proba = probas[neg_idx]
                        else:
                            neg_proba = 0.0
                            
                        # ë¶€ì •+ê¸ì • ë™ì‹œ ì¶œí˜„ ì‹œ otherë¡œ ë³€ê²½
                        if y_pred == "negative" and has_both:
                            final_sentiment = "other"
                            final_basis = "ë¶€ì •+ê¸ì • ë™ì‹œ ì¶œí˜„ â†’ other"
                        else:
                            final_sentiment = y_pred
                            final_basis = "ëª¨ë¸ ì˜ˆì¸¡ ìœ ì§€"
                            
                    except Exception as e:
                        logger.error(f"âŒ ëª¨ë¸ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                        # ëª¨ë¸ ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œë§Œ íŒë‹¨
                        final_sentiment = "negative" if len(neg_keywords) > len(pos_keywords) else "other"
                        final_basis = "í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨ (ëª¨ë¸ ì‹¤íŒ¨)"
                        neg_proba = 1.0 if final_sentiment == "negative" else 0.0
                else:
                    # ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš° í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œë§Œ íŒë‹¨
                    final_sentiment = "negative" if len(neg_keywords) > len(pos_keywords) else "other"
                    final_basis = "í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨ (ëª¨ë¸ ì—†ìŒ)"
                    neg_proba = 1.0 if final_sentiment == "negative" else 0.0
                
                analyzed_articles.append({
                    "title": title_text,
                    "description": desc_text,
                    "sentiment": final_sentiment,
                    "sentiment_confidence": neg_proba if final_sentiment == "negative" else (1 - neg_proba),
                    "neg_keywords": ", ".join(neg_keywords),
                    "pos_keywords": ", ".join(pos_keywords),
                    "sentiment_basis": final_basis,
                    "original_category": article.original_category,
                    "issue": article.issue,
                    "pubDate": article.pubDate,
                    "originallink": article.originallink,
                    "company": article.company,
                    "relevance_score": 0.0  # ê´€ë ¨ì„± ì ìˆ˜ ì´ˆê¸°í™”
                })
                
            except Exception as e:
                logger.error(f"âŒ ê¸°ì‚¬ ê°ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                logger.error(f"ë¬¸ì œëœ ê¸°ì‚¬ ì œëª©: {article.title}")
                continue

        return analyzed_articles
    except Exception as e:
        logger.error(f"âŒ ê°ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []

async def add_relevance_labels(
    articles: List[Dict[str, Any]], 
    company_id: str,
    search_date: datetime,
    year_categories: Set[str],
    common_categories: Set[str]
) -> List[Dict[str, Any]]:
    """
    ê¸°ì‚¬ì— ê´€ë ¨ì„± ë¼ë²¨ ì¶”ê°€
    
    ì ìˆ˜ ì²´ê³„:
    - ++ : 1.0ì 
    - +  : 0.5ì 
    - ì—†ìŒ: 0ì 
    """
    try:
        for article in articles:
            score = 0.0
            reasons = []

            # 1. ì œëª©ì— ê¸°ì—…ëª… í¬í•¨ ì—¬ë¶€ (++)
            if company_id in article["title"]:
                score += 1.0
                reasons.append("ì œëª©ì— ê¸°ì—…ëª… í¬í•¨ (1.0)")

            # 2. ë°œí–‰ì¼ ê¸°ì¤€ ìµœì‹ ì„± (3ê°œì›” ì´ë‚´: ++, 3-6ê°œì›”: +)
            if article["pubDate"]:
                try:
                    pub_date = parse_pubdate(article["pubDate"])
                    months_diff = (search_date - pub_date).days / 30
                    
                    if months_diff <= 3:
                        score += 1.0
                        reasons.append("ìµœê·¼ 3ê°œì›” ì´ë‚´ (1.0)")
                    elif months_diff <= 6:
                        score += 0.5
                        reasons.append("ìµœê·¼ 3-6ê°œì›” (0.5)")
                except Exception as e:
                    logger.warning(f"âš ï¸ ë°œí–‰ì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

            # 3 & 4. ì¹´í…Œê³ ë¦¬ ë§¤ì¹­
            if article["original_category"]:
                # í•´ë‹¹ ì—°ë„ ì¹´í…Œê³ ë¦¬ì™€ ë§¤ì¹­ (++)
                if article["original_category"] in year_categories:
                    score += 1.0
                    reasons.append("ì—°ë„ë³„ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ (1.0)")
                
                # ê³µí†µ ì¹´í…Œê³ ë¦¬ì™€ ë§¤ì¹­ (++)
                if article["original_category"] in common_categories:
                    score += 1.0
                    reasons.append("ê³µí†µ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ (1.0)")

            # ê²°ê³¼ ì €ì¥
            article["relevance_score"] = score
            article["relevance_reasons"] = reasons

        return articles
    except Exception as e:
        logger.error(f"âŒ ê´€ë ¨ì„± ë¼ë²¨ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return articles

async def start_assessment(request: MiddleIssueRequest) -> Dict[str, Any]:
    """
    ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘ - í¬ë¡¤ë§ ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ ì‹œì‘
    
    Args:
        request: ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘ ìš”ì²­ ë°ì´í„° (MiddleIssueRequest)
        
    Returns:
        Dict[str, Any]: ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘ ì‘ë‹µ
    """
    try:
        # 1. ìš”ì²­ ë°ì´í„° ë¡œê¹…
        logger.info("="*50)
        logger.info("ğŸš€ ìƒˆë¡œìš´ ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘")
        logger.info(f"ê¸°ì—…ëª…: {request.company_id}")
        logger.info(f"ë³´ê³ ê¸°ê°„: {request.report_period}")
        logger.info(f"ìš”ì²­ íƒ€ì…: {request.request_type}")
        logger.info(f"íƒ€ì„ìŠ¤íƒ¬í”„: {request.timestamp}")
        logger.info(f"ì´ í¬ë¡¤ë§ ê¸°ì‚¬ ìˆ˜: {request.total_results}")
        
        # í¬ë¡¤ë§ ë°ì´í„° êµ¬ì¡° í™•ì¸
        logger.info("-"*50)
        logger.info("ğŸ“‹ í¬ë¡¤ë§ ë°ì´í„° êµ¬ì¡° í™•ì¸")
        
        if request.articles and len(request.articles) > 0:
            sample_article = request.articles[0]
            logger.info("ìˆ˜ì§‘ëœ ë°ì´í„° í•„ë“œ:")
            
            # í•„ìˆ˜ í•„ë“œ
            logger.info("í•„ìˆ˜ í•„ë“œ:")
            logger.info(f"- title âœ“")
            logger.info(f"- description âœ“")
            logger.info(f"- company âœ“")
            
            # ì„ íƒì  í•„ë“œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            logger.info("\nì„ íƒì  í•„ë“œ:")
            logger.info(f"- originallink {'âœ“' if sample_article.originallink else 'âœ—'}")
            logger.info(f"- pubDate {'âœ“' if sample_article.pubDate else 'âœ—'}")
            logger.info(f"- issue {'âœ“' if sample_article.issue else 'âœ—'}")
            logger.info(f"- original_category {'âœ“' if sample_article.original_category else 'âœ—'}")
            logger.info(f"- query_kind {'âœ“' if sample_article.query_kind else 'âœ—'}")
            logger.info(f"- keyword {'âœ“' if sample_article.keyword else 'âœ—'}")
            
            # ì˜ˆì‹œ ë°ì´í„° êµ¬ì¡°
            logger.info("\në°ì´í„° êµ¬ì¡° ì˜ˆì‹œ:")
            logger.info(f"- title: [ì œëª© í…ìŠ¤íŠ¸...]")
            logger.info(f"- description: [ë³¸ë¬¸ í…ìŠ¤íŠ¸...]")
            logger.info(f"- company: {sample_article.company}")
            if sample_article.original_category:
                logger.info(f"- original_category: {sample_article.original_category}")
            if sample_article.query_kind:
                logger.info(f"- query_kind: {sample_article.query_kind}")
            if sample_article.keyword:
                logger.info(f"- keyword: {sample_article.keyword}")
        else:
            logger.warning("âš ï¸ í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤!")
            
        logger.info("-"*50)

        # 2. ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ
        model = load_sentiment_model()
        if not model:
            raise Exception("ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")

        # 3. í¬ë¡¤ë§ ë°ì´í„° ê°ì„± ë¶„ì„
        logger.info("ğŸ“Š í¬ë¡¤ë§ ë°ì´í„° ê°ì„± ë¶„ì„ ì‹œì‘")
        analyzed_articles = analyze_sentiment(model, request.articles)
        
        # 4. ì¹´í…Œê³ ë¦¬ ë°ì´í„° ì¡°íšŒ
        repository = MiddleIssueRepository()
        search_year = int(request.report_period["end_date"][:4])  # ê²€ìƒ‰ ì—°ë„
        corporation_issues = await repository.get_corporation_issues(
            corporation_name=request.company_id,
            year=search_year
        )

        # ì¹´í…Œê³ ë¦¬ ì„¸íŠ¸ ìƒì„±
        year_categories = {str(issue.category_id) for issue in corporation_issues.year_issues}
        common_categories = {str(issue.category_id) for issue in corporation_issues.common_issues}

        # 5. ê´€ë ¨ì„± ë¼ë²¨ ì¶”ê°€
        logger.info("ğŸ·ï¸ ê´€ë ¨ì„± ë¼ë²¨ ì¶”ê°€ ì‹œì‘")
        search_date = datetime.now()
        labeled_articles = await add_relevance_labels(
            analyzed_articles,
            request.company_id,
            search_date,
            year_categories,
            common_categories
        )

        # 6. ë¶„ì„ ê²°ê³¼ ë¡œê¹…
        negative_count = sum(1 for article in labeled_articles if article["sentiment"] == "negative")
        high_relevance_count = sum(1 for article in labeled_articles if article["relevance_score"] >= 2.0)  # 4ì  ë§Œì ì˜ ì ˆë°˜ ì´ìƒ
        
        logger.info(f"ë¶„ì„ëœ ê¸°ì‚¬ ìˆ˜: {len(labeled_articles)}")
        logger.info(f"ë¶€ì •ì  ê¸°ì‚¬ ìˆ˜: {negative_count}")
        logger.info(f"ë†’ì€ ê´€ë ¨ì„±(2.0ì  ì´ìƒ) ê¸°ì‚¬ ìˆ˜: {high_relevance_count}")
        
        # 7. ìƒ˜í”Œ ê²°ê³¼ ë¡œê¹… (ìµœëŒ€ 5ê°œ)
        logger.info("\nğŸ“° ë¶„ì„ ê²°ê³¼ ìƒ˜í”Œ:")
        for idx, article in enumerate(labeled_articles[:5]):
            logger.info(f"\nê¸°ì‚¬ {idx + 1}:")
            logger.info(f"ì œëª©: {article['title']}")
            logger.info(f"ê°ì„±: {article['sentiment']} (ì‹ ë¢°ë„: {article['sentiment_confidence']:.2f})")
            logger.info(f"ê´€ë ¨ì„± ì ìˆ˜: {article['relevance_score']:.1f}/4.0")
            logger.info(f"ê´€ë ¨ì„± ì´ìœ : {', '.join(article['relevance_reasons'])}")
            logger.info(f"ì¹´í…Œê³ ë¦¬: {article['original_category']}")
            logger.info(f"ë°œí–‰ì¼: {article['pubDate']}")
            logger.info("-"*30)

        if len(labeled_articles) > 5:
            logger.info(f"... ì™¸ {len(labeled_articles) - 5}ê°œ ê¸°ì‚¬")
        
        # 8. ì‘ë‹µ ë°ì´í„° ìƒì„±
        response_data = {
            "success": True,
            "message": "ì¤‘ëŒ€ì„± í‰ê°€ ë°ì´í„° ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "data": {
                "company_id": request.company_id,
                "report_period": request.report_period,
                "assessment_status": "analyzed",
                "total_articles": len(labeled_articles),
                "negative_articles": negative_count,
                "high_relevance_articles": high_relevance_count,
                "negative_ratio": (negative_count/len(labeled_articles))*100 if labeled_articles else 0,
                "analyzed_samples": labeled_articles[:5]  # ìƒ˜í”Œ ë°ì´í„°ë§Œ í¬í•¨
            }
        }
        
        logger.info("âœ… ë°ì´í„° ë¶„ì„ ì™„ë£Œ")
        logger.info("="*50)
        
        return response_data
        
    except Exception as e:
        error_msg = f"âŒ ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(error_msg)
        logger.error("="*50)
        
        return {
            "success": False,
            "message": error_msg,
            "data": None
        }