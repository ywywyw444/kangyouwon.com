"""
Middleissue Service - ì¤‘ëŒ€ì„± í‰ê°€ ê´€ë ¨ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì²˜ë¦¬
í¬ë¡¤ë§ ë°ì´í„° ì²˜ë¦¬, ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì ìš©, ì ìˆ˜ ê³„ì‚° ë“±ì„ ë‹´ë‹¹
"""
# 1. í¬ë¡¤ë§í•œ ì „ì²´ ë°ì´í„° -> ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë¡œ ê¸ë¶€ì •í‰ê°€
# 2. relevance, recent, negative, rank, ê¸°ì¤€ì„œ/í‰ê°€ê¸°ê´€ ì§€í‘œ íŒë‹¨
# 3. ê° ì§€í‘œë³„ score ë¶€ì—¬
# 4. final score ê³„ì‚°
# 5. frontendë¡œ ë³´ë‚´ê³  ë©”ëª¨ë¦¬ ì €ì¥

import logging
import json
import os
import joblib
from datetime import datetime, timedelta
from typing import Dict, Any, List, Set, Tuple
from app.domain.middleissue.schema import MiddleIssueRequest, MiddleIssueResponse, Article
from app.domain.middleissue.repository import MiddleIssueRepository

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# ëª¨ë¸ ê²½ë¡œ ì„¤ì •
MODEL_PATH = os.path.join(
    os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
    'models',
    'model_multinomialnb.joblib'
)

def load_sentiment_model():
    """ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ"""
    try:
        logger.info(f"ğŸ¤– ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹œë„: {MODEL_PATH}")
        
        # ëª¨ë¸ íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
        if not os.path.exists(MODEL_PATH):
            logger.warning(f"âš ï¸ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ 'other'ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.")
            return None
            
        model = joblib.load(MODEL_PATH)
        logger.info("âœ… ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        return model
    except Exception as e:
        logger.error(f"âŒ ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
        return None

def analyze_text_sentiment(model, text: str) -> Tuple[str, float]:
    """í…ìŠ¤íŠ¸ ê°ì„± ë¶„ì„ ìˆ˜í–‰"""
    try:
        # ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
        if model is None:
            return "other", 1.0
            
        prediction = model.predict([text])[0]
        # predict_probaë¡œ í™•ë¥ ê°’ë„ ê°€ì ¸ì˜´
        probabilities = model.predict_proba([text])[0]
        confidence = max(probabilities)  # ê°€ì¥ ë†’ì€ í™•ë¥ ê°’
        sentiment = "negative" if prediction == 1 else "other"
        return sentiment, confidence
    except Exception as e:
        logger.error(f"âŒ í…ìŠ¤íŠ¸ ê°ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return "other", 0.0

def analyze_sentiment(model, articles: List[Article]) -> List[Dict[str, Any]]:
    """ê¸°ì‚¬ ê°ì„± ë¶„ì„ ìˆ˜í–‰"""
    try:
        if not model:
            logger.error("âŒ ê°ì„± ë¶„ì„ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []

        analyzed_articles = []
        for article in articles:
            try:
                # ì œëª©ê³¼ ë³¸ë¬¸ì„ ê°ê° ë¶„ì„
                title_text = f"{article.title} {article.original_category or ''}"
                title_sentiment, title_confidence = analyze_text_sentiment(model, title_text)
                
                desc_sentiment, desc_confidence = analyze_text_sentiment(model, article.description)
                
                # ì œëª©ê³¼ ë³¸ë¬¸ì˜ ê°ì„±ì´ ë‹¤ë¥¸ ê²½ìš°, ë” ë†’ì€ confidenceë¥¼ ê°€ì§„ ìª½ì„ ì„ íƒ
                if title_sentiment != desc_sentiment:
                    if title_confidence > desc_confidence:
                        final_sentiment = title_sentiment
                        confidence = title_confidence
                    else:
                        final_sentiment = desc_sentiment
                        confidence = desc_confidence
                else:
                    # ê°™ì€ ê°ì„±ì´ë©´ í‰ê·  confidence ì‚¬ìš©
                    final_sentiment = title_sentiment
                    confidence = (title_confidence + desc_confidence) / 2
                
                analyzed_articles.append({
                    "title": article.title,
                    "description": article.description,
                    "sentiment": final_sentiment,
                    "sentiment_confidence": confidence,
                    "title_sentiment": title_sentiment,
                    "desc_sentiment": desc_sentiment,
                    "original_category": article.original_category,
                    "issue": article.issue,
                    "pubDate": article.pubDate,
                    "originallink": article.originallink,
                    "company": article.company,
                    "relevance_score": 0.0  # ê´€ë ¨ì„± ì ìˆ˜ ì´ˆê¸°í™”
                })
                
            except Exception as e:
                logger.error(f"âŒ ê¸°ì‚¬ ê°ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                logger.error(f"ë¬¸ì œëœ ê¸°ì‚¬ ì œëª©: {article.title}")
                continue

        return analyzed_articles
    except Exception as e:
        logger.error(f"âŒ ê°ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return []

async def add_relevance_labels(
    articles: List[Dict[str, Any]], 
    company_id: str,
    search_date: datetime,
    year_categories: Set[str],
    common_categories: Set[str]
) -> List[Dict[str, Any]]:
    """
    ê¸°ì‚¬ì— ê´€ë ¨ì„± ë¼ë²¨ ì¶”ê°€
    
    ì ìˆ˜ ì²´ê³„:
    - ++ : 1.0ì 
    - +  : 0.5ì 
    - ì—†ìŒ: 0ì 
    """
    try:
        for article in articles:
            score = 0.0  # floatìœ¼ë¡œ ë³€ê²½
            reasons = []

            # 1. ì œëª©ì— ê¸°ì—…ëª… í¬í•¨ ì—¬ë¶€ (++)
            if company_id in article["title"]:
                score += 1.0
                reasons.append("ì œëª©ì— ê¸°ì—…ëª… í¬í•¨ (1.0)")

            # 2. ë°œí–‰ì¼ ê¸°ì¤€ ìµœì‹ ì„± (3ê°œì›” ì´ë‚´: ++, 3-6ê°œì›”: +)
            if article["pubDate"]:
                pub_date = datetime.fromisoformat(article["pubDate"].replace('Z', '+00:00'))
                months_diff = (search_date - pub_date).days / 30
                
                if months_diff <= 3:
                    score += 1.0
                    reasons.append("ìµœê·¼ 3ê°œì›” ì´ë‚´ (1.0)")
                elif months_diff <= 6:
                    score += 0.5
                    reasons.append("ìµœê·¼ 3-6ê°œì›” (0.5)")

            # 3 & 4. ì¹´í…Œê³ ë¦¬ ë§¤ì¹­
            if article["original_category"]:
                # í•´ë‹¹ ì—°ë„ ì¹´í…Œê³ ë¦¬ì™€ ë§¤ì¹­ (++)
                if article["original_category"] in year_categories:
                    score += 1.0
                    reasons.append("ì—°ë„ë³„ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ (1.0)")
                
                # ê³µí†µ ì¹´í…Œê³ ë¦¬ì™€ ë§¤ì¹­ (++)
                if article["original_category"] in common_categories:
                    score += 1.0
                    reasons.append("ê³µí†µ ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ (1.0)")

            # ê²°ê³¼ ì €ì¥
            article["relevance_score"] = score
            article["relevance_reasons"] = reasons

        return articles
    except Exception as e:
        logger.error(f"âŒ ê´€ë ¨ì„± ë¼ë²¨ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return articles

async def start_assessment(request: MiddleIssueRequest) -> Dict[str, Any]:
    """
    ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘ - í¬ë¡¤ë§ ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ ì‹œì‘
    
    Args:
        request: ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘ ìš”ì²­ ë°ì´í„° (MiddleIssueRequest)
        
    Returns:
        Dict[str, Any]: ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘ ì‘ë‹µ
    """
    try:
        # 1. ìš”ì²­ ë°ì´í„° ë¡œê¹…
        logger.info("="*50)
        logger.info("ğŸš€ ìƒˆë¡œìš´ ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘")
        logger.info(f"ê¸°ì—…ëª…: {request.company_id}")
        logger.info(f"ë³´ê³ ê¸°ê°„: {request.report_period}")
        logger.info(f"ìš”ì²­ íƒ€ì…: {request.request_type}")
        logger.info(f"íƒ€ì„ìŠ¤íƒ¬í”„: {request.timestamp}")
        logger.info(f"ì´ í¬ë¡¤ë§ ê¸°ì‚¬ ìˆ˜: {request.total_results}")
        
        # í¬ë¡¤ë§ ë°ì´í„° êµ¬ì¡° í™•ì¸
        logger.info("-"*50)
        logger.info("ğŸ“‹ í¬ë¡¤ë§ ë°ì´í„° êµ¬ì¡° í™•ì¸")
        
        if request.articles and len(request.articles) > 0:
            sample_article = request.articles[0]
            logger.info("ìˆ˜ì§‘ëœ ë°ì´í„° í•„ë“œ:")
            
            # í•„ìˆ˜ í•„ë“œ
            logger.info("í•„ìˆ˜ í•„ë“œ:")
            logger.info(f"- title âœ“")
            logger.info(f"- description âœ“")
            logger.info(f"- company âœ“")
            
            # ì„ íƒì  í•„ë“œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            logger.info("\nì„ íƒì  í•„ë“œ:")
            logger.info(f"- originallink {'âœ“' if sample_article.originallink else 'âœ—'}")
            logger.info(f"- pubDate {'âœ“' if sample_article.pubDate else 'âœ—'}")
            logger.info(f"- issue {'âœ“' if sample_article.issue else 'âœ—'}")
            logger.info(f"- original_category {'âœ“' if sample_article.original_category else 'âœ—'}")
            logger.info(f"- query_kind {'âœ“' if sample_article.query_kind else 'âœ—'}")
            logger.info(f"- keyword {'âœ“' if sample_article.keyword else 'âœ—'}")
            
            # ì˜ˆì‹œ ë°ì´í„° êµ¬ì¡°
            logger.info("\në°ì´í„° êµ¬ì¡° ì˜ˆì‹œ:")
            logger.info(f"- title: [ì œëª© í…ìŠ¤íŠ¸...]")
            logger.info(f"- description: [ë³¸ë¬¸ í…ìŠ¤íŠ¸...]")
            logger.info(f"- company: {sample_article.company}")
            if sample_article.original_category:
                logger.info(f"- original_category: {sample_article.original_category}")
            if sample_article.query_kind:
                logger.info(f"- query_kind: {sample_article.query_kind}")
            if sample_article.keyword:
                logger.info(f"- keyword: {sample_article.keyword}")
        else:
            logger.warning("âš ï¸ í¬ë¡¤ë§ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤!")
            
        logger.info("-"*50)

        # 2. ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ
        model = load_sentiment_model()
        if not model:
            raise Exception("ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")

        # 3. í¬ë¡¤ë§ ë°ì´í„° ê°ì„± ë¶„ì„
        logger.info("ğŸ“Š í¬ë¡¤ë§ ë°ì´í„° ê°ì„± ë¶„ì„ ì‹œì‘")
        analyzed_articles = analyze_sentiment(model, request.articles)
        
        # 4. ì¹´í…Œê³ ë¦¬ ë°ì´í„° ì¡°íšŒ
        repository = MiddleIssueRepository()
        search_year = int(request.report_period["end_date"][:4])  # ê²€ìƒ‰ ì—°ë„
        corporation_issues = await repository.get_corporation_issues(
            corporation_name=request.company_id,
            year=search_year
        )

        # ì¹´í…Œê³ ë¦¬ ì„¸íŠ¸ ìƒì„±
        year_categories = {str(issue.category_id) for issue in corporation_issues.year_issues}
        common_categories = {str(issue.category_id) for issue in corporation_issues.common_issues}

        # 5. ê´€ë ¨ì„± ë¼ë²¨ ì¶”ê°€
        logger.info("ğŸ·ï¸ ê´€ë ¨ì„± ë¼ë²¨ ì¶”ê°€ ì‹œì‘")
        search_date = datetime.now()
        labeled_articles = await add_relevance_labels(
            analyzed_articles,
            request.company_id,
            search_date,
            year_categories,
            common_categories
        )

        # 6. ë¶„ì„ ê²°ê³¼ ë¡œê¹…
        negative_count = sum(1 for article in labeled_articles if article["sentiment"] == "negative")
        high_relevance_count = sum(1 for article in labeled_articles if article["relevance_score"] >= 2.0)  # 4ì  ë§Œì ì˜ ì ˆë°˜ ì´ìƒ
        
        logger.info(f"ë¶„ì„ëœ ê¸°ì‚¬ ìˆ˜: {len(labeled_articles)}")
        logger.info(f"ë¶€ì •ì  ê¸°ì‚¬ ìˆ˜: {negative_count}")
        logger.info(f"ë†’ì€ ê´€ë ¨ì„±(2.0ì  ì´ìƒ) ê¸°ì‚¬ ìˆ˜: {high_relevance_count}")
        
        # 7. ìƒ˜í”Œ ê²°ê³¼ ë¡œê¹… (ìµœëŒ€ 5ê°œ)
        logger.info("\nğŸ“° ë¶„ì„ ê²°ê³¼ ìƒ˜í”Œ:")
        for idx, article in enumerate(labeled_articles[:5]):
            logger.info(f"\nê¸°ì‚¬ {idx + 1}:")
            logger.info(f"ì œëª©: {article['title']}")
            logger.info(f"ê°ì„±: {article['sentiment']} (ì‹ ë¢°ë„: {article['sentiment_confidence']:.2f})")
            logger.info(f"ì œëª© ê°ì„±: {article['title_sentiment']}")
            logger.info(f"ë³¸ë¬¸ ê°ì„±: {article['desc_sentiment']}")
            logger.info(f"ê´€ë ¨ì„± ì ìˆ˜: {article['relevance_score']:.1f}/4.0")
            logger.info(f"ê´€ë ¨ì„± ì´ìœ : {', '.join(article['relevance_reasons'])}")
            logger.info(f"ì¹´í…Œê³ ë¦¬: {article['original_category']}")
            logger.info(f"ë°œí–‰ì¼: {article['pubDate']}")
            logger.info("-"*30)

        if len(labeled_articles) > 5:
            logger.info(f"... ì™¸ {len(labeled_articles) - 5}ê°œ ê¸°ì‚¬")
        
        # 8. ì‘ë‹µ ë°ì´í„° ìƒì„±
        response_data = {
            "success": True,
            "message": "ì¤‘ëŒ€ì„± í‰ê°€ ë°ì´í„° ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "data": {
                "company_id": request.company_id,
                "report_period": request.report_period,
                "assessment_status": "analyzed",
                "total_articles": len(labeled_articles),
                "negative_articles": negative_count,
                "high_relevance_articles": high_relevance_count,
                "negative_ratio": (negative_count/len(labeled_articles))*100 if labeled_articles else 0,
                "analyzed_samples": labeled_articles[:5]  # ìƒ˜í”Œ ë°ì´í„°ë§Œ í¬í•¨
            }
        }
        
        logger.info("âœ… ë°ì´í„° ë¶„ì„ ì™„ë£Œ")
        logger.info("="*50)
        
        return response_data
        
    except Exception as e:
        error_msg = f"âŒ ì¤‘ëŒ€ì„± í‰ê°€ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        logger.error(error_msg)
        logger.error("="*50)
        
        return {
            "success": False,
            "message": error_msg,
            "data": None
        }