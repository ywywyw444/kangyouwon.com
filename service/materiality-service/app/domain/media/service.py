# app/service/service.py

import os
import time
import random
import logging
import email.utils
from datetime import datetime, timezone, date
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

import httpx
import pandas as pd

logger = logging.getLogger("materiality.service")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë„¤ì´ë²„ ë‰´ìŠ¤ API í´ë¼ì´ì–¸íŠ¸ (ë™ê¸°)  â€” serviceì—ì„œ to_threadë¡œ ì‹¤í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

BASE_URL = "https://openapi.naver.com/v1/search/news.json"
MAX_DISPLAY = 100
MAX_START_LIMIT = 1000
JITTER_RANGE = (0.05, 0.25)


class NaverNewsClient:
    def __init__(
        self,
        *,
        min_interval: float | None = None,
        per_keyword_pause: float | None = None,
        max_retries: int | None = None,
    ) -> None:
        self.client_id = os.getenv("NAVER_CLIENT_ID")
        self.client_secret = os.getenv("NAVER_CLIENT_SECRET")
        if not self.client_id or not self.client_secret:
            raise ValueError("NAVER_CLIENT_ID / NAVER_CLIENT_SECRET í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        self.min_interval = float(os.getenv("NAVER_API_MIN_INTERVAL", "1.2") if min_interval is None else min_interval)
        self.per_keyword_pause = float(os.getenv("NAVER_API_PER_KEYWORD_PAUSE", "2.0") if per_keyword_pause is None else per_keyword_pause)
        self.max_retries = int(os.getenv("NAVER_API_MAX_RETRIES", "3") if max_retries is None else max_retries)

        self._last_request_ts = 0.0

        self.session = httpx.Client()
        self.session.headers.update(
            {
                "X-Naver-Client-Id": self.client_id,
                "X-Naver-Client-Secret": self.client_secret,
                "User-Agent": "materiality-service/1.0",
            }
        )

    # ë‚´ë¶€ ìœ í‹¸ (ë™ê¸°)
    def _throttle(self) -> None:
        elapsed = time.time() - self._last_request_ts
        wait = self.min_interval - elapsed
        if wait > 0:
            time.sleep(wait + random.uniform(*JITTER_RANGE))
        self._last_request_ts = time.time()

    def _request_with_retry(self, params: Dict[str, Any]) -> Dict[str, Any]:
        last_exc: Optional[Exception] = None
        for attempt in range(1, self.max_retries + 1):
            try:
                self._throttle()
                resp = self.session.get(BASE_URL, params=params, timeout=10)
                if resp.status_code == 429 or 500 <= resp.status_code < 600:
                    raise httpx.HTTPStatusError(f"HTTP {resp.status_code}: {resp.text[:160]}")
                resp.raise_for_status()
                return resp.json()
            except httpx.RequestError as e:
                last_exc = e
                backoff = (self.min_interval * (2 ** (attempt - 1))) + random.uniform(*JITTER_RANGE)
                logger.warning("ë„¤ì´ë²„ API ìš”ì²­ ì‹¤íŒ¨(%s/%s): %s â†’ %.2fs í›„ ì¬ì‹œë„", attempt, self.max_retries, e, backoff)
                time.sleep(backoff)
        logger.error("ë„¤ì´ë²„ ë‰´ìŠ¤ API ìš”ì²­ ì‹¤íŒ¨(ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼): %s", last_exc)
        raise last_exc if last_exc else RuntimeError("Unknown request error")

    # ë™ê¸° API
    def search(self, keyword: str, *, display: int = MAX_DISPLAY, sort: str = "date", start: int = 1) -> Dict[str, Any]:
        return self._request_with_retry({"query": keyword, "display": display, "sort": sort, "start": start})

    def search_by_date_range(
        self,
        *,
        keyword: str,
        start_date: str,
        end_date: str,
        max_results: int = 300,
    ) -> Dict[str, Any]:
        """pubDateê°€ start_date~end_date ë‚´ì¸ ê¸°ì‚¬ë§Œ ìˆ˜ì§‘ (ë™ê¸°)"""
        start_dt = datetime.strptime(start_date, "%Y-%m-%d").replace(tzinfo=timezone.utc)
        end_dt = datetime.strptime(end_date, "%Y-%m-%d").replace(tzinfo=timezone.utc)
        if start_dt > end_dt:
            raise ValueError("ì‹œì‘ ë‚ ì§œê°€ ì¢…ë£Œ ë‚ ì§œë³´ë‹¤ ëŠ¦ìŠµë‹ˆë‹¤.")

        collected: List[Dict[str, Any]] = []
        start = 1
        display = MAX_DISPLAY

        while start <= max_results and len(collected) < max_results:
            data = self.search(keyword, display=display, sort="date", start=start)
            items = data.get("items", [])
            if not items:
                break

            for item in items:
                try:
                    pub_dt = email.utils.parsedate_to_datetime(item.get("pubDate", ""))
                except Exception:
                    continue
                if not pub_dt:
                    continue
                if start_dt <= pub_dt <= end_dt:
                    origin = (item.get("originallink") or "").strip()
                    link = (item.get("link") or "").strip()
                    item["ë„¤ì´ë²„ë§í¬"] = link if ("n.news.naver.com" in link) else ""
                    item["ì›ë³¸ë§í¬"] = origin
                    collected.append(item)

            start += display
            if start > MAX_START_LIMIT:
                break

        return {"total": len(collected), "items": collected[:max_results], "start_date": start_date, "end_date": end_date}

    # URL ì •ê·œí™” (ì¤‘ë³µ ì œê±°ìš© í‚¤)
    @staticmethod
    def canonicalize_url(url: str) -> str:
        if not url:
            return ""
        try:
            p = urlparse(url.strip())
            netloc = p.netloc.lower()
            if netloc.startswith("www."):
                netloc = netloc[4:]
            drop_keys = {
                "utm_source", "utm_medium", "utm_campaign", "utm_term", "utm_content",
                "gclid", "fbclid", "igshid", "mc_cid", "mc_eid", "ref"
            }
            q = [(k, v) for k, v in parse_qsl(p.query, keep_blank_values=True) if k not in drop_keys]
            path = p.path.rstrip("/")
            return urlunparse((p.scheme, netloc, path, "", urlencode(q, doseq=True), ""))
        except Exception:
            return url.strip()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸: ì¹´í…Œê³ ë¦¬ ìŠ¬ë˜ì‹œ ë¶„í•´ & ì¤‘ë³µ ì œê±°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _split_category_tokens(raw: str | None) -> List[str]:
    """
    'A/B/C' ë˜ëŠ” 'Aï¼Bï½œC' í˜•íƒœì˜ ì¹´í…Œê³ ë¦¬ë¥¼ '/' ê¸°ì¤€ìœ¼ë¡œ ë¶„í•´í•˜ì—¬ í† í° ë¦¬ìŠ¤íŠ¸ ìƒì„±.
    ê³µë°± ì œê±° ë° ë¹ˆ í† í° ì œê±°.
    """
    if not raw:
        return []
    s = str(raw).strip().replace("ï¼", "/").replace("ï½œ", "/")
    return [p.strip() for p in s.split("/") if p and p.strip()]


def _dedupe_by_url(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ê¸°ì—… ë²”ìœ„ì—ì„œ URL(ì •ê·œí™”) ê¸°ì¤€ ì¤‘ë³µ ì œê±°"""
    seen: set[Tuple[str, str]] = set()
    out: List[Dict[str, Any]] = []
    for it in items:
        url_raw = it.get("originallink") or it.get("ì›ë³¸ë§í¬") or it.get("link") or it.get("ë„¤ì´ë²„ë§í¬") or ""
        key = NaverNewsClient.canonicalize_url(url_raw)
        pair = (it.get("company", ""), key)
        if pair in seen:
            continue
        seen.add(pair)
        out.append(it)
    return out


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„œë¹„ìŠ¤ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ (ë¹„ë™ê¸°)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def search_media(payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    í”„ë¡ íŠ¸ì—ì„œ ì „ë‹¬í•œ JSON(payload)ì„ ë°›ì•„, íšŒì‚¬Ã—ì´ìŠˆ ì¡°í•©ìœ¼ë¡œ
    ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¥¼ ê²€ìƒ‰í•œ ë’¤ JSON ê²°ê³¼ë¥¼ ë°˜í™˜í•œë‹¤.

    ë°˜í™˜ í˜•ì‹:
    {
        "success": True,
        "message": "...",
        "data": {
            "company_id": "...",
            "search_period": {"start_date": "...", "end_date": "..."},
            "search_type": "...",
            "total_results": int,
            "articles": [...],  # title, description, pubDate, originallink, ë„¤ì´ë²„ë§í¬, company, issue, keyword, query_kind
        },
        "timestamp": "...(ìš”ì²­ì—ì„œ ë°›ì€ ê°’ ê·¸ëŒ€ë¡œ ë°˜í™˜)"
    }
    """
    # ìš”ì²­ ë°ì´í„° íŒŒì‹±
    company_id: str = payload.get("company_id") or payload.get("companyname") or ""
    if not company_id:
        raise ValueError("company_id ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    rp: Dict[str, Any] = payload.get("report_period") or {}
    start_date: str = rp.get("start_date")
    if not start_date:
        raise ValueError("report_period.start_date ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    # end_dateëŠ” 'ê²€ìƒ‰ ë‹¹ì¼'ë¡œ ìœ ë™ ì ìš©
    end_date: str = date.today().isoformat()

    search_type: str = payload.get("search_type", "materiality_assessment")
    timestamp: Optional[str] = payload.get("timestamp")

    logger.info("ğŸ” ë§¤ì²´ê²€ìƒ‰: company_id=%s, start=%s, end=%s, type=%s", company_id, start_date, end_date, search_type)

    # ê°„ë‹¨í•œ í† í° ìƒì„± (ì‹¤ì œë¡œëŠ” DBì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨)
    tokens: List[str] = ["ESG", "ì§€ì†ê°€ëŠ¥ì„±", "ì¤‘ëŒ€ì„±"]

    # í† í°ì´ ì—†ìœ¼ë©´ íšŒì‚¬ëª… ë‹¨ë… ê²€ìƒ‰ë§Œ ìˆ˜í–‰
    if not tokens:
        logger.warning("ì¹´í…Œê³ ë¦¬ í† í°ì´ ì—†ì–´ íšŒì‚¬ëª… ë‹¨ë… ê²€ìƒ‰ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤. company=%s", company_id)

    # ë„¤ì´ë²„ API í´ë¼ì´ì–¸íŠ¸
    client = NaverNewsClient()

    # ì§ˆì˜ ëª©ë¡ êµ¬ì„±: (íšŒì‚¬ëª… + í† í°) + íšŒì‚¬ëª… ë‹¨ë…
    queries: List[Dict[str, Any]] = []
    max_results_per_keyword = int(os.getenv("NAVER_MAX_RESULTS_PER_KEYWORD", "300"))
    unique_company_max_results = int(os.getenv("NAVER_UNIQUE_COMPANY_MAX_RESULTS", "150"))

    for tok in tokens:
        keyword = f"{company_id} {tok}"
        queries.append(
            {
                "keyword": keyword,
                "company": company_id,
                "issue": tok,
                "query_kind": "company_issue",
                "max_results": max_results_per_keyword,
            }
        )

    # íšŒì‚¬ëª… ë‹¨ë… ê²€ìƒ‰ë„ ì¶”ê°€
    queries.append(
        {
            "keyword": company_id,
            "company": company_id,
            "issue": "",
            "query_kind": "company_only",
            "max_results": unique_company_max_results,
        }
    )

    # ì‹¤í–‰
    all_items: List[Dict[str, Any]] = []
    for q in queries:
        kw = q["keyword"]
        company = q["company"]
        issue = q["issue"]
        query_kind = q["query_kind"]
        per_kw_limit = int(q["max_results"])
        logger.info("â–¶ï¸ ë„¤ì´ë²„ ê²€ìƒ‰ ì‹œì‘ [%s]: %s (%s~%s, limit=%d)", query_kind, kw, start_date, end_date, per_kw_limit)
        try:
            result = client.search_by_date_range(
                keyword=kw, start_date=start_date, end_date=end_date, max_results=per_kw_limit
            )
            for it in result.get("items", []):
                it["company"] = company
                it["issue"] = issue
                it["keyword"] = kw
                it["query_kind"] = query_kind
                all_items.append(it)
            # í‚¤ì›Œë“œ ê°„ ê°„ê²© (ì§€í„° í¬í•¨)
            time.sleep(max(0.0, client.per_keyword_pause) + random.uniform(*JITTER_RANGE))
        except Exception as e:
            logger.error("ê²€ìƒ‰ ì‹¤íŒ¨ [%s] %s: %s", query_kind, kw, e)

    if not all_items:
        logger.warning("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. company=%s", company_id)

    # URL ê¸°ì¤€ ì¤‘ë³µ ì œê±°(ê¸°ì—… ë²”ìœ„ ë‚´)
    try:
        all_items = _dedupe_by_url(all_items)
    except Exception as e:
        logger.warning("ì¤‘ë³µ ì œê±° ì¤‘ ì˜¤ë¥˜(ë¬´ì‹œí•˜ê³  ê³„ì†): %s", e)

    # ì—‘ì…€ íŒŒì¼ ìƒì„±
    excel_file_path = None
    if all_items:
        try:
            # í˜„ì¬ ì‹œê°„ì„ íŒŒì¼ëª…ì— í¬í•¨
            timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
            excel_filename = f"media_search_{company_id}_{timestamp_str}.xlsx"
            excel_file_path = f"/tmp/{excel_filename}"  # Railwayì—ì„œëŠ” /tmp ë””ë ‰í† ë¦¬ ì‚¬ìš©
            
            # DataFrame ìƒì„± ë° ì—‘ì…€ ì €ì¥
            df = pd.DataFrame(all_items)
            
            # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
            columns_order = [
                'company', 'issue', 'query_kind', 'keyword',
                'title', 'description', 'pubDate', 'originallink', 'ë„¤ì´ë²„ë§í¬'
            ]
            
            # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
            existing_columns = [col for col in columns_order if col in df.columns]
            df_ordered = df[existing_columns]
            
            # ì—‘ì…€ íŒŒì¼ë¡œ ì €ì¥
            df_ordered.to_excel(excel_file_path, index=False, engine='openpyxl')
            logger.info(f"âœ… ì—‘ì…€ íŒŒì¼ ìƒì„± ì™„ë£Œ: {excel_file_path}")
            
        except Exception as e:
            logger.error(f"âŒ ì—‘ì…€ íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            excel_file_path = None

    response = {
        "success": True,
        "message": "ë¯¸ë””ì–´ ê²€ìƒ‰ ìš”ì²­ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤",
        "data": {
            "company_id": company_id,
            "search_period": {"start_date": start_date, "end_date": end_date},
            "search_type": search_type,
            "total_results": len(all_items),
            "articles": all_items,  # ê·¸ëŒ€ë¡œ ë°˜í™˜ (title/description/pubDate/originallink/ë„¤ì´ë²„ë§í¬ ë“± í¬í•¨)
        },
        "timestamp": timestamp,
        "excel_file": excel_file_path if excel_file_path else None
    }
    return response
