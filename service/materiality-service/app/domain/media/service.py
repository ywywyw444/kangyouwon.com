# app/service/service.py

import os
import time
import random
import logging
import email.utils
import traceback
import re
import html
import io
import base64
from datetime import datetime, timezone, date
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

import httpx
import pandas as pd
from app.domain.media.repository import MediaRepository

logger = logging.getLogger("materiality.service")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬ ë° ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def process_materiality_categories(categories: List[Any]) -> Tuple[List[str], Dict[str, str]]:
    """materiality_category ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ ìŠ¬ë˜ì‹œë¡œ ë¶„ë¦¬í•˜ê³  ì›ë³¸ ì¹´í…Œê³ ë¦¬ì™€ ë§¤í•‘"""
    try:
        logger.info(f"ğŸ” process_materiality_categories ì‹œì‘: {len(categories)}ê°œ ì¹´í…Œê³ ë¦¬ ì…ë ¥")
        
        # ì¹´í…Œê³ ë¦¬ë¥¼ ìŠ¬ë˜ì‹œë¡œ ë¶„ë¦¬í•˜ì—¬ ê°œë³„ ì´ìŠˆë¡œ ë§Œë“¤ê¸°
        all_issues = []
        issue_to_category = {}  # ì´ìŠˆ -> ì›ë³¸ ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        
        for i, category in enumerate(categories):
            # Pydantic BaseModelì˜ ê²½ìš° getattrì„ ì‚¬ìš©í•˜ê±°ë‚˜ ì§ì ‘ ì†ì„±ì— ì ‘ê·¼
            try:
                category_name = getattr(category, 'category_name', None)
                if category_name:
                    # ìŠ¬ë˜ì‹œ(/)ë¡œ êµ¬ë¶„ëœ ì¹´í…Œê³ ë¦¬ë¥¼ ê°œë³„ ì´ìŠˆë¡œ ë¶„ë¦¬
                    issues = [issue.strip() for issue in category_name.split('/') if issue.strip()]
                    
                    for issue in issues:
                        all_issues.append(issue)
                        issue_to_category[issue] = category_name  # ê° ì´ìŠˆë¥¼ ì›ë³¸ ì¹´í…Œê³ ë¦¬ì™€ ë§¤í•‘
                else:
                    logger.warning(f"    category_nameì´ ë¹„ì–´ìˆìŒ: {category}")
            except Exception as attr_error:
                logger.error(f"    category_name ì†ì„± ì ‘ê·¼ ì˜¤ë¥˜: {attr_error}")
                logger.error(f"    category ê°ì²´: {category}")
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        unique_issues = sorted(list(set(all_issues)))
        
        logger.info(f"âœ… materiality_categoryì—ì„œ {len(categories)}ê°œ ì¹´í…Œê³ ë¦¬ë¥¼ ê°€ì ¸ì™€ì„œ {len(unique_issues)}ê°œ ì´ìŠˆë¡œ ë¶„ë¦¬í–ˆìŠµë‹ˆë‹¤.")
        logger.info(f"ğŸ“‹ ì´ìŠˆ ëª©ë¡: {unique_issues}")
        
        return unique_issues, issue_to_category
        
    except Exception as e:
        logger.error(f"âŒ materiality_category ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        # ê¸°ë³¸ ì´ìŠˆ ë°˜í™˜
        default_issues = ["ESG", "ì§€ì†ê°€ëŠ¥ì„±", "ì¤‘ëŒ€ì„±"]
        default_mapping = {issue: issue for issue in default_issues}
        return default_issues, default_mapping

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë„¤ì´ë²„ ë‰´ìŠ¤ API í´ë¼ì´ì–¸íŠ¸ (ë™ê¸°)  â€” serviceì—ì„œ to_threadë¡œ ì‹¤í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

BASE_URL = "https://openapi.naver.com/v1/search/news.json"
MAX_DISPLAY = 100  # ë„¤ì´ë²„ API ìµœëŒ€ê°’ ìœ ì§€
MAX_START_LIMIT = 1000
JITTER_RANGE = (0.02, 0.08)  # ì§€í„° ë²”ìœ„ë¥¼ ì¤„ì—¬ì„œ ë” ë¹ ë¥´ê²Œ


class NaverNewsClient:
    def __init__(
        self,
        *,
        min_interval: float | None = None,
        per_keyword_pause: float | None = None,
        max_retries: int | None = None,
    ) -> None:
        self.client_id = os.getenv("NAVER_CLIENT_ID")
        self.client_secret = os.getenv("NAVER_CLIENT_SECRET")
        if not self.client_id or not self.client_secret:
            raise ValueError("NAVER_CLIENT_ID / NAVER_CLIENT_SECRET í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        self.min_interval = float(os.getenv("NAVER_API_MIN_INTERVAL", "0.8") if min_interval is None else min_interval)  # 1.2 â†’ 0.8ì´ˆë¡œ ë‹¨ì¶•
        self.per_keyword_pause = float(os.getenv("NAVER_API_PER_KEYWORD_PAUSE", "1.0") if per_keyword_pause is None else per_keyword_pause)  # 2.0 â†’ 1.0ì´ˆë¡œ ë‹¨ì¶•
        self.max_retries = int(os.getenv("NAVER_API_MAX_RETRIES", "3") if max_retries is None else max_retries)

        self._last_request_ts = 0.0

        self.session = httpx.Client()
        self.session.headers.update(
            {
                "X-Naver-Client-Id": self.client_id,
                "X-Naver-Client-Secret": self.client_secret,
                "User-Agent": "materiality-service/1.0",
            }
        )

    # ë‚´ë¶€ ìœ í‹¸ (ë™ê¸°)
    def _throttle(self) -> None:
        elapsed = time.time() - self._last_request_ts
        wait = self.min_interval - elapsed
        if wait > 0:
            time.sleep(wait + random.uniform(*JITTER_RANGE))
        self._last_request_ts = time.time()

    def _request_with_retry(self, params: Dict[str, Any]) -> Dict[str, Any]:
        last_exc: Optional[Exception] = None
        for attempt in range(1, self.max_retries + 1):
            try:
                self._throttle()
                resp = self.session.get(BASE_URL, params=params, timeout=10)
                if resp.status_code == 429 or 500 <= resp.status_code < 600:
                    raise httpx.HTTPStatusError(f"HTTP {resp.status_code}: {resp.text[:160]}")
                resp.raise_for_status()
                return resp.json()
            except httpx.RequestError as e:
                last_exc = e
                backoff = (self.min_interval * (2 ** (attempt - 1))) + random.uniform(*JITTER_RANGE)
                logger.warning("ë„¤ì´ë²„ API ìš”ì²­ ì‹¤íŒ¨(%s/%s): %s â†’ %.2fs í›„ ì¬ì‹œë„", attempt, self.max_retries, e, backoff)
                time.sleep(backoff)
        logger.error("ë„¤ì´ë²„ ë‰´ìŠ¤ API ìš”ì²­ ì‹¤íŒ¨(ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼): %s", last_exc)
        raise last_exc if last_exc else RuntimeError("Unknown request error")

    # ë™ê¸° API
    def search(self, keyword: str, *, display: int = MAX_DISPLAY, sort: str = "date", start: int = 1) -> Dict[str, Any]:
        return self._request_with_retry({"query": keyword, "display": display, "sort": sort, "start": start})

    def search_by_date_range(
        self,
        *,
        keyword: str,
        start_date: str,
        end_date: str,
        max_results: int = 300,
    ) -> Dict[str, Any]:
        """pubDateê°€ start_date~end_date ë‚´ì¸ ê¸°ì‚¬ë§Œ ìˆ˜ì§‘ (ë™ê¸°)"""
        start_dt = datetime.strptime(start_date, "%Y-%m-%d").replace(tzinfo=timezone.utc)
        end_dt = datetime.strptime(end_date, "%Y-%m-%d").replace(tzinfo=timezone.utc)
        if start_dt > end_dt:
            raise ValueError("ì‹œì‘ ë‚ ì§œê°€ ì¢…ë£Œ ë‚ ì§œë³´ë‹¤ ëŠ¦ìŠµë‹ˆë‹¤.")

        collected: List[Dict[str, Any]] = []
        start = 1
        display = MAX_DISPLAY

        while start <= max_results and len(collected) < max_results:
            data = self.search(keyword, display=display, sort="date", start=start)
            items = data.get("items", [])
            if not items:
                break

            for item in items:
                try:
                    pub_dt = email.utils.parsedate_to_datetime(item.get("pubDate", ""))
                except Exception:
                    continue
                if not pub_dt:
                    continue
                if start_dt <= pub_dt <= end_dt:
                    origin = (item.get("originallink") or "").strip()
                    link = (item.get("link") or "").strip()
                    item["ë„¤ì´ë²„ë§í¬"] = link if ("n.news.naver.com" in link) else ""
                    item["ì›ë³¸ë§í¬"] = origin
                    collected.append(item)

            start += display
            if start > MAX_START_LIMIT:
                break

        return {"total": len(collected), "items": collected[:max_results], "start_date": start_date, "end_date": end_date}

    # URL ì •ê·œí™” (ì¤‘ë³µ ì œê±°ìš© í‚¤)
    @staticmethod
    def canonicalize_url(url: str) -> str:
        if not url:
            return ""
        try:
            p = urlparse(url.strip())
            netloc = p.netloc.lower()
            if netloc.startswith("www."):
                netloc = netloc[4:]
            drop_keys = {
                "utm_source", "utm_medium", "utm_campaign", "utm_term", "utm_content",
                "gclid", "fbclid", "igshid", "mc_cid", "mc_eid", "ref"
            }
            q = [(k, v) for k, v in parse_qsl(p.query, keep_blank_values=True) if k not in drop_keys]
            path = p.path.rstrip("/")
            return urlunparse((p.scheme, netloc, path, "", urlencode(q, doseq=True), ""))
        except Exception:
            return url.strip()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸: ì¹´í…Œê³ ë¦¬ ìŠ¬ë˜ì‹œ ë¶„í•´ & ì¤‘ë³µ ì œê±°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _split_category_tokens(raw: str | None) -> List[str]:
    """
    'A/B/C' ë˜ëŠ” 'Aï¼Bï½œC' í˜•íƒœì˜ ì¹´í…Œê³ ë¦¬ë¥¼ '/' ê¸°ì¤€ìœ¼ë¡œ ë¶„í•´í•˜ì—¬ í† í° ë¦¬ìŠ¤íŠ¸ ìƒì„±.
    ê³µë°± ì œê±° ë° ë¹ˆ í† í° ì œê±°.
    """
    if not raw:
        return []
    s = str(raw).strip().replace("ï¼", "/").replace("ï½œ", "/")
    return [p.strip() for p in s.split("/") if p and p.strip()]


def _dedupe_by_url(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ê¸°ì—… ë²”ìœ„ì—ì„œ URL(ì •ê·œí™”) ê¸°ì¤€ ì¤‘ë³µ ì œê±°"""
    seen: set[Tuple[str, str]] = set()
    out: List[Dict[str, Any]] = []
    for it in items:
        url_raw = it.get("originallink") or it.get("ì›ë³¸ë§í¬") or it.get("link") or it.get("ë„¤ì´ë²„ë§í¬") or ""
        key = NaverNewsClient.canonicalize_url(url_raw)
        pair = (it.get("company", ""), key)
        if pair in seen:
            continue
        seen.add(pair)
        out.append(it)
    return out


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë°ì´í„° ì •ì œ í•¨ìˆ˜ë“¤ (tuning.py ê¸°ë°˜)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def strip_html(text: str) -> str:
    """HTML íƒœê·¸ ì œê±° ë° ì—”í‹°í‹° í•´ì œ"""
    if not text or pd.isna(text):
        return ""
    s = str(text)
    s = re.sub(r'<\s*br\s*/?>', ' ', s, flags=re.I)  # <br> â†’ ê³µë°±
    s = re.sub(r'<[^>]+>', '', s)   # ëª¨ë“  íƒœê·¸ ì œê±°
    s = html.unescape(s)             # &quot; ë“± ì—”í‹°í‹° í•´ì œ
    s = re.sub(r'\s+', ' ', s).strip()
    return s


def clean_pubdate(pubdate_str: str) -> str:
    """pubDateë¥¼ 'Thu, 14 Aug 2025' í˜•íƒœë¡œ ì •ì œ"""
    if not pubdate_str:
        return ""
    
    try:
        # RFC 2822 í˜•ì‹ íŒŒì‹± (Thu, 14 Aug 2025 07:08:00 +0900)
        dt = email.utils.parsedate_to_datetime(pubdate_str)
        if dt:
            # ìš”ì¼, ì¼, ì›”, ë…„ë„ë§Œ ì¶”ì¶œ
            return dt.strftime("%a, %d %b %Y")
    except Exception:
        pass
    
    # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
    return str(pubdate_str)


def norm_plain(text: str) -> str:
    """ì¼ë°˜ ì •ê·œí™”(ì˜ë¬¸+í•œê¸€)"""
    s = strip_html(text).lower()
    s = re.sub(r'[^ê°€-í£a-z0-9]', '', s)
    return s


def has_triangle_then_company(desc: str, company: str) -> bool:
    """â–³/â–² ë’¤ì— íšŒì‚¬ëª…ì´ ë‚˜ì˜¤ë©´ True (í˜¼í•©í‘œê¸° ì‹œ í•œê¸€ë§Œ ì¼ì¹˜ë„ í—ˆìš©)"""
    if not desc or not company:
        return False
    
    d = strip_html(desc).lower()
    comp_norm = norm_plain(company)
    
    if not comp_norm:
        return False
    
    # â–³/â–² ì´í›„ íšŒì‚¬ëª…ì´ ë‚˜ì˜¤ëŠ”ì§€ í™•ì¸
    pattern = rf'[â–³â–²][^â–³â–²]*{re.escape(comp_norm)}'
    return bool(re.search(pattern, d))


def filter_news_items(items: List[Dict[str, Any]], company: str) -> List[Dict[str, Any]]:
    """ë‰´ìŠ¤ ì•„ì´í…œ í•„í„°ë§ ë° ì •ì œ"""
    if not items:
        return []
    
    filtered_items = []
    
    for item in items:
        # HTML íƒœê·¸ ì œê±°
        if "title" in item:
            item["title"] = strip_html(item["title"])
        if "description" in item:
            item["description"] = strip_html(item["description"])
        
        # pubDate ì •ì œ
        if "pubDate" in item:
            item["pubDate"] = clean_pubdate(item["pubDate"])
        
        # â–³/â–² ë’¤ì— íšŒì‚¬ëª…ì´ ë‚˜ì˜¤ëŠ” ê¸°ì‚¬ ì œì™¸
        if has_triangle_then_company(item.get("description", ""), company):
            continue
        
        # ë¶ˆìš© í‚¤ì›Œë“œ ê¸°ì‚¬ ì œì™¸ (ì™„í™”ëœ ë²„ì „)
        # ë„ˆë¬´ ì—„ê²©í•œ í•„í„°ë§ìœ¼ë¡œ ì¸í•´ ê¸°ì‚¬ê°€ ëª¨ë‘ ì œê±°ë˜ëŠ” ê²ƒì„ ë°©ì§€
        keywords = ["ë¶€ê³ ", "ê¸°ê³ "]  # ì •ë§ ë¶ˆí•„ìš”í•œ ê²ƒë§Œ ì œì™¸
        pattern = "|".join(keywords)
        
        title = item.get("title", "").lower()
        description = item.get("description", "").lower()
        
        # ì œëª©ê³¼ ë‚´ìš© ëª¨ë‘ì— ë¶ˆìš© í‚¤ì›Œë“œê°€ ìˆëŠ” ê²½ìš°ë§Œ ì œì™¸
        if re.search(pattern, title) and re.search(pattern, description):
            continue
        
        filtered_items.append(item)
    
    return filtered_items


def _make_excel_bytes(items: List[Dict[str, Any]], company_id: str) -> Tuple[str, bytes]:
    """ì—‘ì…€ì„ ë©”ëª¨ë¦¬ì—ì„œ ìƒì„±í•˜ì—¬ ë°”ì´íŠ¸ì™€ íŒŒì¼ëª… ë°˜í™˜"""
    if not items:
        raise ValueError("ì—‘ì…€ ìƒì„±í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
    
    try:
        # DataFrame ìƒì„± ì „ ë°ì´í„° ì •ë¦¬
        cleaned_items = []
        for item in items:
            cleaned_item = {}
            for key, value in item.items():
                # None ê°’ê³¼ ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬
                if value is None:
                    cleaned_item[key] = ""
                elif isinstance(value, (dict, list)):
                    cleaned_item[key] = str(value)
                else:
                    cleaned_item[key] = str(value).strip() if isinstance(value, str) else value
            cleaned_items.append(cleaned_item)
        
        # DataFrame ìƒì„±
        df = pd.DataFrame(cleaned_items)
        logger.info(f"ğŸ“Š DataFrame ìƒì„± ì™„ë£Œ: {df.shape[0]}í–‰ x {df.shape[1]}ì—´")
        
        # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
        columns_order = [
            'company', 'issue', 'original_category', 'query_kind', 'keyword',
            'title', 'description', 'pubDate', 'originallink', 'ë„¤ì´ë²„ë§í¬'
        ]
        
        # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ
        existing_columns = [col for col in columns_order if col in df.columns]
        df_ordered = df[existing_columns]
        
        # NaN ê°’ ì²˜ë¦¬
        df_ordered = df_ordered.fillna("")
        
        # ë©”ëª¨ë¦¬ì—ì„œ ì—‘ì…€ ìƒì„±
        buf = io.BytesIO()
        with pd.ExcelWriter(buf, engine="openpyxl") as writer:
            df_ordered.to_excel(writer, sheet_name="ê²€ìƒ‰ê²°ê³¼", index=False)
            
            # ì›Œí¬ì‹œíŠ¸ ìŠ¤íƒ€ì¼ë§
            worksheet = writer.sheets["ê²€ìƒ‰ê²°ê³¼"]
            for column in worksheet.columns:
                max_length = 0
                column_letter = column[0].column_letter
                for cell in column:
                    try:
                        if len(str(cell.value)) > max_length:
                            max_length = len(str(cell.value))
                    except:
                        pass
                adjusted_width = min(max_length + 2, 50)
                worksheet.column_dimensions[column_letter].width = adjusted_width
        
        buf.seek(0)
        excel_bytes = buf.getvalue()
        
        # íŒŒì¼ëª… ìƒì„±
        timestamp_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"media_search_{company_id}_{timestamp_str}.xlsx"
        
        logger.info(f"âœ… ë©”ëª¨ë¦¬ì—ì„œ ì—‘ì…€ ìƒì„± ì™„ë£Œ: {filename} (í¬ê¸°: {len(excel_bytes)} bytes)")
        
        return filename, excel_bytes
        
    except Exception as e:
        logger.error(f"âŒ ì—‘ì…€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        raise


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„œë¹„ìŠ¤ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ (ë¹„ë™ê¸°)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def search_media(payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    í”„ë¡ íŠ¸ì—ì„œ ì „ë‹¬í•œ JSON(payload)ì„ ë°›ì•„, íšŒì‚¬Ã—ì´ìŠˆ ì¡°í•©ìœ¼ë¡œ
    ë„¤ì´ë²„ ë‰´ìŠ¤ APIë¥¼ ê²€ìƒ‰í•œ ë’¤ JSON ê²°ê³¼ë¥¼ ë°˜í™˜í•œë‹¤.

    ë°˜í™˜ í˜•ì‹:
    {
        "success": True,
        "message": "...",
        "data": {
            "company_id": "...",
            "search_period": {"start_date": "...", "end_date": "..."},
            "search_type": "...",
            "total_results": int,
            "articles": [...],  # title, description, pubDate, originallink, ë„¤ì´ë²„ë§í¬, company, issue, keyword, query_kind
        },
        "timestamp": "...(ìš”ì²­ì—ì„œ ë°›ì€ ê°’ ê·¸ëŒ€ë¡œ ë°˜í™˜)"
    }
    """
    # ìš”ì²­ ë°ì´í„° íŒŒì‹±
    company_id: str = payload.get("company_id") or payload.get("companyname") or ""
    if not company_id:
        raise ValueError("company_id ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    rp: Dict[str, Any] = payload.get("report_period") or {}
    start_date: str = rp.get("start_date")
    if not start_date:
        raise ValueError("report_period.start_date ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

    # end_dateëŠ” 'ê²€ìƒ‰ ë‹¹ì¼'ë¡œ ìœ ë™ ì ìš©
    end_date: str = date.today().isoformat()

    search_type: str = payload.get("search_type", "materiality_assessment")
    timestamp: Optional[str] = payload.get("timestamp")

    logger.info("ğŸ” ë§¤ì²´ê²€ìƒ‰: company_id=%s, start=%s, end=%s, type=%s", company_id, start_date, end_date, search_type)

    # materiality_category í…Œì´ë¸”ì—ì„œ ì¹´í…Œê³ ë¦¬ ê°€ì ¸ì˜¤ê¸° (ë¦¬í¬ì§€í† ë¦¬ ì‚¬ìš©)
    try:
        repository = MediaRepository()
        # ë™ê¸° í•¨ìˆ˜ì—ì„œ ë¹„ë™ê¸° ë¦¬í¬ì§€í† ë¦¬ í˜¸ì¶œì„ ìœ„í•´ ë” ì•ˆì „í•œ ë°©ì‹ ì‚¬ìš©
        import asyncio
        import concurrent.futures
        
        # ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future = executor.submit(asyncio.run, repository.get_all_materiality_categories())
            categories = future.result()
        
        # ì¹´í…Œê³ ë¦¬ ë°ì´í„° ì²˜ë¦¬
        logger.info(f"ğŸ” DBì—ì„œ ê°€ì ¸ì˜¨ ì¹´í…Œê³ ë¦¬ ë°ì´í„°: {len(categories)}ê°œ")
        tokens, issue_to_category = process_materiality_categories(categories)
            
    except Exception as e:
        logger.error(f"âŒ materiality_category ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        # ê¸°ë³¸ í† í° ì‚¬ìš©
        tokens = ["ESG", "ì§€ì†ê°€ëŠ¥ì„±", "ì¤‘ëŒ€ì„±"]
        issue_to_category = {token: token for token in tokens}

    # í† í°ì´ ì—†ìœ¼ë©´ íšŒì‚¬ëª… ë‹¨ë… ê²€ìƒ‰ë§Œ ìˆ˜í–‰
    if not tokens:
        logger.warning("ì¹´í…Œê³ ë¦¬ í† í°ì´ ì—†ì–´ íšŒì‚¬ëª… ë‹¨ë… ê²€ìƒ‰ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤. company=%s", company_id)

    # ë„¤ì´ë²„ API í´ë¼ì´ì–¸íŠ¸
    try:
        # ë™ê¸° í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
        import asyncio
        loop = asyncio.get_event_loop()
        client = await loop.run_in_executor(None, NaverNewsClient)
        logger.info("âœ… NaverNewsClient ì´ˆê¸°í™” ì„±ê³µ")
    except Exception as e:
        logger.error(f"âŒ NaverNewsClient ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        raise ValueError(f"ë„¤ì´ë²„ API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")

    # ì§ˆì˜ ëª©ë¡ êµ¬ì„±: (íšŒì‚¬ëª… + í† í°) + íšŒì‚¬ëª… ë‹¨ë…
    queries: List[Dict[str, Any]] = []
    max_results_per_keyword = int(os.getenv("NAVER_MAX_RESULTS_PER_KEYWORD", "500"))  # 300 â†’ 500ìœ¼ë¡œ ì¦ê°€
    unique_company_max_results = int(os.getenv("NAVER_UNIQUE_COMPANY_MAX_RESULTS", "300"))  # 150 â†’ 300ìœ¼ë¡œ ì¦ê°€

    for tok in tokens:
        # ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ë” ë‹¨ìˆœí•˜ê²Œ êµ¬ì„±
        keyword = f"{company_id} {tok}"
        queries.append(
            {
                "keyword": keyword,
                "company": company_id,
                "issue": tok,
                "query_kind": "company_issue",
                "max_results": max_results_per_keyword,
            }
        )
        
        # íšŒì‚¬ëª…ë§Œìœ¼ë¡œë„ ê²€ìƒ‰ ì¶”ê°€ (ë” ë§ì€ ê²°ê³¼ë¥¼ ìœ„í•´)
        if tok:  # ë¹ˆ í† í°ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ
            queries.append(
                {
                    "keyword": company_id,  # íšŒì‚¬ëª…ë§Œ
                    "company": company_id,
                    "issue": tok,
                    "query_kind": "company_only_issue",
                    "max_results": max_results_per_keyword // 2,  # ì ˆë°˜ë§Œ
                }
            )

    # íšŒì‚¬ëª… ë‹¨ë… ê²€ìƒ‰ë„ ì¶”ê°€
    queries.append(
        {
            "keyword": company_id,
            "company": company_id,
            "issue": "",
            "query_kind": "company_only",
            "max_results": unique_company_max_results,
        }
    )

                   # ì‹¤í–‰
               all_items: List[Dict[str, Any]] = []
               
               # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ë¨¼ì € ì‹œë„
               try:
                   logger.info("ğŸ” ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì‹œì‘: 'ì„¸ë°©'")
                   test_result = await loop.run_in_executor(
                       None, 
                       client.search, 
                       "ì„¸ë°©"
                   )
                   logger.info(f"âœ… í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ê²°ê³¼: {len(test_result.get('items', []))}ê°œ ê¸°ì‚¬")
                   
                   # í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ all_itemsì— ì¶”ê°€
                   for it in test_result.get("items", []):
                       it["company"] = company_id
                       it["issue"] = "í…ŒìŠ¤íŠ¸"
                       it["keyword"] = "ì„¸ë°©"
                       it["query_kind"] = "test_search"
                       it["original_category"] = "í…ŒìŠ¤íŠ¸"
                       all_items.append(it)
                       
               except Exception as e:
                   logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
                   logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
               
               # ê¸°ì¡´ ê²€ìƒ‰ ë¡œì§
               for q in queries:
                   kw = q["keyword"]
                   company = q["company"]
                   issue = q["issue"]
                   query_kind = q["query_kind"]
                   per_kw_limit = int(q["max_results"])
                   logger.info("â–¶ï¸ ë„¤ì´ë²„ ê²€ìƒ‰ ì‹œì‘ [%s]: %s (%s~%s, limit=%d)", query_kind, kw, start_date, end_date, per_kw_limit)
                   try:
                       # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
                       import asyncio
                       loop = asyncio.get_event_loop()
                       result = await loop.run_in_executor(
                           None, 
                           client.search_by_date_range,
                           kw, start_date, end_date, per_kw_limit
                       )
                       for it in result.get("items", []):
                           it["company"] = company
                           it["issue"] = issue
                           it["keyword"] = kw
                           it["query_kind"] = query_kind
                           # ì›ë³¸ ì¹´í…Œê³ ë¦¬ ì •ë³´ ì¶”ê°€
                           if issue in issue_to_category:
                               it["original_category"] = issue_to_category[issue]
                           else:
                               it["original_category"] = issue
                           all_items.append(it)
                       # í‚¤ì›Œë“œ ê°„ ê°„ê²© (ì§€í„° í¬í•¨) - ë¹„ë™ê¸°ë¡œ ëŒ€ê¸°
                       await asyncio.sleep(max(0.0, client.per_keyword_pause) + random.uniform(*JITTER_RANGE))
                   except Exception as e:
                       logger.error("ê²€ìƒ‰ ì‹¤íŒ¨ [%s] %s: %s", query_kind, kw, e)

                   if not all_items:
                   logger.warning("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. company=%s", company_id)
                   logger.warning("ğŸ” ê²€ìƒ‰ëœ ê¸°ì‚¬ê°€ 0ê±´ì…ë‹ˆë‹¤. ë‹¤ìŒì„ í™•ì¸í•´ë³´ì„¸ìš”:")
                   logger.warning("  1. ê²€ìƒ‰ í‚¤ì›Œë“œ: %s", [q["keyword"] for q in queries])
                   logger.warning("  2. ê²€ìƒ‰ ê¸°ê°„: %s ~ %s", start_date, end_date)
                   logger.warning("  3. ë„¤ì´ë²„ API ì‘ë‹µ í™•ì¸ í•„ìš”")
                   logger.warning("  4. ë„¤ì´ë²„ API í‚¤ ì„¤ì • í™•ì¸ í•„ìš”")
                   logger.warning("  5. ë„¤ì´ë²„ API í• ë‹¹ëŸ‰ í™•ì¸ í•„ìš”")
               else:
                   logger.info(f"âœ… ë„¤ì´ë²„ APIì—ì„œ ì´ {len(all_items)}ê±´ì˜ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
                   logger.info(f"ğŸ“Š ê¸°ì‚¬ ìƒ˜í”Œ: {[item.get('title', 'ì œëª©ì—†ìŒ')[:30] for item in all_items[:3]]}")

    # URL ê¸°ì¤€ ì¤‘ë³µ ì œê±°(ê¸°ì—… ë²”ìœ„ ë‚´)
    try:
        all_items = _dedupe_by_url(all_items)
        logger.info(f"âœ… ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(all_items)}ê°œ ê¸°ì‚¬")
    except Exception as e:
        logger.warning("ì¤‘ë³µ ì œê±° ì¤‘ ì˜¤ë¥˜(ë¬´ì‹œí•˜ê³  ê³„ì†): %s", e)

    # ë°ì´í„° ì •ì œ
    try:
        original_count = len(all_items)
        all_items = filter_news_items(all_items, company_id)
        filtered_count = len(all_items)
        logger.info(f"âœ… ë°ì´í„° ì •ì œ ì™„ë£Œ: {original_count}ê°œ â†’ {filtered_count}ê°œ ê¸°ì‚¬")
    except Exception as e:
        logger.warning("ë°ì´í„° ì •ì œ ì¤‘ ì˜¤ë¥˜(ë¬´ì‹œí•˜ê³  ê³„ì†): %s", e)

    # ì—‘ì…€ ìƒì„± (ë©”ëª¨ë¦¬ì—ì„œ)
    excel_filename = None
    excel_base64 = None
    if all_items:
        try:
            # ë™ê¸° ì—‘ì…€ ìƒì„± í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            import asyncio
            loop = asyncio.get_event_loop()
            filename, excel_bytes = await loop.run_in_executor(
                None, 
                _make_excel_bytes, 
                all_items, 
                company_id
            )
            excel_filename = filename
            excel_base64 = base64.b64encode(excel_bytes).decode("ascii")
            logger.info(f"âœ… ì—‘ì…€ ìƒì„± ì™„ë£Œ: {filename} (Base64 ê¸¸ì´: {len(excel_base64)})")
        except Exception as e:
            logger.error(f"âŒ ì—‘ì…€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            excel_filename = None
            excel_base64 = None

    response = {
        "success": True,
        "message": "ë¯¸ë””ì–´ ê²€ìƒ‰ ìš”ì²­ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤",
        "data": {
            "company_id": company_id,
            "search_period": {"start_date": start_date, "end_date": end_date},
            "search_type": search_type,
            "total_results": len(all_items),
            "articles": all_items,  # ê·¸ëŒ€ë¡œ ë°˜í™˜ (title/description/pubDate/originallink/ë„¤ì´ë²„ë§í¬ ë“± í¬í•¨)
        },
        "timestamp": timestamp,
        "excel_filename": excel_filename,
        "excel_base64": excel_base64
    }
    return response
