# app/domain/media/service.py

from __future__ import annotations

import os
import time
import random
import logging
import email.utils
import asyncio
from datetime import datetime, timezone, date
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode

import requests

from app.domain.media.repository import MediaRepository

logger = logging.getLogger(__name__)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë„¤ì´ë²„ ë‰´ìŠ¤ API í´ë¼ì´ì–¸íŠ¸ (ë™ê¸°)  â€” serviceì—ì„œ to_threadë¡œ ì‹¤í–‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

BASE_URL = "https://openapi.naver.com/v1/search/news.json"
MAX_DISPLAY = 100
MAX_START_LIMIT = 1000
JITTER_RANGE = (0.05, 0.25)


class NaverNewsClient:
    def __init__(
        self,
        *,
        min_interval: float | None = None,
        per_keyword_pause: float | None = None,
        max_retries: int | None = None,
    ) -> None:
        self.client_id = os.getenv("NAVER_CLIENT_ID")
        self.client_secret = os.getenv("NAVER_CLIENT_SECRET")
        if not self.client_id or not self.client_secret:
            raise ValueError("NAVER_CLIENT_ID / NAVER_CLIENT_SECRET í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

        self.min_interval = float(os.getenv("NAVER_API_MIN_INTERVAL", "1.2") if min_interval is None else min_interval)
        self.per_keyword_pause = float(os.getenv("NAVER_API_PER_KEYWORD_PAUSE", "2.0") if per_keyword_pause is None else per_keyword_pause)
        self.max_retries = int(os.getenv("NAVER_API_MAX_RETRIES", "3") if max_retries is None else max_retries)

        self._last_request_ts = 0.0

        self.session = requests.Session()
        self.session.headers.update(
            {
                "X-Naver-Client-Id": self.client_id,
                "X-Naver-Client-Secret": self.client_secret,
                "User-Agent": "materiality-service/1.0",
            }
        )

    # ë‚´ë¶€ ìœ í‹¸ (ë™ê¸°)
    def _throttle(self) -> None:
        elapsed = time.time() - self._last_request_ts
        wait = self.min_interval - elapsed
        if wait > 0:
            time.sleep(wait + random.uniform(*JITTER_RANGE))
        self._last_request_ts = time.time()

    def _request_with_retry(self, params: Dict[str, Any]) -> Dict[str, Any]:
        last_exc: Optional[Exception] = None
        for attempt in range(1, self.max_retries + 1):
            try:
                self._throttle()
                resp = self.session.get(BASE_URL, params=params, timeout=10)
                if resp.status_code == 429 or 500 <= resp.status_code < 600:
                    raise requests.HTTPError(f"HTTP {resp.status_code}: {resp.text[:180]}")
                resp.raise_for_status()
                return resp.json()
            except requests.RequestException as e:
                last_exc = e
                backoff = (self.min_interval * (2 ** (attempt - 1))) + random.uniform(*JITTER_RANGE)
                logger.warning("ë„¤ì´ë²„ API ìš”ì²­ ì‹¤íŒ¨(%s/%s): %s â†’ %.2fs í›„ ì¬ì‹œë„", attempt, self.max_retries, e, backoff)
                time.sleep(backoff)
        logger.error("ë„¤ì´ë²„ ë‰´ìŠ¤ API ìš”ì²­ ì‹¤íŒ¨(ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼): %s", last_exc)
        raise last_exc if last_exc else RuntimeError("Unknown request error")

    # ë™ê¸° API
    def search(self, keyword: str, *, display: int = MAX_DISPLAY, sort: str = "date", start: int = 1) -> Dict[str, Any]:
        return self._request_with_retry({"query": keyword, "display": display, "sort": sort, "start": start})

    def search_by_date_range(
        self,
        *,
        keyword: str,
        start_date: str,
        end_date: str,
        max_results: int = 300,
    ) -> Dict[str, Any]:
        """pubDateê°€ start_date~end_date ë‚´ì¸ ê¸°ì‚¬ë§Œ ìˆ˜ì§‘ (ë™ê¸°)"""
        start_dt = datetime.strptime(start_date, "%Y-%m-%d").replace(tzinfo=timezone.utc)
        end_dt = datetime.strptime(end_date, "%Y-%m-%d").replace(tzinfo=timezone.utc)
        if start_dt > end_dt:
            raise ValueError("ì‹œì‘ ë‚ ì§œê°€ ì¢…ë£Œ ë‚ ì§œë³´ë‹¤ ëŠ¦ìŠµë‹ˆë‹¤.")

        collected: List[Dict[str, Any]] = []
        start = 1
        display = MAX_DISPLAY

        while start <= max_results and len(collected) < max_results:
            data = self.search(keyword, display=display, sort="date", start=start)
            items = data.get("items", [])
            if not items:
                break

            for item in items:
                try:
                    pub_dt = email.utils.parsedate_to_datetime(item.get("pubDate", ""))
                except Exception:
                    continue
                if not pub_dt:
                    continue
                if start_dt <= pub_dt <= end_dt:
                    origin = (item.get("originallink") or "").strip()
                    link = (item.get("link") or "").strip()
                    item["ë„¤ì´ë²„ë§í¬"] = link if ("n.news.naver.com" in link) else ""
                    item["ì›ë³¸ë§í¬"] = origin
                    collected.append(item)

            start += display
            if start > MAX_START_LIMIT:
                break

        return {"total": len(collected), "items": collected[:max_results], "start_date": start_date, "end_date": end_date}

    # URL ì •ê·œí™” (ì¤‘ë³µ ì œê±°ìš© í‚¤)
    @staticmethod
    def canonicalize_url(url: str) -> str:
        if not url:
            return ""
        try:
            p = urlparse(url.strip())
            netloc = p.netloc.lower()
            if netloc.startswith("www."):
                netloc = netloc[4:]
            drop_keys = {
                "utm_source", "utm_medium", "utm_campaign", "utm_term", "utm_content",
                "gclid", "fbclid", "igshid", "mc_cid", "mc_eid", "ref"
            }
            q = [(k, v) for k, v in parse_qsl(p.query, keep_blank_values=True) if k not in drop_keys]
            path = p.path.rstrip("/")
            return urlunparse((p.scheme, netloc, path, "", urlencode(q, doseq=True), ""))
        except Exception:
            return url.strip()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸: ì¹´í…Œê³ ë¦¬ ìŠ¬ë˜ì‹œ ë¶„í•´ & ì¤‘ë³µ ì œê±°
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _split_category_tokens(raw: str | None) -> List[str]:
    """
    'A/B/C' ë˜ëŠ” 'Aï¼Bï½œC' í˜•íƒœì˜ ì¹´í…Œê³ ë¦¬ë¥¼ '/' ê¸°ì¤€ìœ¼ë¡œ ë¶„í•´í•˜ì—¬ í† í° ë¦¬ìŠ¤íŠ¸ ìƒì„±.
    ê³µë°± ì œê±° ë° ë¹ˆ í† í° ì œê±°.
    """
    if not raw:
        return []
    s = str(raw).strip().replace("ï¼", "/").replace("ï½œ", "/")
    return [p.strip() for p in s.split("/") if p and p.strip()]


def _dedupe_by_url(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """ê¸°ì—… ë²”ìœ„ì—ì„œ URL(ì •ê·œí™”) ê¸°ì¤€ ì¤‘ë³µ ì œê±°"""
    seen: set[Tuple[str, str]] = set()
    out: List[Dict[str, Any]] = []
    for it in items:
        url_raw = it.get("originallink") or it.get("ì›ë³¸ë§í¬") or it.get("link") or it.get("ë„¤ì´ë²„ë§í¬") or ""
        key = NaverNewsClient.canonicalize_url(url_raw)
        pair = (it.get("company", ""), key)
        if pair in seen:
            continue
        seen.add(pair)
        out.append(it)
    return out


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„œë¹„ìŠ¤ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ (ë¹„ë™ê¸°)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def search_media(search_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    controller â†’ service ì§„ì…ì .
    - search_data: {'company_id': '...', 'report_period': {'start_date': 'YYYY-MM-DD', 'end_date': '...'}, ...}
    - end_dateëŠ” ë¬´ì‹œí•˜ê³  'ì˜¤ëŠ˜ ë‚ ì§œ'ë¡œ ìë™ ì„¤ì •.
    - repositoryì—ì„œ ëª¨ë“  ì¹´í…Œê³ ë¦¬ë¥¼ BaseModelë¡œ ë°›ì•„ì™€ category_nameì„ '/' ë¶„í•´í•˜ì—¬ í† í° ìƒì„±.
    """
    company_id = (search_data or {}).get("company_id") or (search_data or {}).get("companyname") or ""
    if not company_id:
        return {"success": False, "message": "company_idê°€ í•„ìš”í•©ë‹ˆë‹¤."}

    rp = (search_data or {}).get("report_period") or {}
    start_date = rp.get("start_date")
    if not start_date:
        return {"success": False, "message": "report_period.start_dateê°€ í•„ìš”í•©ë‹ˆë‹¤."}

    # ê²€ìƒ‰ ì¢…ë£Œì¼ì€ 'ìš”ì²­ ì‹œì ì˜ ë‹¹ì¼'
    end_date = date.today().isoformat()
    search_type = (search_data or {}).get("search_type", "materiality_assessment")
    timestamp = (search_data or {}).get("timestamp")

    logger.info("ğŸ” ë¯¸ë””ì–´ ê²€ìƒ‰: company=%s, start=%s, end=%s, type=%s", company_id, start_date, end_date, search_type)

    # 1) Repositoryì—ì„œ ëª¨ë“  ì¹´í…Œê³ ë¦¬(BaseModel) ì¡°íšŒ í›„ í† í°í™”
    repo = MediaRepository()
    category_models = await repo.get_all_materiality_categories()  # List[MaterialityCategoryRequest]
    tokens: List[str] = []
    for cm in category_models or []:
        # BaseModel: MaterialityCategoryRequest(category_name, esg_classification_id)
        tokens.extend(_split_category_tokens(getattr(cm, "category_name", None)))

    # ì¤‘ë³µ ì œê±° & ì •ë ¬
    tokens = sorted({t for t in tokens if t})

    if not tokens:
        logger.warning("ì¹´í…Œê³ ë¦¬ í† í°ì´ ë¹„ì–´ ìˆì–´ íšŒì‚¬ëª… ë‹¨ë… ê²€ìƒ‰ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.")

    # 2) ë„¤ì´ë²„ í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
    client = NaverNewsClient()

    # í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ íŠœë‹
    max_results_per_keyword = int(os.getenv("NAVER_MAX_RESULTS_PER_KEYWORD", "300"))
    unique_company_max_results = int(os.getenv("NAVER_UNIQUE_COMPANY_MAX_RESULTS", "150"))

    # 3) ì§ˆì˜ êµ¬ì„±: (íšŒì‚¬ëª… Ã— í† í°) + íšŒì‚¬ëª… ë‹¨ë…
    queries: List[Dict[str, Any]] = []
    for tok in tokens:
        queries.append(
            {
                "keyword": f"{company_id} {tok}",
                "company": company_id,
                "issue": tok,
                "query_kind": "company_issue",
                "max_results": max_results_per_keyword,
            }
        )
    # íšŒì‚¬ëª… ë‹¨ë…
    queries.append(
        {
            "keyword": company_id,
            "company": company_id,
            "issue": "",
            "query_kind": "company_only",
            "max_results": unique_company_max_results,
        }
    )

    # 4) ì‹¤í–‰ (ë™ê¸° í´ë¼ì´ì–¸íŠ¸ë¥¼ ìŠ¤ë ˆë“œë¡œ ëŒë¦¼)
    all_items: List[Dict[str, Any]] = []
    for q in queries:
        kw = q["keyword"]
        company = q["company"]
        issue = q["issue"]
        query_kind = q["query_kind"]
        per_kw_limit = int(q["max_results"])

        logger.info("â–¶ï¸ ë„¤ì´ë²„ ê²€ìƒ‰ ì‹œì‘ [%s]: %s (%s~%s, limit=%d)", query_kind, kw, start_date, end_date, per_kw_limit)

        try:
            result: Dict[str, Any] = await asyncio.to_thread(
                client.search_by_date_range,
                keyword=kw,
                start_date=start_date,
                end_date=end_date,
                max_results=per_kw_limit,
            )
            for it in result.get("items", []):
                it["company"] = company
                it["issue"] = issue
                it["keyword"] = kw
                it["query_kind"] = query_kind
                all_items.append(it)

            # í‚¤ì›Œë“œ ê°„ ëŒ€ê¸° (ì´ê±´ ë¹„ë™ê¸° sleep)
            await asyncio.sleep(max(0.0, client.per_keyword_pause) + random.uniform(*JITTER_RANGE))
        except Exception as e:
            logger.error("ê²€ìƒ‰ ì‹¤íŒ¨ [%s] %s: %s", query_kind, kw, e)

    if not all_items:
        logger.warning("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. company=%s", company_id)

    # 5) ì¤‘ë³µ ì œê±°
    try:
        all_items = _dedupe_by_url(all_items)
    except Exception as e:
        logger.warning("ì¤‘ë³µ ì œê±° ì¤‘ ì˜¤ë¥˜: %s (ë¬´ì‹œ)", e)

    # 6) ê²°ê³¼ ë°˜í™˜(JSON)
    return {
        "success": True,
        "message": "ë¯¸ë””ì–´ ê²€ìƒ‰ ìš”ì²­ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤",
        "data": {
            "company_id": company_id,
            "search_period": {"start_date": start_date, "end_date": end_date},
            "search_type": search_type,
            "total_results": len(all_items),
            "articles": all_items,
        },
        "timestamp": timestamp,
    }
